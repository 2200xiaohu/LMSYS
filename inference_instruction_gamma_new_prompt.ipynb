{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06762a62-6035-4580-a70a-8578f5a58fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "from typing import Optional, Union\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    EarlyStoppingCallback,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForMultipleChoice,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    RobertaForMultipleChoice,\n",
    "    AutoModelForSequenceClassification,\n",
    "    LlamaModel,\n",
    "    LlamaForSequenceClassification,\n",
    "    BitsAndBytesConfig,\n",
    "    get_polynomial_decay_schedule_with_warmup,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    TrainerCallback,\n",
    ")\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "\n",
    "from peft import (\n",
    "    get_peft_config,\n",
    "    PeftModel,\n",
    "    PeftConfig,\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    ")\n",
    "import os\n",
    "\n",
    "import random\n",
    "def seed_everything(seed=None):\n",
    "    '''\n",
    "    固定seed\n",
    "    :param seed: int, 随机种子\n",
    "    '''\n",
    "    max_seed_value = np.iinfo(np.uint32).max\n",
    "    min_seed_value = np.iinfo(np.uint32).min\n",
    "\n",
    "    if (seed is None) or not (min_seed_value <= seed <= max_seed_value):\n",
    "        seed = random.randint(np.iinfo(np.uint32).min, np.iinfo(np.uint32).max)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    return seed\n",
    "\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c616174-6786-4811-8ba1-9354514fc45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_values(A, B, a_space, b_space, ex_space):\n",
    "    # 计算A和a_space的差值\n",
    "    a_diff = a_space - A\n",
    "    b_diff = b_space - B\n",
    "    \n",
    "    # 第一种情况：A小于a_space，B小于b_space\n",
    "    if A < a_space and B < b_space:\n",
    "        ex_space += a_diff + b_diff\n",
    "        return A, B, ex_space\n",
    "\n",
    "    # 第二种情况：如果A和B都各自大于自己的space\n",
    "    elif A > a_space and B > b_space:\n",
    "        total_extra_needed = (A - a_space) + (B - b_space)\n",
    "        if total_extra_needed > ex_space:\n",
    "            A = int(a_space + ex_space / 2)\n",
    "            B = int(b_space + ex_space / 2)\n",
    "            ex_space = 0\n",
    "        else:\n",
    "            a_space = A\n",
    "            b_space = B\n",
    "            ex_space -= total_extra_needed\n",
    "            \n",
    "        return A, B, ex_space\n",
    "        \n",
    "    # 第三种情况：A或者B其中有一个大于a_space, b_space\n",
    "    elif A > a_space or B > b_space:\n",
    "        # 如果A大于a_space但是B小于b_space\n",
    "        if A > a_space and B < b_space:\n",
    "            extra_needed = A - a_space\n",
    "            ex_space += b_space - B\n",
    "            #够用\n",
    "            if ex_space >= extra_needed:\n",
    "                ex_space -= extra_needed\n",
    "                \n",
    "            else:\n",
    "                #不够用\n",
    "                #b_space = B + available_space\n",
    "                A = a_space + ex_space\n",
    "                ex_space = 0\n",
    "\n",
    "        # 如果B大于b_space但是A小于a_space\n",
    "        elif B > b_space and A < a_space:\n",
    "            extra_needed = B - b_space\n",
    "            ex_space += a_space - A\n",
    "            \n",
    "            if ex_space >= extra_needed:\n",
    "                ex_space -= extra_needed\n",
    "                \n",
    "            else:\n",
    "                B = b_space + ex_space\n",
    "                ex_space = 0\n",
    "\n",
    "        return A, B, ex_space\n",
    "    \n",
    "\n",
    "def adjust(current_lengths, prompt_length_space=300, response_length_space=800):\n",
    "    prompt_length = current_lengths[0]\n",
    "    response_a_length = current_lengths[1]\n",
    "    response_b_length = current_lengths[2]\n",
    "    #先看prompt的额度\n",
    "    ex_space = max(0, prompt_length_space - prompt_length)\n",
    "    response_a_length, response_b_length, ex_space = adjust_values(response_a_length, response_b_length, response_length_space, response_length_space, ex_space)\n",
    "    prompt_length = min(prompt_length, prompt_length_space)\n",
    "    prompt_length += ex_space\n",
    "\n",
    "    return prompt_length, response_a_length, response_b_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d60939f7-fda3-47cd-883c-7ad7939a60e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class InstructionDataSet(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_source_length, max_target_length):\n",
    "        super(InstructionDataSet, self).__init__()\n",
    "        #self.data = data.sample(len(data), random_state=0).reset_index(drop=True)\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_source_length = max_source_length\n",
    "        self.max_target_length = max_target_length\n",
    "        # self.A_token = self.tokenizer.encode(text='A', add_special_tokens=False, truncation=True, )\n",
    "        # self.B_token = self.tokenizer.encode(text='B', add_special_tokens=False, truncation=True, )\n",
    "        # self.C_token = self.tokenizer.encode(text='C', add_special_tokens=False, truncation=True, )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        now_data = self.data.loc[index]\n",
    "        idx = now_data['id']\n",
    "        \n",
    "        over_max_length = now_data['over_max_length']\n",
    "        \n",
    "        templete_part1 = \"<start_of_turn>user\\nHere are two question-answering dialogues. Compare two model performance on answering question, determine which is better.\\n\\n\"\n",
    "        templete_part1_input_ids = self.tokenizer(text=templete_part1, add_special_tokens=True, padding=False)['input_ids']\n",
    "        \n",
    "        templete_part2 = \"\\n###options\\nA. Model A\\nB. Model B\\nC. Tie\\n<end_of_turn>\\n\"\n",
    "        templete_part2_input_ids = self.tokenizer(text=templete_part2, add_special_tokens=True, padding=False)['input_ids'][1:]\n",
    "        #print(f\"templete_part2 is {templete_part2_input_ids}\")\n",
    "        templete_part3 = \"<start_of_turn>model\\n\"\n",
    "        templete_part3_input_ids = self.tokenizer(text=templete_part3, add_special_tokens=True, padding=False)['input_ids'][1:]\n",
    "        \n",
    "        templete_part4_input_ids = self.tokenizer(text=\"\\n\\n\", add_special_tokens=False, padding=False)['input_ids']\n",
    "        \n",
    "        if over_max_length:\n",
    "            prompt = \"#Prompt\\n\" + now_data['overflow_prompt']\n",
    "            r_a = \"#Response\\n\" + \"##Model A\\n\" + now_data['overflow_response_a']\n",
    "            r_b = \"##Model B\\n\" + now_data['overflow_response_b']\n",
    "            \n",
    "            prompt_ids = self.tokenizer(text=prompt, add_special_tokens=False, truncation=False, padding=False)['input_ids']\n",
    "            model_a_input_ids = self.tokenizer(text=r_a, add_special_tokens=False, truncation=False, padding=False)['input_ids']\n",
    "            model_b_input_ids = self.tokenizer(text=r_b, add_special_tokens=False, truncation=False, padding=False)['input_ids']\n",
    "\n",
    "            if len(prompt_ids) + len(model_a_input_ids) + len(model_b_input_ids) <= self.max_source_length:\n",
    "                prompt_response_ids = prompt_ids + model_a_input_ids + model_b_input_ids\n",
    "            \n",
    "            else:\n",
    "                '''\n",
    "                prompt 和 response 按照 300， 800， 800\n",
    "                response 优先\n",
    "                多的再给prompt\n",
    "                '''\n",
    "                length = [len(prompt_ids), len(model_a_input_ids), len(model_b_input_ids)]\n",
    "                print(f\"before {len(prompt_ids) + len(model_a_input_ids) + len(model_b_input_ids)}\")\n",
    "                print(f\"before {length}\")\n",
    "                prompt_max_length, a_max_length, b_max_length = adjust(length)\n",
    "                \n",
    "                prompt_ids = prompt_ids[:prompt_max_length] + templete_part4_input_ids\n",
    "                model_a_input_ids = model_a_input_ids[:a_max_length] + templete_part4_input_ids\n",
    "                model_b_input_ids = model_b_input_ids[:b_max_length] + templete_part4_input_ids\n",
    "                \n",
    "                prompt_response_ids = prompt_ids + model_a_input_ids + model_b_input_ids\n",
    "                print(f\"after {[prompt_max_length, a_max_length, b_max_length]}\")\n",
    "                print(f\"after {len(prompt_response_ids)}\")\n",
    "        \n",
    "        else:\n",
    "            prompt_response = now_data['prompt_response']\n",
    "            #print(f\"id is {now_data['id']}\")\n",
    "            #print(prompt_response)\n",
    "            prompt_response_ids = self.tokenizer(text=prompt_response, add_special_tokens=True, truncation=True,\n",
    "                                              max_length=self.max_source_length, padding=False)['input_ids'][1:]\n",
    "            #print(prompt_response_ids)        \n",
    "            \n",
    "        input_ids = templete_part1_input_ids + prompt_response_ids + templete_part2_input_ids + templete_part3_input_ids\n",
    "        input_text = self.tokenizer.decode(input_ids, skip_special_tokens=False)\n",
    "        #print(f\"input is {self.tokenizer.decode(input_ids)}\")\n",
    "        return {\n",
    "            \"input_ids\": input_text,\n",
    "            \"id\": idx\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "506790b9-c1ff-44f8-84a2-d2a186593f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = {k: [item[k] for item in batch] for k in ('input_ids','id')}\n",
    "    #print(batch)\n",
    "    batch_input = tokenizer(\n",
    "        batch['input_ids'],\n",
    "        padding='longest',\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_LENGTH + 50\n",
    "    )\n",
    "    return batch_input, batch['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0208959-dfd7-4218-b958-a367f9521742",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6168/6168 [00:00<00:00, 11089.66it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils import load_split_data\n",
    "data_path = \"dataset/non_overlap/valid.json\"\n",
    "prompt_type = 3\n",
    "MAX_INPUT = 1900\n",
    "if_train = True\n",
    "split = False\n",
    "if_drop_duplicate = True\n",
    "keep = 'last'\n",
    "df_train , df_valid = load_split_data(data_path, prompt_type, MAX_INPUT, if_train, split, False, if_drop_duplicate, 'last')\n",
    "test = df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2a1d9e5-14ed-4ba4-b701-dc878f211a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def inference(model, test_dataloader):\n",
    "    test_predictions = []\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        batch_input, idx = batch\n",
    "        for k in batch_input.keys():\n",
    "            batch_input[k] = batch_input[k].to(device)\n",
    "        with torch.no_grad():\n",
    "            response = model.generate(**batch_input, max_new_tokens=1, return_dict_in_generate=True, output_scores=True)\n",
    "            score = response.scores[0]\n",
    "            A_prob, B_prob, C_prob = score[:,A_TOKEN_IDS], score[:,B_TOKEN_IDS], score[:,C_TOKEN_IDS]\n",
    "            #print(f\"A_prob shape is {A_prob.shape}\")\n",
    "            #logits = torch.Tensor([[A_prob,B_prob,C_prob]])\n",
    "            logits = torch.cat([A_prob, B_prob, C_prob], dim=-1)\n",
    "            logits = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "            #print(f\"logits is {logits.shape}\")\n",
    "            node_result = [[idx[i],logits[i]] for i in range(len(idx))]\n",
    "\n",
    "        test_predictions.append(node_result)\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5600585-ab23-4b1c-a723-7c6ab546e9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62cc96ee-74c1-4f9a-a50d-9a3598330116",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = 'google/gemma-2-9b-it'\n",
    "model_path = \"output/wise-glade-506/checkpoint-5400\"\n",
    "MAX_LENGTH = MAX_INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57af0d60-712e-4efa-b0cd-3731f3198ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "341bb8e4a02e4907b6c8e6a20980ce1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Gemma2ForCausalLM(\n",
       "      (model): Gemma2Model(\n",
       "        (embed_tokens): Embedding(256000, 3584, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-41): 42 x Gemma2DecoderLayer(\n",
       "            (self_attn): Gemma2SdpaAttention(\n",
       "              (q_proj): Linear4bit(\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (base_layer): Linear4bit(in_features=3584, out_features=4096, bias=False)\n",
       "              )\n",
       "              (k_proj): Linear4bit(\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (base_layer): Linear4bit(in_features=3584, out_features=2048, bias=False)\n",
       "              )\n",
       "              (v_proj): Linear4bit(\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (base_layer): Linear4bit(in_features=3584, out_features=2048, bias=False)\n",
       "              )\n",
       "              (o_proj): Linear4bit(\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=3584, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=3584, bias=False)\n",
       "              )\n",
       "              (rotary_emb): Gemma2RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Gemma2MLP(\n",
       "              (gate_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=3584, bias=False)\n",
       "              (act_fn): PytorchGELUTanh()\n",
       "            )\n",
       "            (input_layernorm): Gemma2RMSNorm()\n",
       "            (post_attention_layernorm): Gemma2RMSNorm()\n",
       "            (pre_feedforward_layernorm): Gemma2RMSNorm()\n",
       "            (post_feedforward_layernorm): Gemma2RMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): Gemma2RMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3584, out_features=256000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, truncation_side = 'left')\n",
    "config = AutoConfig.from_pretrained(base_model, trust_remote_code=True)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  \n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "base_model_0 = AutoModelForCausalLM.from_pretrained(base_model,\n",
    "                                                 config=config,\n",
    "                                                 quantization_config=bnb_config,\n",
    "                                                 torch_dtype=torch.float16,\n",
    "                                                 device_map=\"auto\",\n",
    "                                                 trust_remote_code=True)\n",
    "# base_model_0.config.pad_token_id = tokenizer.pad_token_id\n",
    "# base_model_0.resize_token_embeddings(len(tokenizer))\n",
    "new_model = model_path\n",
    "model0 = PeftModel.from_pretrained(base_model_0, new_model).to(device)\n",
    "#model0 = model0.merge_and_unload()\n",
    "model0.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae5a3f5e-455d-4318-ad64-02280a99aa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_TOKEN_IDS = tokenizer('A',add_special_tokens=True, truncation=True, max_length=1024)['input_ids'][1:]\n",
    "B_TOKEN_IDS = tokenizer('B',add_special_tokens=True, truncation=True, max_length=1024)['input_ids'][1:]\n",
    "C_TOKEN_IDS = tokenizer('C',add_special_tokens=True, truncation=True, max_length=1024)['input_ids'][1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d6ba1af6-413e-4fdb-9357-9d7094743e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([235280], [235305], [235288])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_TOKEN_IDS,B_TOKEN_IDS,C_TOKEN_IDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6316fe95-7d22-4033-a697-1852f00750fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# tokenized_dataset = InstructionDataSet(test, tokenizer, MAX_LENGTH, 1)\n",
    "\n",
    "# token_ids = tokenized_dataset[0]['input_ids']\n",
    "# tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "\n",
    "# for token, token_id in zip(tokens, token_ids):\n",
    "#     print(f\"Token: {token}, ID: {token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d05711e4-02ae-430c-b7fc-1c6a789f16dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['length'] = test['prompt_response'].apply(lambda x: len(x.split(\" \")))\n",
    "test = test.sort_values(by = ['length']).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bd6cf97a-1ad2-4cdb-92c1-17690144d40d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "tokenized_dataset = InstructionDataSet(test, tokenizer, MAX_LENGTH, 1)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(tokenized_dataset, batch_size = batch_size ,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e194ff9-be6d-4472-a28b-b30b91a334a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 1219/1236 [12:50<00:24,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before 2254\n",
      "before [799, 1282, 173]\n",
      "after [445, 1282, 173]\n",
      "after 1903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 1220/1236 [12:52<00:22,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before 1945\n",
      "before [630, 662, 653]\n",
      "after [585, 662, 653]\n",
      "after 1903\n",
      "before 1951\n",
      "before [834, 567, 550]\n",
      "after [783, 567, 550]\n",
      "after 1903\n",
      "before 2061\n",
      "before [951, 603, 507]\n",
      "after [790, 603, 507]\n",
      "after 1903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 1221/1236 [12:53<00:21,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before 2204\n",
      "before [972, 689, 543]\n",
      "after [668, 689, 543]\n",
      "after 1903\n",
      "before 2064\n",
      "before [1633, 217, 214]\n",
      "after [1469, 217, 214]\n",
      "after 1903\n",
      "before 2783\n",
      "before [1967, 517, 299]\n",
      "after [1084, 517, 299]\n",
      "after 1903\n",
      "before 2022\n",
      "before [521, 656, 845]\n",
      "after [399, 656, 845]\n",
      "after 1903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 1222/1236 [12:55<00:20,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before 4866\n",
      "before [4219, 642, 5]\n",
      "after [1253, 642, 5]\n",
      "after 1903\n",
      "before 2122\n",
      "before [990, 574, 558]\n",
      "after [768, 574, 558]\n",
      "after 1903\n",
      "before 2117\n",
      "before [476, 797, 844]\n",
      "after [300, 797, 803]\n",
      "after 1903\n",
      "before 2467\n",
      "before [740, 962, 765]\n",
      "after [300, 835, 765]\n",
      "after 1903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 1223/1236 [12:56<00:18,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before 2907\n",
      "before [1107, 932, 868]\n",
      "after [300, 800, 800]\n",
      "after 1903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 1224/1236 [12:58<00:17,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before 2050\n",
      "before [390, 890, 770]\n",
      "after [300, 830, 770]\n",
      "after 1903\n",
      "before 2427\n",
      "before [740, 928, 759]\n",
      "after [300, 841, 759]\n",
      "after 1903\n",
      "before 2020\n",
      "before [163, 1045, 812]\n",
      "after [163, 868, 868]\n",
      "after 1846\n"
     ]
    }
   ],
   "source": [
    "sub_pred = inference(model = model0, test_dataloader = test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4663b87-6701-4705-8e0a-346cc43aa7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if batch_size != 1:\n",
    "    t = [j for i in sub_pred for j in i]\n",
    "    # 提取数据\n",
    "    processed_data = []\n",
    "    for item in t:\n",
    "        #item = item[0]\n",
    "        id = item[0].item()  # 获取id\n",
    "        array_values = item[1].tolist()  # 获取array并转换为列表\n",
    "        processed_data.append([id] + array_values)\n",
    "    \n",
    "\n",
    "else:\n",
    "    # 提取数据\n",
    "    processed_data = []\n",
    "    for item in sub_pred:\n",
    "        item = item[0]\n",
    "        id = item[0].item()  # 获取id\n",
    "        array_values = item[1].tolist()  # 获取array并转换为列表\n",
    "        processed_data.append([id] + array_values)\n",
    "\n",
    "new_columns = ['id', 'winner_model_a', 'winner_model_b', 'winner_tie']\n",
    "df = pd.DataFrame(processed_data, columns=new_columns)\n",
    "df = df.groupby('id').mean().reset_index()\n",
    "str2num = {'A':0, \"B\":1, \"C\":2}\n",
    "test['label_number'] = test.label.map(str2num)\n",
    "from sklearn.metrics import log_loss\n",
    "prediction = np.array(df[new_columns[1:]])\n",
    "test = test.drop_duplicates(subset = ['id']).reset_index(drop = True)\n",
    "test = test.sort_values(by = ['id']).reset_index(drop = True)\n",
    "print(f\"model {model_path}\\nscore: {log_loss(test.label_number, prediction)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a95c11-78b3-475e-b51a-b6311080a5bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
