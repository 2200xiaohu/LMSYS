{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fb2180d-f52a-4427-9475-4ada7accd2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from typing import Optional, Union\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    EarlyStoppingCallback,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForMultipleChoice,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    RobertaForMultipleChoice,\n",
    "    AutoModelForSequenceClassification,\n",
    "    LlamaModel,\n",
    "    LlamaForSequenceClassification,\n",
    "    BitsAndBytesConfig,\n",
    "    get_polynomial_decay_schedule_with_warmup,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    TrainerCallback,\n",
    "    AutoModel\n",
    ")\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "\n",
    "from peft import (\n",
    "    get_peft_config,\n",
    "    PeftModel,\n",
    "    PeftConfig,\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "import os\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b745e63d-04ea-4f09-ab0b-27a92772af68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d737d466e1b4bce9579ad68e1dcaffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL = 'google/gemma-2-9b-it'\n",
    "\n",
    "config = AutoConfig.from_pretrained(MODEL, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True, truncation_side = 'left')\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  \n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "model = AutoModel.from_pretrained(MODEL,\n",
    "                                 config=config,\n",
    "                                 quantization_config=bnb_config,\n",
    "                                 torch_dtype=torch.bfloat16,\n",
    "                                 device_map=\"auto\",\n",
    "                                 trust_remote_code=True,\n",
    "                                 attn_implementation='eager')\n",
    "peft_config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,  # For sequence classification\n",
    "            inference_mode=False,\n",
    "            r=32,\n",
    "            lora_alpha=64,\n",
    "            lora_dropout=0.1,\n",
    "            #bias = 'none',\n",
    "            target_modules=['q_proj','k_proj','v_proj','o_proj'] #,\n",
    "        )\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6925ddec-4cf1-4928-8e1f-98ba17fa43f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Gemma2Model(\n",
       "      (embed_tokens): Embedding(256000, 3584, padding_idx=0)\n",
       "      (layers): ModuleList(\n",
       "        (0-41): 42 x Gemma2DecoderLayer(\n",
       "          (self_attn): Gemma2Attention(\n",
       "            (q_proj): Linear4bit(\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=3584, out_features=32, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (base_layer): Linear4bit(in_features=3584, out_features=4096, bias=False)\n",
       "            )\n",
       "            (k_proj): Linear4bit(\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=3584, out_features=32, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=32, out_features=2048, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (base_layer): Linear4bit(in_features=3584, out_features=2048, bias=False)\n",
       "            )\n",
       "            (v_proj): Linear4bit(\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=3584, out_features=32, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=32, out_features=2048, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (base_layer): Linear4bit(in_features=3584, out_features=2048, bias=False)\n",
       "            )\n",
       "            (o_proj): Linear4bit(\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=32, out_features=3584, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (base_layer): Linear4bit(in_features=4096, out_features=3584, bias=False)\n",
       "            )\n",
       "            (rotary_emb): Gemma2RotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Gemma2MLP(\n",
       "            (gate_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n",
       "            (up_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n",
       "            (down_proj): Linear4bit(in_features=14336, out_features=3584, bias=False)\n",
       "            (act_fn): PytorchGELUTanh()\n",
       "          )\n",
       "          (input_layernorm): Gemma2RMSNorm()\n",
       "          (post_attention_layernorm): Gemma2RMSNorm()\n",
       "          (pre_feedforward_layernorm): Gemma2RMSNorm()\n",
       "          (post_feedforward_layernorm): Gemma2RMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): Gemma2RMSNorm()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbb8f993-9186-4901-806f-465ad94eaf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tokenizer(\"Hello Who are you\",return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88762750-8846-45d5-90b0-84ade4fe2d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   2, 4521, 7702,  708,  692]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3154701d-cb87-473d-9bd0-fb1da1634562",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "318dd70a-bfe9-4bf4-bcd8-08aa92a66f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 3584])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "81afcd5e-fdfc-4727-aff4-29bea275db84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 256000])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.loss['logits'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0335763a-7813-4595-8ec1-f1b1d355b675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 256000])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "70f2d763-8eb5-462d-8b3d-f955bf2f9f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-26.2500,  17.5000,  21.2500,  ..., -10.4375,  -3.5469, -14.0625],\n",
       "         [-24.8750,   1.1641, -15.9375,  ..., -14.4375, -11.8125, -25.7500]]],\n",
       "       device='cuda:0', grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.loss['logits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e067a7b4-9138-48a5-85cd-9cd682d5bd97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-26.2500,  17.5000,  21.2500,  ..., -10.4375,  -3.5469, -14.0625],\n",
       "         [-24.8750,   1.1641, -15.9375,  ..., -14.4375, -11.8125, -25.7500]]],\n",
       "       device='cuda:0', grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2d79eb3-eaec-45a0-9dae-73823ad77515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3584"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df94cadc-c857-42f7-9d82-9be2b1605ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.randint(0, 100, (32, 50))  # 示例输入\n",
    "attention_mask = torch.ones((32, 50))  # 示例attention mask\n",
    "\n",
    "# 确保输入张量在正确的设备上\n",
    "input_ids = input_ids.to(next(model.parameters()).device)\n",
    "attention_mask = attention_mask.to(next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dc18125-4e58-4881-abe0-cf673a0acc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaLSTM(nn.Module):\n",
    "    def __init__(self, gemma, lstm_hidden_size, num_classes):\n",
    "        super(GemmaLSTM, self).__init__()\n",
    "        self.gemma = gemma\n",
    "        self.lstm = nn.LSTM(gemma.config.hidden_size, lstm_hidden_size, batch_first=True)\n",
    "        self.classifier = nn.Linear(lstm_hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.gemma(input_ids, attention_mask)\n",
    "        hidden_states = output.last_hidden_state\n",
    "        print(hidden_states)\n",
    "        lstm_output, _ = self.lstm(hidden_states)\n",
    "        lstm_output = lstm_output[:,-1,:]\n",
    "        logits = self.classifier(lstm_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68489bae-c91e-4c54-8144-879d9b52c068",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom = GemmaLSTM(model, 1024, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df2cabf7-314d-4f4f-bfb8-cf40707ee6a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 't' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mt\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 't' is not defined"
     ]
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bbb65c9-5142-4cba-80b4-c77982b1963a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.1406,  2.8438,  0.3203,  ...,  0.3535,  0.0669,  3.7500],\n",
      "         [ 1.2656,  3.2656,  0.4004,  ...,  0.3711,  0.0369,  3.6875],\n",
      "         [ 1.1172,  3.0312,  0.6602,  ..., -0.4629,  0.2988,  3.4531],\n",
      "         ...,\n",
      "         [ 0.3906,  1.5781, -1.3828,  ..., -1.6953,  1.2500,  2.8750],\n",
      "         [ 0.0173,  2.0312, -1.2422,  ..., -1.0859,  1.2031,  3.0938],\n",
      "         [ 0.1260,  2.1719, -1.2109,  ..., -1.2109,  1.6250,  2.8281]],\n",
      "\n",
      "        [[ 1.2031,  3.5781,  0.7305,  ..., -0.1182,  0.1631,  4.2500],\n",
      "         [ 1.1719,  3.7031,  0.8047,  ..., -0.2334,  0.0215,  4.3438],\n",
      "         [ 0.9180,  3.2969,  1.3516,  ..., -0.3672,  0.5586,  3.5469],\n",
      "         ...,\n",
      "         [ 2.5156,  5.0312, -1.0000,  ..., -2.2031,  2.2969,  3.2812],\n",
      "         [ 2.0156,  6.0312, -1.2422,  ..., -1.6250,  2.2188,  3.0938],\n",
      "         [ 1.7188,  6.7812, -1.5469,  ..., -2.6562,  2.4844,  4.1250]],\n",
      "\n",
      "        [[ 1.3203,  3.5625,  0.6094,  ...,  0.0215, -0.0214,  4.2500],\n",
      "         [ 1.3047,  3.8906,  0.8008,  ..., -0.0977, -0.0084,  4.2500],\n",
      "         [ 1.2500,  3.7031,  0.8398,  ..., -0.2461, -0.0332,  4.1562],\n",
      "         ...,\n",
      "         [ 1.5859,  5.5938,  1.1719,  ..., -2.0000,  2.2969,  5.7188],\n",
      "         [ 1.3281,  6.8125,  0.7344,  ..., -1.2422,  2.0312,  5.6875],\n",
      "         [ 2.7812,  4.5625,  0.9453,  ..., -1.5469,  2.1250,  5.5000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.0312,  3.5781,  0.7383,  ..., -0.1079, -0.0129,  4.3125],\n",
      "         [ 1.0625,  3.5781,  0.7305,  ..., -0.1143, -0.0515,  4.2812],\n",
      "         [ 1.0703,  4.5938,  1.7109,  ..., -0.0530,  0.2012,  3.0625],\n",
      "         ...,\n",
      "         [ 2.2031,  7.8750, -1.9141,  ..., -0.1924,  0.2891,  3.0156],\n",
      "         [ 1.9219,  7.5312, -0.7344,  ..., -0.8906, -0.1299,  3.7500],\n",
      "         [ 1.4062,  7.4375, -1.2891,  ..., -1.5234,  0.8359,  3.2031]],\n",
      "\n",
      "        [[-1.5312, -0.7383,  0.1060,  ...,  3.7812, -1.1797,  6.3438],\n",
      "         [-0.2393,  4.6875,  0.4746,  ...,  0.3379,  0.1553,  4.2812],\n",
      "         [-0.3105,  3.9375,  1.0547,  ..., -1.2188,  0.6562,  2.6875],\n",
      "         ...,\n",
      "         [ 1.2344,  3.7188, -1.8750,  ..., -3.0000,  1.5469,  2.4844],\n",
      "         [ 1.0000,  2.3125, -1.6562,  ..., -3.2188,  1.8516,  2.6875],\n",
      "         [ 0.6992,  2.1875, -1.5312,  ..., -3.2500,  2.0469,  2.1562]],\n",
      "\n",
      "        [[ 0.5078, -3.8594, -5.5938,  ...,  5.4062,  4.2188,  3.9688],\n",
      "         [ 0.9609,  4.3750, -2.2188,  ...,  1.5234,  0.5430,  4.4688],\n",
      "         [ 0.8984,  3.2500,  0.1377,  ...,  0.0894,  0.8164,  3.4219],\n",
      "         ...,\n",
      "         [ 1.5547,  5.0000,  0.4590,  ..., -1.2891,  0.8633,  4.8125],\n",
      "         [ 1.3125,  5.7812,  0.2520,  ..., -1.2734,  0.4219,  4.6562],\n",
      "         [ 1.2812,  6.3750,  0.3008,  ..., -1.5469,  0.2793,  5.0938]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input and parameter tensors are not at the same device, found input tensor at cuda:0 and parameter tensor at cpu",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcustom\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m, in \u001b[0;36mGemmaLSTM.forward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     11\u001b[0m idden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(hidden_states)\n\u001b[0;32m---> 13\u001b[0m lstm_output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m lstm_output \u001b[38;5;241m=\u001b[39m lstm_output[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:]\n\u001b[1;32m     15\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(lstm_output)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/rnn.py:879\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    876\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 879\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    882\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    883\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input and parameter tensors are not at the same device, found input tensor at cuda:0 and parameter tensor at cpu"
     ]
    }
   ],
   "source": [
    "custom(input_ids,attention_mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
