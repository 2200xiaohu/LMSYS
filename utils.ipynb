{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f717bef9-e3c0-4ded-b04f-22e8479c22bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from typing import Optional, Union\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    EarlyStoppingCallback,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForMultipleChoice,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    RobertaForMultipleChoice,\n",
    "    AutoModelForSequenceClassification,\n",
    "    LlamaModel,\n",
    "    LlamaForSequenceClassification,\n",
    "    BitsAndBytesConfig,\n",
    "    get_polynomial_decay_schedule_with_warmup,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    TrainerCallback,\n",
    ")\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "\n",
    "from peft import (\n",
    "    get_peft_config,\n",
    "    PeftModel,\n",
    "    PeftConfig,\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "import os\n",
    "\n",
    "import random\n",
    "\n",
    "def get_label(row):\n",
    "    label = [idx for idx, option in enumerate(['winner_model_a','winner_model_b','winner_tie']) if row[option] == 1]\n",
    "    if label[-1] == 0:\n",
    "        return 'A'\n",
    "    elif label[-1] == 1:\n",
    "        return 'B'\n",
    "    else:\n",
    "        return 'C'\n",
    "    return label[-1]\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9545ccc-f70c-4f4b-96bd-de8dd97c0ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(\"dataset/non_overlap/train_subset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ccf03ef-1416-45ce-9054-b1dc4669bdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate prompt-response\n",
    "data = data.explode(['prompt','response_a','response_b']).reset_index(drop = True)\n",
    "\n",
    "data['label'] = data.apply(lambda x: get_label(x), axis = 1)\n",
    "\n",
    "data = data.fillna('None')\n",
    "data['response_a'] = data['response_a'].apply(lambda x: 'None' if len(x)==0 else x)\n",
    "data['response_b'] = data['response_b'].apply(lambda x: 'None' if len(x)==0 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "703240ab-2bd9-49d6-b1ed-381ae1d8b3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'google/gemma-2-9b-it'\n",
    "config = AutoConfig.from_pretrained(MODEL, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True, truncation_side = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c1f3c37b-b913-4ad7-bbc5-27760f3799d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "templete_part1 = \"<start_of_turn>user\\nHere are two question-answering dialogues. Compare two model performance on answering question, determine which is better.\\n\\n\"\n",
    "templete_part1_input_ids = tokenizer(text=templete_part1, add_special_tokens=True, padding=False)['input_ids']\n",
    "\n",
    "templete_part2 = \"\\n###options\\nA. Model A\\nB. Model B\\nC. Tie\\n<end_of_turn>\\n\"\n",
    "templete_part2_input_ids = tokenizer(text=templete_part2, add_special_tokens=False, padding=False)['input_ids']\n",
    "#print(f\"templete_part2 is {templete_part2_input_ids}\")\n",
    "templete_part3 = \"<start_of_turn>model\\n\"\n",
    "templete_part3_input_ids = tokenizer(text=templete_part3, add_special_tokens=False, padding=False)['input_ids']\n",
    "\n",
    "use_in_prompt_1 = tokenizer(text=\"#Prompt\\n\", add_special_tokens=False, padding=False)['input_ids']\n",
    "use_in_prompt_2 = tokenizer(text=\"\\n\\n\" + \"#Response\\n\" + \"##Model A\\n\", add_special_tokens=False, padding=False)['input_ids']\n",
    "use_in_prompt_3 = tokenizer(text=\"\\n\\n\" + \"##Model B\\n\", add_special_tokens=False, padding=False)['input_ids']\n",
    "\n",
    "templete_part4_input_ids = tokenizer(text=\"\\n\\n\", add_special_tokens=False, padding=False)['input_ids']\n",
    "eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce7ebfb7-da36-456c-95a7-21ae0f72efe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(1000).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae5266fc-585f-49a0-88ff-a665131216f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(row, tokenizer):\n",
    "\n",
    "    now_data = row\n",
    "    response_a = row['response_a']\n",
    "    response_a_input_ids = tokenizer(text=response_a, add_special_tokens=False, padding=False)['input_ids']\n",
    "    row['response_a_input_ids'] = response_a_input_ids\n",
    "    \n",
    "    response_b = row['response_b']\n",
    "    response_b_input_ids = tokenizer(text=response_b, add_special_tokens=False, padding=False)['input_ids']\n",
    "    row['response_b_input_ids'] = response_b_input_ids\n",
    "    \n",
    "    prompt = row['prompt']\n",
    "    prompt_input_ids = tokenizer(text=prompt, add_special_tokens=False, padding=False)['input_ids']\n",
    "    row['prompt_input_ids'] = prompt_input_ids\n",
    "    \n",
    "    label = now_data['label']\n",
    "    label_ids = tokenizer.encode(text=label, add_special_tokens=False)\n",
    "    row['label_ids'] = label_ids\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6e465ee-94c2-40e5-9064-75b59be4ab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.apply(lambda x: tokenize(x, tokenizer), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f0b0c047-62c0-4051-9e57-40edc52291cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_values(A, B, a_space, b_space, ex_space):\n",
    "    # 计算A和a_space的差值\n",
    "    a_diff = a_space - A\n",
    "    b_diff = b_space - B\n",
    "    \n",
    "    # 第一种情况：A小于a_space，B小于b_space\n",
    "    if A < a_space and B < b_space:\n",
    "        ex_space += a_diff + b_diff\n",
    "        return A, B, ex_space\n",
    "\n",
    "    # 第二种情况：如果A和B都各自大于自己的space\n",
    "    elif A > a_space and B > b_space:\n",
    "        total_extra_needed = (A - a_space) + (B - b_space)\n",
    "        if total_extra_needed > ex_space:\n",
    "            A = int(a_space + ex_space / 2)\n",
    "            B = int(b_space + ex_space / 2)\n",
    "            ex_space = 0\n",
    "        else:\n",
    "            a_space = A\n",
    "            b_space = B\n",
    "            ex_space -= total_extra_needed\n",
    "            \n",
    "        return A, B, ex_space\n",
    "        \n",
    "    # 第三种情况：A或者B其中有一个大于a_space, b_space\n",
    "    elif A >= a_space or B >= b_space:\n",
    "        # 如果A大于a_space但是B小于b_space\n",
    "        if A >= a_space and B <= b_space:\n",
    "            extra_needed = A - a_space\n",
    "            ex_space += b_space - B\n",
    "            #够用\n",
    "            if ex_space >= extra_needed:\n",
    "                ex_space -= extra_needed\n",
    "                \n",
    "            else:\n",
    "                #不够用\n",
    "                #b_space = B + available_space\n",
    "                A = a_space + ex_space\n",
    "                ex_space = 0\n",
    "\n",
    "        # 如果B大于b_space但是A小于a_space\n",
    "        elif B > b_space and A < a_space:\n",
    "            extra_needed = B - b_space\n",
    "            ex_space += a_space - A\n",
    "            \n",
    "            if ex_space >= extra_needed:\n",
    "                ex_space -= extra_needed\n",
    "                \n",
    "            else:\n",
    "                B = b_space + ex_space\n",
    "                ex_space = 0\n",
    "\n",
    "        return A, B, ex_space\n",
    "    \n",
    "\n",
    "def adjust(current_lengths, prompt_length_space=300, response_length_space=800):\n",
    "    prompt_length = current_lengths[0]\n",
    "    response_a_length = current_lengths[1]\n",
    "    response_b_length = current_lengths[2]\n",
    "    #先看prompt的额度\n",
    "    ex_space = max(0, prompt_length_space - prompt_length)\n",
    "    response_a_length, response_b_length, ex_space = adjust_values(response_a_length, response_b_length, response_length_space, response_length_space, ex_space)\n",
    "    prompt_length = min(prompt_length, prompt_length_space)\n",
    "    prompt_length += ex_space\n",
    "\n",
    "    return prompt_length, response_a_length, response_b_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6bdca9ea-4263-42d9-b86b-486dd747d8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def over_max_length(prompt_input_ids, model_a_input_ids, model_b_input_ids, max_length):\n",
    "    '''\n",
    "    单条超出max length\n",
    "    '''\n",
    "    length = [len(prompt_input_ids), len(model_a_input_ids), len(model_b_input_ids)]\n",
    "    prompt_length = int(max_length // 5)\n",
    "    response_length = int((max_length - prompt_length) // 2)\n",
    "    prompt_max_length, a_max_length, b_max_length = adjust(length, prompt_length, response_length)\n",
    "    prompt_ids = prompt_input_ids[:prompt_max_length] + templete_part4_input_ids\n",
    "    model_a_input_ids = model_a_input_ids[:a_max_length] + templete_part4_input_ids\n",
    "    model_b_input_ids = model_b_input_ids[:b_max_length] + templete_part4_input_ids\n",
    "    prompt_response_ids = prompt_ids + model_a_input_ids + model_b_input_ids\n",
    "    return prompt_response_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "66796657-13bd-4dc7-9ec8-484d4cbe87cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_template(prompt_response_ids):\n",
    "    input_ids = templete_part1_input_ids + prompt_response_ids + templete_part2_input_ids + templete_part3_input_ids\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3a1dc2-ab29-402b-8218-5fcb939f7c37",
   "metadata": {},
   "source": [
    "# make prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8a5a2bfc-cc01-4d85-9fa0-dbd3c54cbebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_3(data, max_length, if_train):\n",
    "    data = data.iloc[::-1].reset_index(drop = True)#反转\n",
    "    prompt_response = []\n",
    "    ids = []\n",
    "    labels = []\n",
    "    #只有一种可能会超出max length：\n",
    "    #单条的prompt和reponse加在一起超出max length\n",
    "    \n",
    "    text_length = 0\n",
    "    for idx, row in tqdm(data.iterrows(), total=len(data)):\n",
    "        # data['prompt_response_ids'] = use_in_prompt_1 + data['prompt_input_ids'] + use_in_prompt_2 + data['response_a_input_ids'] + use_in_prompt_3 + data['response_b_input_ids']\n",
    "        # text = row['prompt_response_ids']\n",
    "        text = use_in_prompt_1 + row['prompt_input_ids'] + use_in_prompt_2 + row['response_a_input_ids'] + use_in_prompt_3 + row['response_b_input_ids']\n",
    "        response_a = row['response_a_input_ids']\n",
    "        response_b = row['response_b_input_ids']\n",
    "        prompt = row['prompt_input_ids']\n",
    "        id = row['id']\n",
    "        \n",
    "        if if_train:\n",
    "            label = row['label_ids']\n",
    "        \n",
    "        if id not in ids:\n",
    "            #第一次出现\n",
    "            text_length = len(text)\n",
    "            ids.append(id)\n",
    "            if if_train:\n",
    "                labels.append(label)\n",
    "            if text_length > max_length:\n",
    "                text = over_max_length(prompt_input_ids = prompt, model_a_input_ids = response_a, model_b_input_ids = response_b, max_length = max_length)\\\n",
    "                \n",
    "            text = add_template(text)\n",
    "            prompt_response.append(text)\n",
    "        \n",
    "        else:\n",
    "            text_length += len(text)\n",
    "            if text_length <= max_length:\n",
    "                #取上一个text出来，合并后替换\n",
    "                text = text + templete_part4_input_ids + prompt_response[-1]\n",
    "                prompt_response[-1] = text\n",
    "                \n",
    "            else:\n",
    "                #另一起一行\n",
    "                text_length = len(text)\n",
    "                ids.append(id)\n",
    "                \n",
    "                if if_train:\n",
    "                    labels.append(label)\n",
    "                    \n",
    "                #另起一行但超出长度\n",
    "                if text_length > max_length:\n",
    "                    text = over_max_length(prompt_input_ids = prompt, model_a_input_ids = response_a, model_b_input_ids = response_b, max_length = max_length)\n",
    "                \n",
    "                text = add_template(text)\n",
    "                prompt_response.append(text)\n",
    "                    \n",
    "                \n",
    "                    \n",
    "    if if_train:           \n",
    "        data = pd.DataFrame({'id': ids, 'prompt_response': prompt_response, \"label\": labels})\n",
    "        data = data.iloc[::-1].reset_index(drop = True)#反转\n",
    "    else:\n",
    "        data = pd.DataFrame({'id': ids, 'prompt_response': prompt_response})\n",
    "        data = data.iloc[::-1].reset_index(drop = True)#反转\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0f05e2cd-44d2-4794-b541-672844992cc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 8356.32it/s]\n"
     ]
    }
   ],
   "source": [
    "final = prompt_3(data, max_length = 1900, if_train = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e4e7ffc7-3779-4cf4-986c-d18d53ae59d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "final['length'] = final['prompt_response'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "93d2ce92-006b-415e-a952-03e5d1cc088b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt_response</th>\n",
       "      <th>label</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2639720283</td>\n",
       "      <td>[2, 106, 1645, 108, 4858, 708, 1378, 2872, 235...</td>\n",
       "      <td>[235305]</td>\n",
       "      <td>1953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>685334275</td>\n",
       "      <td>[2, 106, 1645, 108, 4858, 708, 1378, 2872, 235...</td>\n",
       "      <td>[235280]</td>\n",
       "      <td>1953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>1383698196</td>\n",
       "      <td>[2, 106, 1645, 108, 4858, 708, 1378, 2872, 235...</td>\n",
       "      <td>[235280]</td>\n",
       "      <td>1953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>3837487706</td>\n",
       "      <td>[2, 106, 1645, 108, 4858, 708, 1378, 2872, 235...</td>\n",
       "      <td>[235280]</td>\n",
       "      <td>1953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>3712274856</td>\n",
       "      <td>[2, 106, 1645, 108, 4858, 708, 1378, 2872, 235...</td>\n",
       "      <td>[235280]</td>\n",
       "      <td>1953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>2581731284</td>\n",
       "      <td>[2, 106, 1645, 108, 4858, 708, 1378, 2872, 235...</td>\n",
       "      <td>[235288]</td>\n",
       "      <td>1953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>3071276906</td>\n",
       "      <td>[2, 106, 1645, 108, 4858, 708, 1378, 2872, 235...</td>\n",
       "      <td>[235305]</td>\n",
       "      <td>1953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>4097865015</td>\n",
       "      <td>[2, 106, 1645, 108, 4858, 708, 1378, 2872, 235...</td>\n",
       "      <td>[235288]</td>\n",
       "      <td>1953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                    prompt_response     label  \\\n",
       "70   2639720283  [2, 106, 1645, 108, 4858, 708, 1378, 2872, 235...  [235305]   \n",
       "255   685334275  [2, 106, 1645, 108, 4858, 708, 1378, 2872, 235...  [235280]   \n",
       "376  1383698196  [2, 106, 1645, 108, 4858, 708, 1378, 2872, 235...  [235280]   \n",
       "466  3837487706  [2, 106, 1645, 108, 4858, 708, 1378, 2872, 235...  [235280]   \n",
       "827  3712274856  [2, 106, 1645, 108, 4858, 708, 1378, 2872, 235...  [235280]   \n",
       "851  2581731284  [2, 106, 1645, 108, 4858, 708, 1378, 2872, 235...  [235288]   \n",
       "858  3071276906  [2, 106, 1645, 108, 4858, 708, 1378, 2872, 235...  [235305]   \n",
       "976  4097865015  [2, 106, 1645, 108, 4858, 708, 1378, 2872, 235...  [235288]   \n",
       "\n",
       "     length  \n",
       "70     1953  \n",
       "255    1953  \n",
       "376    1953  \n",
       "466    1953  \n",
       "827    1953  \n",
       "851    1953  \n",
       "858    1953  \n",
       "976    1953  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = final['length'] == final['length'].max()\n",
    "final.loc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f5146e95-2b48-42d1-84e9-805f0fd4e947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Here are two question-answering dialogues. Compare two model performance on answering question, determine which is better.\n",
      "\n",
      "#Prompt\n",
      "What is discursive reasoning\n",
      "\n",
      "#Response\n",
      "##Model A\n",
      "Discursive reasoning is a type of reasoning that involves making arguments and defending them using logical reasoning. It involves constructing a series of statements or claims that can be challenged and supported by evidence. Discursive reasoning is often used in debates and discussions, and is a key part of critical thinking.\n",
      "\n",
      "##Model B\n",
      "Hello! I'm here to help answer your questions safely and helpfully. Discursive reasoning is a type of thinking and reasoning that involves evaluating and analyzing information, arguments, and beliefs in a systematic and logical way. It involves considering multiple perspectives, weighing evidence, and making connections between ideas.\n",
      "\n",
      "Discursive reasoning is an important skill for critical thinking, problem-solving, and effective communication. It can help you to make informed decisions, evaluate the validity of information, and understand complex issues from multiple angles.\n",
      "\n",
      "If you have any more questions or would like to know more about discursive reasoning, please don't hesitate to ask!\n",
      "###options\n",
      "A. Model A\n",
      "B. Model B\n",
      "C. Tie\n",
      "<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(final.prompt_response.values[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0ef9fe7-d8bc-47b0-8b21-4521e1e25063",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas(desc = 'pandas bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "538feeb4-06b4-4046-8e70-8056adbdcda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_v2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df467948-9661-48d7-9508-d59768465bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar: 100%|██████████| 55745/55745 [02:56<00:00, 315.25it/s]\n",
      "100%|██████████| 55745/55745 [00:19<00:00, 2818.03it/s]\n"
     ]
    }
   ],
   "source": [
    "tmp_train , _ = load_split_data(\"dataset/non_overlap/train_subset.json\", 3, 1900, True, False, False, False, 'last', 'google/gemma-2-9b-it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de766f88-fc6e-4908-9af7-c8924aa8471b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_train['length'] = tmp_train['prompt_response'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1678bab6-a168-4bf2-ad3e-788ad5617c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt_response</th>\n",
       "      <th>label</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>3254113</td>\n",
       "      <td>[235345, 55440, 108, 3611, 692, 1707, 3104, 48...</td>\n",
       "      <td>[235288]</td>\n",
       "      <td>1916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>35088788</td>\n",
       "      <td>[235345, 55440, 108, 24926, 5598, 476, 4866, 1...</td>\n",
       "      <td>[235288]</td>\n",
       "      <td>1916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>37697874</td>\n",
       "      <td>[235345, 55440, 108, 11071, 235292, 1646, 708,...</td>\n",
       "      <td>[235288]</td>\n",
       "      <td>1916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>43053669</td>\n",
       "      <td>[235345, 55440, 108, 156910, 889, 736, 5078, 2...</td>\n",
       "      <td>[235280]</td>\n",
       "      <td>1916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>48531611</td>\n",
       "      <td>[235345, 55440, 108, 108, 235345, 1915, 664, 1...</td>\n",
       "      <td>[235305]</td>\n",
       "      <td>1916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46553</th>\n",
       "      <td>4280501571</td>\n",
       "      <td>[235345, 55440, 108, 1638, 608, 7588, 8293, 61...</td>\n",
       "      <td>[235305]</td>\n",
       "      <td>1916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46570</th>\n",
       "      <td>4281980776</td>\n",
       "      <td>[235345, 55440, 108, 90822, 573, 2412, 3409, 2...</td>\n",
       "      <td>[235280]</td>\n",
       "      <td>1916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46637</th>\n",
       "      <td>4289338231</td>\n",
       "      <td>[235345, 55440, 108, 49688, 573, 2412, 2793, 7...</td>\n",
       "      <td>[235288]</td>\n",
       "      <td>1916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46648</th>\n",
       "      <td>4290839285</td>\n",
       "      <td>[235345, 55440, 108, 19584, 145739, 26399, 235...</td>\n",
       "      <td>[235305]</td>\n",
       "      <td>1916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46681</th>\n",
       "      <td>4293704939</td>\n",
       "      <td>[235345, 55440, 108, 20700, 6555, 235260, 577,...</td>\n",
       "      <td>[235305]</td>\n",
       "      <td>1916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>706 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                    prompt_response  \\\n",
       "39        3254113  [235345, 55440, 108, 3611, 692, 1707, 3104, 48...   \n",
       "416      35088788  [235345, 55440, 108, 24926, 5598, 476, 4866, 1...   \n",
       "447      37697874  [235345, 55440, 108, 11071, 235292, 1646, 708,...   \n",
       "497      43053669  [235345, 55440, 108, 156910, 889, 736, 5078, 2...   \n",
       "554      48531611  [235345, 55440, 108, 108, 235345, 1915, 664, 1...   \n",
       "...           ...                                                ...   \n",
       "46553  4280501571  [235345, 55440, 108, 1638, 608, 7588, 8293, 61...   \n",
       "46570  4281980776  [235345, 55440, 108, 90822, 573, 2412, 3409, 2...   \n",
       "46637  4289338231  [235345, 55440, 108, 49688, 573, 2412, 2793, 7...   \n",
       "46648  4290839285  [235345, 55440, 108, 19584, 145739, 26399, 235...   \n",
       "46681  4293704939  [235345, 55440, 108, 20700, 6555, 235260, 577,...   \n",
       "\n",
       "          label  length  \n",
       "39     [235288]    1916  \n",
       "416    [235288]    1916  \n",
       "447    [235288]    1916  \n",
       "497    [235280]    1916  \n",
       "554    [235305]    1916  \n",
       "...         ...     ...  \n",
       "46553  [235305]    1916  \n",
       "46570  [235280]    1916  \n",
       "46637  [235288]    1916  \n",
       "46648  [235305]    1916  \n",
       "46681  [235305]    1916  \n",
       "\n",
       "[706 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = tmp_train['length'] == tmp_train['length'].max()\n",
    "tmp_train.loc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c8d7b7c-ca3b-4864-9dde-e28e6ed74d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(\"dataset/non_overlap/train_subset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b33f8614-8c8f-40b3-ac40-c8086019e811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'please make it shorter, reddit style reply'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['id'] == 35088788].prompt.values[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8262f96b-c552-43d0-8d83-cc701024b20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt_response</th>\n",
       "      <th>label</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>35088788</td>\n",
       "      <td>[235345, 55440, 108, 24926, 5598, 476, 4866, 1...</td>\n",
       "      <td>[235288]</td>\n",
       "      <td>1916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>35088788</td>\n",
       "      <td>[235345, 55440, 108, 24926, 1501, 665, 25270, ...</td>\n",
       "      <td>[235288]</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                    prompt_response     label  \\\n",
       "416  35088788  [235345, 55440, 108, 24926, 5598, 476, 4866, 1...  [235288]   \n",
       "417  35088788  [235345, 55440, 108, 24926, 1501, 665, 25270, ...  [235288]   \n",
       "\n",
       "     length  \n",
       "416    1916  \n",
       "417     380  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_train[tmp_train['id'] == 35088788]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9185935-68f5-47b0-9581-6eefa795875c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1178"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tmp_train.prompt_response.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98d10d30-abe2-4549-aa91-a077edff5129",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'google/gemma-2-9b-it'\n",
    "config = AutoConfig.from_pretrained(MODEL, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True, truncation_side = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82b06d8b-7fcb-42cd-8fec-8d530ad50bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaTokenizerFast(name_or_path='google/gemma-2-9b-it', vocab_size=256000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='left', special_tokens={'bos_token': '<bos>', 'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<start_of_turn>', '<end_of_turn>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<eos>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"<bos>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t5: AddedToken(\"<2mass>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t6: AddedToken(\"[@BOS@]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t7: AddedToken(\"<unused0>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t8: AddedToken(\"<unused1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t9: AddedToken(\"<unused2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t10: AddedToken(\"<unused3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t11: AddedToken(\"<unused4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t12: AddedToken(\"<unused5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t13: AddedToken(\"<unused6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t14: AddedToken(\"<unused7>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t15: AddedToken(\"<unused8>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t16: AddedToken(\"<unused9>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t17: AddedToken(\"<unused10>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t18: AddedToken(\"<unused11>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t19: AddedToken(\"<unused12>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t20: AddedToken(\"<unused13>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t21: AddedToken(\"<unused14>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t22: AddedToken(\"<unused15>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t23: AddedToken(\"<unused16>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t24: AddedToken(\"<unused17>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t25: AddedToken(\"<unused18>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t26: AddedToken(\"<unused19>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t27: AddedToken(\"<unused20>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t28: AddedToken(\"<unused21>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t29: AddedToken(\"<unused22>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t30: AddedToken(\"<unused23>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t31: AddedToken(\"<unused24>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t32: AddedToken(\"<unused25>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t33: AddedToken(\"<unused26>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t34: AddedToken(\"<unused27>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t35: AddedToken(\"<unused28>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t36: AddedToken(\"<unused29>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t37: AddedToken(\"<unused30>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t38: AddedToken(\"<unused31>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t39: AddedToken(\"<unused32>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t40: AddedToken(\"<unused33>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t41: AddedToken(\"<unused34>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t42: AddedToken(\"<unused35>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t43: AddedToken(\"<unused36>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t44: AddedToken(\"<unused37>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t45: AddedToken(\"<unused38>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t46: AddedToken(\"<unused39>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t47: AddedToken(\"<unused40>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t48: AddedToken(\"<unused41>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t49: AddedToken(\"<unused42>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t50: AddedToken(\"<unused43>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t51: AddedToken(\"<unused44>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t52: AddedToken(\"<unused45>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t53: AddedToken(\"<unused46>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t54: AddedToken(\"<unused47>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t55: AddedToken(\"<unused48>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t56: AddedToken(\"<unused49>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t57: AddedToken(\"<unused50>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t58: AddedToken(\"<unused51>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t59: AddedToken(\"<unused52>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t60: AddedToken(\"<unused53>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t61: AddedToken(\"<unused54>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t62: AddedToken(\"<unused55>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t63: AddedToken(\"<unused56>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t64: AddedToken(\"<unused57>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t65: AddedToken(\"<unused58>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t66: AddedToken(\"<unused59>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t67: AddedToken(\"<unused60>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t68: AddedToken(\"<unused61>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t69: AddedToken(\"<unused62>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t70: AddedToken(\"<unused63>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t71: AddedToken(\"<unused64>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t72: AddedToken(\"<unused65>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t73: AddedToken(\"<unused66>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t74: AddedToken(\"<unused67>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t75: AddedToken(\"<unused68>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t76: AddedToken(\"<unused69>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t77: AddedToken(\"<unused70>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t78: AddedToken(\"<unused71>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t79: AddedToken(\"<unused72>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t80: AddedToken(\"<unused73>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t81: AddedToken(\"<unused74>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t82: AddedToken(\"<unused75>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t83: AddedToken(\"<unused76>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t84: AddedToken(\"<unused77>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t85: AddedToken(\"<unused78>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t86: AddedToken(\"<unused79>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t87: AddedToken(\"<unused80>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t88: AddedToken(\"<unused81>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t89: AddedToken(\"<unused82>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t90: AddedToken(\"<unused83>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t91: AddedToken(\"<unused84>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t92: AddedToken(\"<unused85>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t93: AddedToken(\"<unused86>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t94: AddedToken(\"<unused87>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t95: AddedToken(\"<unused88>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t96: AddedToken(\"<unused89>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t97: AddedToken(\"<unused90>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t98: AddedToken(\"<unused91>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t99: AddedToken(\"<unused92>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t100: AddedToken(\"<unused93>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t101: AddedToken(\"<unused94>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t102: AddedToken(\"<unused95>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t103: AddedToken(\"<unused96>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t104: AddedToken(\"<unused97>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t105: AddedToken(\"<unused98>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t106: AddedToken(\"<start_of_turn>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t107: AddedToken(\"<end_of_turn>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t108: AddedToken(\"\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t109: AddedToken(\"\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t110: AddedToken(\"\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t111: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t112: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t113: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t114: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t115: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t116: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t117: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t118: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t119: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t120: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t121: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t122: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t123: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t124: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t125: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t126: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t127: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t128: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t129: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t130: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t131: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t132: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t133: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t134: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t135: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t136: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t137: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t138: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t139: AddedToken(\"▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t140: AddedToken(\"▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t141: AddedToken(\"▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t142: AddedToken(\"▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t143: AddedToken(\"▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t144: AddedToken(\"▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t145: AddedToken(\"▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t146: AddedToken(\"▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t147: AddedToken(\"▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t148: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t149: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t150: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t152: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t153: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t154: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t155: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t156: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t157: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t158: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t159: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t160: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t161: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t162: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t163: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t164: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t165: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t166: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t167: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t168: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t169: AddedToken(\"<table>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t170: AddedToken(\"<caption>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t171: AddedToken(\"<thead>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t172: AddedToken(\"<tbody>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t173: AddedToken(\"<tfoot>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t174: AddedToken(\"<tr>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t175: AddedToken(\"<th>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t176: AddedToken(\"<td>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t177: AddedToken(\"</table>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t178: AddedToken(\"</caption>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t179: AddedToken(\"</thead>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t180: AddedToken(\"</tbody>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t181: AddedToken(\"</tfoot>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t182: AddedToken(\"</tr>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t183: AddedToken(\"</th>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t184: AddedToken(\"</td>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t185: AddedToken(\"<h1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t186: AddedToken(\"<h2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t187: AddedToken(\"<h3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t188: AddedToken(\"<h4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t189: AddedToken(\"<h5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t190: AddedToken(\"<h6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t191: AddedToken(\"<blockquote>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t192: AddedToken(\"</h1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t193: AddedToken(\"</h2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t194: AddedToken(\"</h3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t195: AddedToken(\"</h4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t196: AddedToken(\"</h5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t197: AddedToken(\"</h6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t198: AddedToken(\"</blockquote>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t199: AddedToken(\"<strong>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t200: AddedToken(\"<em>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t201: AddedToken(\"<b>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t202: AddedToken(\"<i>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t203: AddedToken(\"<u>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t204: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t205: AddedToken(\"<sub>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t206: AddedToken(\"<sup>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t207: AddedToken(\"<code>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t208: AddedToken(\"</strong>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t209: AddedToken(\"</em>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t210: AddedToken(\"</b>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t211: AddedToken(\"</i>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t212: AddedToken(\"</u>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t213: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t214: AddedToken(\"</sub>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t215: AddedToken(\"</sup>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t216: AddedToken(\"</code>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d8a13e3-6273-48b9-92ea-375e293cca94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Prompt\n",
      "please make it shorter, reddit style reply\n",
      "\n",
      "#Response\n",
      "##Model A\n",
      "Hey [username], I feel the same way as you regarding XMR and BCH. While getting delisted can be tough, it could also spur innovation. Excited to see what new ideas emerge! And I couldn't agree more about XMR's focus on privacy aligning with the Cypherpunk's Manifesto. As someone who values privacy, I'm glad XMR is becoming less of a speculator's game. I'm also selling stuff on Monero Market now! [Insert link] Let's work together towards a future where privacy is a right, not a luxury. 🙌🔒🔑 #XMR #Cypherpunks #PrivacyMatters #AnonymityRocks\n",
      "\n",
      "P.S. If you're interested in buying any of my items on Monero Market, check them out! [Insert link]\n",
      "\n",
      "##Model B\n",
      "Hey!\n",
      "\n",
      "Totally vibing with your take on XMR's situation! Watching Bitcoin's journey had me worried too, especially seeing how the quick-buck mindset overshadowed the real essence of crypto. It's refreshing to see someone else who values privacy and the core principles laid out in \"A Cypherpunk's Manifesto.\"\n",
      "\n",
      "Your optimism about Monero facing challenges head-on and sparking innovation is contagious. It's a solid reminder that sometimes, being cornered is what drives us to break new ground. And honestly, your excitement has nudged me to finally dive into Monero, starting by selling my collection on MoneroMarket. It feels like we're getting back to what crypto was supposed to be about – not speculation, but privacy and empowerment.\n",
      "\n",
      "Thanks for the inspiration! Let's see what amazing solutions this situation brings. Onward to a more private and decentralized future!\n",
      "\n",
      "Cheers!\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tmp_train.prompt_response.values[417]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
