train_data: 'dataset/random_train.csv'
valid_data: 'dataset/random_valid.csv'
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
learning_rate: 1e-5
lr_end: 2e-6
warmup_ratio: 0.1
num_train_epochs: 5
gradient_accumulation_steps: 16
logging_steps: 5
eval_steps: 200
save_steps: 200
weight_decay: 1e-4
MAX_INPUT: 1024
MODEL: 'meta-llama/Meta-Llama-3-8B-Instruct'
dropout_rate: 0.1
awp_lr: 0
awp_eps: 1e-4
awp_start_epoch: 0.5
label_smoothing_factor: 0
output_dir: 'output'
use_cache: true
token_type: 'MC'
lora_r: 64
lora_alpha: 32
lora_dropout: 0.2
