{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ac7f010-4ef6-4b50-8b20-8a98b8a44895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from typing import Optional, Union\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    EarlyStoppingCallback,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForMultipleChoice,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    RobertaForMultipleChoice,\n",
    "    AutoModelForSequenceClassification,\n",
    "    LlamaModel,\n",
    "    LlamaForSequenceClassification,\n",
    "    BitsAndBytesConfig,\n",
    "    get_polynomial_decay_schedule_with_warmup,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    TrainerCallback,\n",
    ")\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "\n",
    "from peft import (\n",
    "    get_peft_config,\n",
    "    PeftModel,\n",
    "    PeftConfig,\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    ")\n",
    "import os\n",
    "\n",
    "import random\n",
    "from random import randint\n",
    "def seed_everything(seed=None):\n",
    "    '''\n",
    "    固定seed\n",
    "    :param seed: int, 随机种子\n",
    "    '''\n",
    "    max_seed_value = np.iinfo(np.uint32).max\n",
    "    min_seed_value = np.iinfo(np.uint32).min\n",
    "\n",
    "    if (seed is None) or not (min_seed_value <= seed <= max_seed_value):\n",
    "        seed = random.randint(np.iinfo(np.uint32).min, np.iinfo(np.uint32).max)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    return seed\n",
    "\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "from utils import load_split_data, load_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d97a2bfe-0604-4dd2-9db9-b0f1175793ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class InstructionDataSet(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_source_length, max_target_length):\n",
    "        super(InstructionDataSet, self).__init__()\n",
    "        #self.data = data.sample(len(data), random_state=0).reset_index(drop=True)\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_source_length = max_source_length\n",
    "        self.max_target_length = max_target_length\n",
    "        # self.A_token = self.tokenizer.encode(text='A', add_special_tokens=False, truncation=True, )\n",
    "        # self.B_token = self.tokenizer.encode(text='B', add_special_tokens=False, truncation=True, )\n",
    "        # self.C_token = self.tokenizer.encode(text='C', add_special_tokens=False, truncation=True, )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        now_data = self.data.loc[index]\n",
    "        idx = now_data['id']\n",
    "        templete_part1 = \"<start_of_turn>user\\nHere are two question-answering dialogues. Compare two model performance on answering question, determine which is better.\\n\\n\"\n",
    "        templete_part1_input_ids = self.tokenizer(text=templete_part1, add_special_tokens=True, padding=False)['input_ids']\n",
    "        \n",
    "        templete_part2 = \"\\n###options\\nA. Model A\\nB. Model B\\nC. Tie\\n<end_of_turn>\\n\"\n",
    "        templete_part2_input_ids = self.tokenizer(text=templete_part2, add_special_tokens=True, padding=False)['input_ids'][1:]\n",
    "        #print(f\"templete_part2 is {templete_part2_input_ids}\")\n",
    "        templete_part3 = \"<start_of_turn>model\\n\"\n",
    "        templete_part3_input_ids = self.tokenizer(text=templete_part3, add_special_tokens=True, padding=False)['input_ids'][1:]\n",
    "        prompt_response = now_data['prompt_response']\n",
    "        #print(f\"id is {now_data['id']}\")\n",
    "        #print(prompt_response)\n",
    "        prompt_response_ids = self.tokenizer(text=prompt_response, add_special_tokens=True, truncation=True,\n",
    "                                          max_length=self.max_source_length, padding=False)['input_ids'][1:]\n",
    "        \n",
    "        input_ids = templete_part1_input_ids + prompt_response_ids + templete_part2_input_ids + templete_part3_input_ids\n",
    "        input_text = self.tokenizer.decode(input_ids, skip_special_tokens=False)\n",
    "        #print(f\"input is {self.tokenizer.decode(input_ids)}\")\n",
    "        return {\n",
    "            \"input_ids\": input_text,\n",
    "            \"id\": idx\n",
    "        }\n",
    "\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = {k: [item[k] for item in batch] for k in ('input_ids','id')}\n",
    "    #print(batch)\n",
    "    batch_input = tokenizer(\n",
    "        batch['input_ids'],\n",
    "        padding='longest',\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_LENGTH + 50\n",
    "    )\n",
    "    return batch_input, batch['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b57bcf4-b6e9-4f21-8671-b43700a1344f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6168/6168 [00:00<00:00, 11885.84it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils import load_split_data\n",
    "data_path = \"dataset/non_overlap/valid.json\"\n",
    "prompt_type = 3\n",
    "MAX_INPUT = 1900\n",
    "if_train = True\n",
    "split = False\n",
    "if_drop_duplicate = True\n",
    "keep = 'last'\n",
    "df_train , df_valid = load_split_data(data_path, prompt_type, MAX_INPUT, if_train, split, False, if_drop_duplicate, 'last')\n",
    "test = df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2c87a00-8d6b-419c-8358-e1c8dd8a5828",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['length'] = test['prompt_response'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14275570-5606-496e-96b8-01e0d5eaa17c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt_response</th>\n",
       "      <th>label</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3169432118</td>\n",
       "      <td>#Prompt\\nCreate an ascii map of a fantasy land...</td>\n",
       "      <td>A</td>\n",
       "      <td>23134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>344759482</td>\n",
       "      <td>#Prompt\\nORDER              CARROLL, United St...</td>\n",
       "      <td>C</td>\n",
       "      <td>18025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3173055602</td>\n",
       "      <td>#Prompt\\nusing Microsoft.Extensions.Configurat...</td>\n",
       "      <td>C</td>\n",
       "      <td>17780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1601148012</td>\n",
       "      <td>#Prompt\\nGiven this existing Typescript file h...</td>\n",
       "      <td>B</td>\n",
       "      <td>17525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2751917273</td>\n",
       "      <td>#Prompt\\nimport * as applicationautoscaling fr...</td>\n",
       "      <td>C</td>\n",
       "      <td>17289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4939</th>\n",
       "      <td>2034821902</td>\n",
       "      <td>#Prompt\\nWrite a single dot\\n\\n#Response\\n##Mo...</td>\n",
       "      <td>C</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4940</th>\n",
       "      <td>4008034811</td>\n",
       "      <td>#Prompt\\nWrite a single dot\\n\\n#Response\\n##Mo...</td>\n",
       "      <td>C</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4941</th>\n",
       "      <td>2261801017</td>\n",
       "      <td>#Prompt\\nWrite a single dot\\n\\n#Response\\n##Mo...</td>\n",
       "      <td>C</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4942</th>\n",
       "      <td>1351744298</td>\n",
       "      <td>#Prompt\\nWrite a single dot\\n\\n#Response\\n##Mo...</td>\n",
       "      <td>C</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4943</th>\n",
       "      <td>2555724000</td>\n",
       "      <td>#Prompt\\nWrite a single dot\\n\\n#Response\\n##Mo...</td>\n",
       "      <td>C</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4944 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                    prompt_response label  \\\n",
       "0     3169432118  #Prompt\\nCreate an ascii map of a fantasy land...     A   \n",
       "1      344759482  #Prompt\\nORDER              CARROLL, United St...     C   \n",
       "2     3173055602  #Prompt\\nusing Microsoft.Extensions.Configurat...     C   \n",
       "3     1601148012  #Prompt\\nGiven this existing Typescript file h...     B   \n",
       "4     2751917273  #Prompt\\nimport * as applicationautoscaling fr...     C   \n",
       "...          ...                                                ...   ...   \n",
       "4939  2034821902  #Prompt\\nWrite a single dot\\n\\n#Response\\n##Mo...     C   \n",
       "4940  4008034811  #Prompt\\nWrite a single dot\\n\\n#Response\\n##Mo...     C   \n",
       "4941  2261801017  #Prompt\\nWrite a single dot\\n\\n#Response\\n##Mo...     C   \n",
       "4942  1351744298  #Prompt\\nWrite a single dot\\n\\n#Response\\n##Mo...     C   \n",
       "4943  2555724000  #Prompt\\nWrite a single dot\\n\\n#Response\\n##Mo...     C   \n",
       "\n",
       "      length  \n",
       "0      23134  \n",
       "1      18025  \n",
       "2      17780  \n",
       "3      17525  \n",
       "4      17289  \n",
       "...      ...  \n",
       "4939      62  \n",
       "4940      62  \n",
       "4941      62  \n",
       "4942      62  \n",
       "4943      62  \n",
       "\n",
       "[4944 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = test.sort_values(by = ['length'], ascending = False).reset_index(drop = True)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19164b06-76e6-45b8-97a6-80f517231fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def inference(model, test_dataloader):\n",
    "    test_predictions = []\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        batch_input, idx = batch\n",
    "        for k in batch_input.keys():\n",
    "            batch_input[k] = batch_input[k].to(device)\n",
    "        with torch.no_grad():\n",
    "            response = model.generate(**batch_input, max_new_tokens=1, return_dict_in_generate=True, output_scores=True)\n",
    "            #batch_input['input_ids'].shape[-1] + 1\n",
    "            score = response.scores[0]\n",
    "            A_prob, B_prob, C_prob = score[:,A_TOKEN_IDS], score[:,B_TOKEN_IDS], score[:,C_TOKEN_IDS]\n",
    "            logits = torch.cat([A_prob, B_prob, C_prob], dim=-1)\n",
    "            #logits = torch.Tensor([[A_prob,B_prob,C_prob]]) / 1.1\n",
    "            logits = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "            node_result = [[idx[i],logits[i]] for i in range(len(idx))]\n",
    "        test_predictions.extend(node_result)\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52cc5551-8b74-4099-9d77-26ed2faadcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "base_model = 'google/gemma-2-9b-it'\n",
    "model_path = \"output/morning-waterfall-460/checkpoint-5200_888\"\n",
    "MAX_LENGTH = 1900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2936ddc-b99a-4c6e-85f0-3a1a5b273f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0579ae798b2f4759a864d19adf863bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Gemma2ForCausalLM(\n",
       "      (model): Gemma2Model(\n",
       "        (embed_tokens): Embedding(256000, 3584, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-41): 42 x Gemma2DecoderLayer(\n",
       "            (self_attn): Gemma2SdpaAttention(\n",
       "              (q_proj): Linear4bit(\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (base_layer): Linear4bit(in_features=3584, out_features=4096, bias=False)\n",
       "              )\n",
       "              (k_proj): Linear4bit(\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (base_layer): Linear4bit(in_features=3584, out_features=2048, bias=False)\n",
       "              )\n",
       "              (v_proj): Linear4bit(\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (base_layer): Linear4bit(in_features=3584, out_features=2048, bias=False)\n",
       "              )\n",
       "              (o_proj): Linear4bit(\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=3584, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=3584, bias=False)\n",
       "              )\n",
       "              (rotary_emb): Gemma2RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Gemma2MLP(\n",
       "              (gate_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=3584, bias=False)\n",
       "              (act_fn): PytorchGELUTanh()\n",
       "            )\n",
       "            (input_layernorm): Gemma2RMSNorm()\n",
       "            (post_attention_layernorm): Gemma2RMSNorm()\n",
       "            (pre_feedforward_layernorm): Gemma2RMSNorm()\n",
       "            (post_feedforward_layernorm): Gemma2RMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): Gemma2RMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3584, out_features=256000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, truncation_side = 'left')\n",
    "config = AutoConfig.from_pretrained(base_model, trust_remote_code=True)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  \n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "base_model_0 = AutoModelForCausalLM.from_pretrained(base_model,\n",
    "                                                 config=config,\n",
    "                                                 quantization_config=bnb_config,\n",
    "                                                 torch_dtype=torch.float16,\n",
    "                                                 device_map=\"auto\",\n",
    "                                                 trust_remote_code=True)\n",
    "# base_model_0.config.pad_token_id = tokenizer.pad_token_id\n",
    "# base_model_0.resize_token_embeddings(len(tokenizer))\n",
    "new_model = model_path\n",
    "model0 = PeftModel.from_pretrained(base_model_0, new_model).to(device)\n",
    "#model0 = model0.merge_and_unload()\n",
    "model0.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "716e3aa7-bc8b-48c2-a87c-751baeb5f884",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_TOKEN_IDS = tokenizer('A',add_special_tokens=True, truncation=True, max_length=1024)['input_ids'][1:]\n",
    "B_TOKEN_IDS = tokenizer('B',add_special_tokens=True, truncation=True, max_length=1024)['input_ids'][1:]\n",
    "C_TOKEN_IDS = tokenizer('C',add_special_tokens=True, truncation=True, max_length=1024)['input_ids'][1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af5caf08-32ba-4b84-b13f-a5a0a21e5118",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "tokenized_dataset = InstructionDataSet(test, tokenizer, MAX_LENGTH, 1)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(tokenized_dataset, batch_size = batch_size ,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d36e303-5623-4fd6-8724-139701fa44ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 618/618 [13:37<00:00,  1.32s/it]\n"
     ]
    }
   ],
   "source": [
    "sub_pred = inference(model = model0, test_dataloader = test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfcf6215-5ee6-40ff-a7d0-9fc64a9a8037",
   "metadata": {},
   "outputs": [],
   "source": [
    "if batch_size != 1:\n",
    "    # 提取数据\n",
    "    processed_data = []\n",
    "    for item in sub_pred:\n",
    "        #item = item[0]\n",
    "        id = item[0].item()  # 获取id\n",
    "        array_values = item[1].tolist()  # 获取array并转换为列表\n",
    "        processed_data.append([id] + array_values)\n",
    "    \n",
    "\n",
    "else:\n",
    "    # 提取数据\n",
    "    processed_data = []\n",
    "    for item in sub_pred:\n",
    "        item = item[0]\n",
    "        id = item[0].item()  # 获取id\n",
    "        array_values = item[1].tolist()  # 获取array并转换为列表\n",
    "        processed_data.append([id] + array_values)\n",
    "\n",
    "new_columns = ['id', 'winner_model_a', 'winner_model_b', 'winner_tie']\n",
    "df = pd.DataFrame(processed_data, columns=new_columns)\n",
    "df = df.groupby('id').mean().reset_index()\n",
    "\n",
    "prediction = np.array(df[new_columns[1:]])\n",
    "test = test.drop_duplicates(subset = ['id']).reset_index(drop = True)\n",
    "test = test.sort_values(by = ['id']).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4548df7d-898f-4a11-8a2f-8579489754a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"dataset/non_overlap/valid.json\"\n",
    "data = pd.read_json(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a36dec6-180f-41fa-8925-3603f4f0786b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = data.merge(df, how = 'left', on = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a444e5d1-741a-483b-800f-bd7bbecb9387",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_json(\"dataset/persudo_label/prediction.json\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbe5072-2566-4dee-b672-09a3ccf279c7",
   "metadata": {},
   "source": [
    "# 合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07643a6b-bec0-41da-8319-1ce9f53045d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pd.read_csv(\"dataset/prediction.csv\")\n",
    "ex = pd.read_json(\"dataset/ex70k.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cc433f92-0a0b-4226-adb8-6b32351cc18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = p.rename(columns = {'winner_model_a':\"p_winner_model_a\", 'winner_model_b':\"p_winner_model_b\",  'winner_tie':\"p_winner_tie\"})\n",
    "final = pd.concat([ex, p], axis = 1)\n",
    "final = final.drop(columns= ['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6a084b1a-5939-4675-b695-28d0542a0142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_label(row):\n",
    "    a = row.p_winner_model_a\n",
    "    b = row.p_winner_model_b\n",
    "    c = row.p_winner_tie\n",
    "\n",
    "    l = [a ,b, c]\n",
    "    label = l.index(max(l))\n",
    "    return label\n",
    "\n",
    "final['p_label'] = final.apply(get_p_label, axis = 1)\n",
    "\n",
    "def get_label(row):\n",
    "    label = [idx for idx, option in enumerate(['winner_model_a','winner_model_b','winner_tie']) if row[option] == 1]\n",
    "    return label[-1]\n",
    "\n",
    "final['label'] = final.apply(get_label, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0d3afd29-0199-4658-a5ad-68dab36b399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold1 = 0.9\n",
    "filter_same = final.loc[final.p_label == final.label,:].reset_index(drop = True)\n",
    "filter_list = (filter_same.p_winner_model_a >= threshold1) | (filter_same.p_winner_model_b >= threshold1) | (filter_same.p_winner_tie >= threshold1)\n",
    "filter_same = filter_same.loc[filter_list,:].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f6414821-a739-4fea-a27b-e05df1d2fb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_list = (filter_same.difference >= 1) | (filter_same.winner_tie == 1)\n",
    "filter_same = filter_same.loc[filter_list,:].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fe0492d7-2278-469e-9076-30e85dce79ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold2 = 0.6\n",
    "filter_dif = final.loc[final.p_label != final.label,:].reset_index(drop = True)\n",
    "filter_list = (filter_dif.p_winner_model_a >= threshold2) | (filter_dif.p_winner_model_b >= threshold2) | (filter_dif.p_winner_tie >= threshold2)\n",
    "filter_dif = filter_dif.loc[filter_list,:].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a35cb084-ab73-4068-a6a0-9df095f673e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_list = (filter_dif.difference >= 1) | (filter_dif.winner_tie == 1)\n",
    "filter_dif = filter_dif.loc[filter_list,:].reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c9164dba-d5bb-483a-94a0-b8aba849c6ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>difference</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>p_winner_model_a</th>\n",
       "      <th>p_winner_model_b</th>\n",
       "      <th>p_winner_tie</th>\n",
       "      <th>p_label</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ Given a sentence in the English language, tr...</td>\n",
       "      <td>llama-2-70b-chat</td>\n",
       "      <td>vicuna-33b</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[I don't think that's an appropriate or respec...</td>\n",
       "      <td>[वे भी क्रमिक त्वचा, निविधता स्वरूप अँवलोकन र ...</td>\n",
       "      <td>0.212914</td>\n",
       "      <td>0.104654</td>\n",
       "      <td>0.682431</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ Given a sentence in the English language, tr...</td>\n",
       "      <td>starchat</td>\n",
       "      <td>alpaca-7b</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[In the Nepali language, the sentence would be...</td>\n",
       "      <td>[ये लाग्दे खोयेचे पीचोगियु र घोख्ने बजारोबोन ग...</td>\n",
       "      <td>0.255210</td>\n",
       "      <td>0.093090</td>\n",
       "      <td>0.651701</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[!2 / 2write a screenplay in which trump and b...</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>wizardlm-7b</td>\n",
       "      <td>3.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[I'm sorry, but I cannot write a screenplay wi...</td>\n",
       "      <td>[Title: The Naughty Nazis\\nFADE IN:\\nEXT. PARI...</td>\n",
       "      <td>0.062963</td>\n",
       "      <td>0.847231</td>\n",
       "      <td>0.089806</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[!I have lots of corrupted photos on my comput...</td>\n",
       "      <td>llama-2-70b-chat</td>\n",
       "      <td>bard</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[I cannot provide a Python script to scan a fo...</td>\n",
       "      <td>[I'm sorry, I can't help you with that. I'm no...</td>\n",
       "      <td>0.228464</td>\n",
       "      <td>0.089468</td>\n",
       "      <td>0.682068</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[** Waking up late in the mornings may cause l...</td>\n",
       "      <td>mpt-30b-chat</td>\n",
       "      <td>wizardlm-13b</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[Confidence: 90%\\n\\nAnswer:\\n\\n1. prison\\n\\nTh...</td>\n",
       "      <td>[Which pocket is most likely to contain a plan...</td>\n",
       "      <td>0.689259</td>\n",
       "      <td>0.095697</td>\n",
       "      <td>0.215044</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>[write me a script to scrape the internet for ...</td>\n",
       "      <td>mpt-30b-chat</td>\n",
       "      <td>wizardlm-70b</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[To scrape the internet for companies/organiza...</td>\n",
       "      <td>[As an AI language model, I am not able to pro...</td>\n",
       "      <td>0.733839</td>\n",
       "      <td>0.096259</td>\n",
       "      <td>0.169902</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>[write me an example code in C++ for rendering...</td>\n",
       "      <td>falcon-40b-instruct</td>\n",
       "      <td>wizardlm-7b</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[Here's an example code for rendering a triang...</td>\n",
       "      <td>[Unfortunately, as an AI language model, I can...</td>\n",
       "      <td>0.710349</td>\n",
       "      <td>0.074235</td>\n",
       "      <td>0.215417</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3263</th>\n",
       "      <td>[write message reply for this message: \\n\\n Yo...</td>\n",
       "      <td>mpt-30b-chat</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[Thank you for your help in getting a response...</td>\n",
       "      <td>[Dear [User],\\n\\nThank you for your message an...</td>\n",
       "      <td>0.656775</td>\n",
       "      <td>0.175268</td>\n",
       "      <td>0.167957</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3264</th>\n",
       "      <td>[you didn't list smartcare.com. can you provid...</td>\n",
       "      <td>llama-2-13b-chat</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[Of course! As a helpful and honest assistant,...</td>\n",
       "      <td>[I apologize for the oversight. Smartcare (sma...</td>\n",
       "      <td>0.121752</td>\n",
       "      <td>0.718776</td>\n",
       "      <td>0.159472</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3265</th>\n",
       "      <td>[‘I need your help to write an article. The to...</td>\n",
       "      <td>mpt-30b-chat</td>\n",
       "      <td>wizardlm-70b</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[Acknowledged. I understand that you need my h...</td>\n",
       "      <td>[acknowledged\\n\\nI understand that you need he...</td>\n",
       "      <td>0.223034</td>\n",
       "      <td>0.611457</td>\n",
       "      <td>0.165509</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3266 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 prompt              model_a  \\\n",
       "0     [ Given a sentence in the English language, tr...     llama-2-70b-chat   \n",
       "1     [ Given a sentence in the English language, tr...             starchat   \n",
       "2     [!2 / 2write a screenplay in which trump and b...                gpt-4   \n",
       "3     [!I have lots of corrupted photos on my comput...     llama-2-70b-chat   \n",
       "4     [** Waking up late in the mornings may cause l...         mpt-30b-chat   \n",
       "...                                                 ...                  ...   \n",
       "3261  [write me a script to scrape the internet for ...         mpt-30b-chat   \n",
       "3262  [write me an example code in C++ for rendering...  falcon-40b-instruct   \n",
       "3263  [write message reply for this message: \\n\\n Yo...         mpt-30b-chat   \n",
       "3264  [you didn't list smartcare.com. can you provid...     llama-2-13b-chat   \n",
       "3265  [‘I need your help to write an article. The to...         mpt-30b-chat   \n",
       "\n",
       "            model_b  difference  winner_model_a  winner_model_b  winner_tie  \\\n",
       "0        vicuna-33b        1.25               1               0           0   \n",
       "1         alpaca-7b        2.00               1               0           0   \n",
       "2       wizardlm-7b        3.50               1               0           0   \n",
       "3              bard        2.00               1               0           0   \n",
       "4      wizardlm-13b        1.75               0               1           0   \n",
       "...             ...         ...             ...             ...         ...   \n",
       "3261   wizardlm-70b        0.00               0               0           1   \n",
       "3262    wizardlm-7b        0.00               0               0           1   \n",
       "3263          gpt-4        0.00               0               0           1   \n",
       "3264  gpt-3.5-turbo        0.00               0               0           1   \n",
       "3265   wizardlm-70b        0.00               0               0           1   \n",
       "\n",
       "                                             response_a  \\\n",
       "0     [I don't think that's an appropriate or respec...   \n",
       "1     [In the Nepali language, the sentence would be...   \n",
       "2     [I'm sorry, but I cannot write a screenplay wi...   \n",
       "3     [I cannot provide a Python script to scan a fo...   \n",
       "4     [Confidence: 90%\\n\\nAnswer:\\n\\n1. prison\\n\\nTh...   \n",
       "...                                                 ...   \n",
       "3261  [To scrape the internet for companies/organiza...   \n",
       "3262  [Here's an example code for rendering a triang...   \n",
       "3263  [Thank you for your help in getting a response...   \n",
       "3264  [Of course! As a helpful and honest assistant,...   \n",
       "3265  [Acknowledged. I understand that you need my h...   \n",
       "\n",
       "                                             response_b  p_winner_model_a  \\\n",
       "0     [वे भी क्रमिक त्वचा, निविधता स्वरूप अँवलोकन र ...          0.212914   \n",
       "1     [ये लाग्दे खोयेचे पीचोगियु र घोख्ने बजारोबोन ग...          0.255210   \n",
       "2     [Title: The Naughty Nazis\\nFADE IN:\\nEXT. PARI...          0.062963   \n",
       "3     [I'm sorry, I can't help you with that. I'm no...          0.228464   \n",
       "4     [Which pocket is most likely to contain a plan...          0.689259   \n",
       "...                                                 ...               ...   \n",
       "3261  [As an AI language model, I am not able to pro...          0.733839   \n",
       "3262  [Unfortunately, as an AI language model, I can...          0.710349   \n",
       "3263  [Dear [User],\\n\\nThank you for your message an...          0.656775   \n",
       "3264  [I apologize for the oversight. Smartcare (sma...          0.121752   \n",
       "3265  [acknowledged\\n\\nI understand that you need he...          0.223034   \n",
       "\n",
       "      p_winner_model_b  p_winner_tie  p_label  label  \n",
       "0             0.104654      0.682431        2      0  \n",
       "1             0.093090      0.651701        2      0  \n",
       "2             0.847231      0.089806        1      0  \n",
       "3             0.089468      0.682068        2      0  \n",
       "4             0.095697      0.215044        0      1  \n",
       "...                ...           ...      ...    ...  \n",
       "3261          0.096259      0.169902        0      2  \n",
       "3262          0.074235      0.215417        0      2  \n",
       "3263          0.175268      0.167957        0      2  \n",
       "3264          0.718776      0.159472        1      2  \n",
       "3265          0.611457      0.165509        1      2  \n",
       "\n",
       "[3266 rows x 14 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "99d070f0-a45a-453f-a770-ba4b1c9e905a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>difference</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>p_winner_model_a</th>\n",
       "      <th>p_winner_model_b</th>\n",
       "      <th>p_winner_tie</th>\n",
       "      <th>p_label</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[## How to write a C++ program to solve the eq...</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>vicuna-33b</td>\n",
       "      <td>3.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[However, this code only calculates a single s...</td>\n",
       "      <td>[Msg#: 1\\nBranches: 1\\n-----------------------...</td>\n",
       "      <td>0.901630</td>\n",
       "      <td>0.020033</td>\n",
       "      <td>0.078337</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[## question\\nAfter Jessica makes up with Bill...</td>\n",
       "      <td>ultralm-13b</td>\n",
       "      <td>wizardlm-7b</td>\n",
       "      <td>3.25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[Mary Camden meets Carlos Rivera, a man she ha...</td>\n",
       "      <td>[There is no mention of Mary Camden marrying a...</td>\n",
       "      <td>0.926649</td>\n",
       "      <td>0.015279</td>\n",
       "      <td>0.058072</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(3 + 4i) * (5 - 6i) + 7i - 8=]</td>\n",
       "      <td>ultralm-13b</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[Certainly! I'd first like to simplify the giv...</td>\n",
       "      <td>[To perform the calculation, we need to follow...</td>\n",
       "      <td>0.023027</td>\n",
       "      <td>0.912023</td>\n",
       "      <td>0.064950</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(4 x 8) ÷ 10 + 2=]</td>\n",
       "      <td>vicuna-33b</td>\n",
       "      <td>wizardlm-7b</td>\n",
       "      <td>3.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[To calculate this expression, perform the ope...</td>\n",
       "      <td>[The answer is 2.2. \\nTo solve this expression...</td>\n",
       "      <td>0.909157</td>\n",
       "      <td>0.022312</td>\n",
       "      <td>0.068531</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(Q).\\n\"Kevork Ajemian\", given a list of categ...</td>\n",
       "      <td>falcon-40b-instruct</td>\n",
       "      <td>llama-2-70b-chat</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"Athlete\"&gt; \\nCan you please give me the answe...</td>\n",
       "      <td>[(A). Village]</td>\n",
       "      <td>0.023844</td>\n",
       "      <td>0.904970</td>\n",
       "      <td>0.071185</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3381</th>\n",
       "      <td>[구글스프래드시드로 관심있는 분야의 유튜브 동영상을 검색하는 것을 만들고 싶어.\\n...</td>\n",
       "      <td>falcon-40b-instruct</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>3.50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[Are you looking for YouTube videos related to...</td>\n",
       "      <td>[If you want to create a Google Spreadsheet th...</td>\n",
       "      <td>0.015519</td>\n",
       "      <td>0.927956</td>\n",
       "      <td>0.056525</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3382</th>\n",
       "      <td>[너가 오랜 백엔드 경험이 있는 리액트 전문가로써 나와 대화하길 원해. 너의 목표는...</td>\n",
       "      <td>ultralm-65b</td>\n",
       "      <td>mpt-30b-chat</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[로켓스테더링(rockestering)과 유지도(Upholding)을 위해 만들어진...</td>\n",
       "      <td>[React was created to improve the user interfa...</td>\n",
       "      <td>0.023282</td>\n",
       "      <td>0.922121</td>\n",
       "      <td>0.054597</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3383</th>\n",
       "      <td>[딥러닝을 활용한 양자컴퓨팅 데이터 분석과 관련된 연구 주제로는 뭐가 있을까?\\n\\...</td>\n",
       "      <td>vicuna-33b</td>\n",
       "      <td>falcon-40b-instruct</td>\n",
       "      <td>3.25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[Here are some study subjects related to deep ...</td>\n",
       "      <td>[What specific topics or research topics are r...</td>\n",
       "      <td>0.917127</td>\n",
       "      <td>0.012752</td>\n",
       "      <td>0.070121</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3384</th>\n",
       "      <td>[아래 책 내용을 요약해줘 ",
       " - 저자 : 유발 하라리  ",
       " - 제목 : 사피엔스 ",
       " -...</td>\n",
       "      <td>falcon-40b-instruct</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[Yes, I can help you with that. Can you please...</td>\n",
       "      <td>[제목: 사피엔스 (Sapiens)\\n저자: 유발 하라리 (Yuval Noah Ha...</td>\n",
       "      <td>0.020359</td>\n",
       "      <td>0.903353</td>\n",
       "      <td>0.076289</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3385</th>\n",
       "      <td>[우리는 프랜차이즈에게 식자재를 공급하고 있어. 추가적으로 레스토랑 솔루션을 도입한...</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>falcon-40b-instruct</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[프랜차이즈에게 식자재를 공급하는 것 외에도 다양한 레스토랑 솔루션을 제공하면 프랜...</td>\n",
       "      <td>[It is recommended to give bread, rice, vegeta...</td>\n",
       "      <td>0.926220</td>\n",
       "      <td>0.012341</td>\n",
       "      <td>0.061439</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3386 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 prompt              model_a  \\\n",
       "0     [## How to write a C++ program to solve the eq...                gpt-4   \n",
       "1     [## question\\nAfter Jessica makes up with Bill...          ultralm-13b   \n",
       "2                       [(3 + 4i) * (5 - 6i) + 7i - 8=]          ultralm-13b   \n",
       "3                                   [(4 x 8) ÷ 10 + 2=]           vicuna-33b   \n",
       "4     [(Q).\\n\"Kevork Ajemian\", given a list of categ...  falcon-40b-instruct   \n",
       "...                                                 ...                  ...   \n",
       "3381  [구글스프래드시드로 관심있는 분야의 유튜브 동영상을 검색하는 것을 만들고 싶어.\\n...  falcon-40b-instruct   \n",
       "3382  [너가 오랜 백엔드 경험이 있는 리액트 전문가로써 나와 대화하길 원해. 너의 목표는...          ultralm-65b   \n",
       "3383  [딥러닝을 활용한 양자컴퓨팅 데이터 분석과 관련된 연구 주제로는 뭐가 있을까?\\n\\...           vicuna-33b   \n",
       "3384  [아래 책 내용을 요약해줘\n",
       " - 저자 : 유발 하라리 \n",
       " - 제목 : 사피엔스\n",
       " -...  falcon-40b-instruct   \n",
       "3385  [우리는 프랜차이즈에게 식자재를 공급하고 있어. 추가적으로 레스토랑 솔루션을 도입한...                gpt-4   \n",
       "\n",
       "                  model_b  difference  winner_model_a  winner_model_b  \\\n",
       "0              vicuna-33b        3.50               1               0   \n",
       "1             wizardlm-7b        3.25               1               0   \n",
       "2           gpt-3.5-turbo        4.00               0               1   \n",
       "3             wizardlm-7b        3.50               1               0   \n",
       "4        llama-2-70b-chat        3.00               0               1   \n",
       "...                   ...         ...             ...             ...   \n",
       "3381        gpt-3.5-turbo        3.50               0               1   \n",
       "3382         mpt-30b-chat        3.75               0               1   \n",
       "3383  falcon-40b-instruct        3.25               1               0   \n",
       "3384        gpt-3.5-turbo        3.25               0               1   \n",
       "3385  falcon-40b-instruct        4.00               1               0   \n",
       "\n",
       "      winner_tie                                         response_a  \\\n",
       "0              0  [However, this code only calculates a single s...   \n",
       "1              0  [Mary Camden meets Carlos Rivera, a man she ha...   \n",
       "2              0  [Certainly! I'd first like to simplify the giv...   \n",
       "3              0  [To calculate this expression, perform the ope...   \n",
       "4              0  [\"Athlete\"> \\nCan you please give me the answe...   \n",
       "...          ...                                                ...   \n",
       "3381           0  [Are you looking for YouTube videos related to...   \n",
       "3382           0  [로켓스테더링(rockestering)과 유지도(Upholding)을 위해 만들어진...   \n",
       "3383           0  [Here are some study subjects related to deep ...   \n",
       "3384           0  [Yes, I can help you with that. Can you please...   \n",
       "3385           0  [프랜차이즈에게 식자재를 공급하는 것 외에도 다양한 레스토랑 솔루션을 제공하면 프랜...   \n",
       "\n",
       "                                             response_b  p_winner_model_a  \\\n",
       "0     [Msg#: 1\\nBranches: 1\\n-----------------------...          0.901630   \n",
       "1     [There is no mention of Mary Camden marrying a...          0.926649   \n",
       "2     [To perform the calculation, we need to follow...          0.023027   \n",
       "3     [The answer is 2.2. \\nTo solve this expression...          0.909157   \n",
       "4                                        [(A). Village]          0.023844   \n",
       "...                                                 ...               ...   \n",
       "3381  [If you want to create a Google Spreadsheet th...          0.015519   \n",
       "3382  [React was created to improve the user interfa...          0.023282   \n",
       "3383  [What specific topics or research topics are r...          0.917127   \n",
       "3384  [제목: 사피엔스 (Sapiens)\\n저자: 유발 하라리 (Yuval Noah Ha...          0.020359   \n",
       "3385  [It is recommended to give bread, rice, vegeta...          0.926220   \n",
       "\n",
       "      p_winner_model_b  p_winner_tie  p_label  label  \n",
       "0             0.020033      0.078337        0      0  \n",
       "1             0.015279      0.058072        0      0  \n",
       "2             0.912023      0.064950        1      1  \n",
       "3             0.022312      0.068531        0      0  \n",
       "4             0.904970      0.071185        1      1  \n",
       "...                ...           ...      ...    ...  \n",
       "3381          0.927956      0.056525        1      1  \n",
       "3382          0.922121      0.054597        1      1  \n",
       "3383          0.012752      0.070121        0      0  \n",
       "3384          0.903353      0.076289        1      1  \n",
       "3385          0.012341      0.061439        0      0  \n",
       "\n",
       "[3386 rows x 14 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4d56a3b0-e34c-4fbe-bcdb-2d28cd3010f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dif['id'] = [randint(100,999999) + i for i in range(len(filter_dif))]\n",
    "filter_same['id'] = [randint(100,999999) + i for i in range(len(filter_same))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "203da592-cd71-4ade-8c2a-4b1626942be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_columns = ['prompt', 'model_a', 'model_b', 'winner_model_a', 'winner_model_b', 'winner_tie', 'response_a', 'response_b', 'id']\n",
    "filter_dif[save_columns].to_json(f'dataset/70k_dif_thr{int(threshold2 * 100)}.json', index = False)\n",
    "filter_same[save_columns].to_json(f'dataset/70k_same_thr{int(threshold1 * 100)}.json', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d85e80-74fe-400e-b35f-2203eca9e67e",
   "metadata": {},
   "source": [
    "# 检查是否与valid重复"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5980197d-491a-4341-82a2-3f00a1e3a01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查\n",
    "\n",
    "valid = pd.read_json(\"dataset/non_overlap/valid.json\")\n",
    "filter_dif = pd.read_json(\"dataset/70k_dif_thr60.json\")\n",
    "filter_same = pd.read_json(\"dataset/70k_same_thr90.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a70bacfa-2971-41f8-b6d4-b09f9ac69792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set_prompt_response(data):\n",
    "    set_prompt_response = []\n",
    "    for i in data.itertuples():\n",
    "        prompt_response = i.prompt + i.response_a + i.response_b\n",
    "        set_prompt_response.append(set(prompt_response))\n",
    "    data['set_prompt_response'] = set_prompt_response  \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b52169e3-4352-4cad-9ba5-34bb508ecc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = get_set_prompt_response(valid)\n",
    "filter_dif = get_set_prompt_response(filter_dif)\n",
    "filter_same = get_set_prompt_response(filter_same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2049c0af-537e-437f-bc66-35e84d342acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid和任何都不重合\n",
    "assert len([idx for idx, i in enumerate(valid.set_prompt_response.values) if i in filter_dif.set_prompt_response.values]) == 0\n",
    "assert len([idx for idx, i in enumerate(valid.set_prompt_response.values) if i in filter_same.set_prompt_response.values]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c5cdae1c-3902-43d6-86f2-75e16982bde6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([idx for idx, i in enumerate(valid.set_prompt_response.values) if i in filter_dif.set_prompt_response.values])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
