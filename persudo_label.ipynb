{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ac7f010-4ef6-4b50-8b20-8a98b8a44895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from typing import Optional, Union\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    EarlyStoppingCallback,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForMultipleChoice,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    RobertaForMultipleChoice,\n",
    "    AutoModelForSequenceClassification,\n",
    "    LlamaModel,\n",
    "    LlamaForSequenceClassification,\n",
    "    BitsAndBytesConfig,\n",
    "    get_polynomial_decay_schedule_with_warmup,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    TrainerCallback,\n",
    ")\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "\n",
    "from peft import (\n",
    "    get_peft_config,\n",
    "    PeftModel,\n",
    "    PeftConfig,\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    ")\n",
    "import os\n",
    "\n",
    "import random\n",
    "from random import randint\n",
    "def seed_everything(seed=None):\n",
    "    '''\n",
    "    固定seed\n",
    "    :param seed: int, 随机种子\n",
    "    '''\n",
    "    max_seed_value = np.iinfo(np.uint32).max\n",
    "    min_seed_value = np.iinfo(np.uint32).min\n",
    "\n",
    "    if (seed is None) or not (min_seed_value <= seed <= max_seed_value):\n",
    "        seed = random.randint(np.iinfo(np.uint32).min, np.iinfo(np.uint32).max)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    return seed\n",
    "\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "from utils import load_split_data, load_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97a2bfe-0604-4dd2-9db9-b0f1175793ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class InstructionDataSet(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_source_length, max_target_length):\n",
    "        super(InstructionDataSet, self).__init__()\n",
    "        #self.data = data.sample(len(data), random_state=0).reset_index(drop=True)\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_source_length = max_source_length\n",
    "        self.max_target_length = max_target_length\n",
    "        # self.A_token = self.tokenizer.encode(text='A', add_special_tokens=False, truncation=True, )\n",
    "        # self.B_token = self.tokenizer.encode(text='B', add_special_tokens=False, truncation=True, )\n",
    "        # self.C_token = self.tokenizer.encode(text='C', add_special_tokens=False, truncation=True, )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        now_data = self.data.loc[index]\n",
    "        idx = now_data['id']\n",
    "        templete_part1 = \"<start_of_turn>user\\nHere are two question-answering dialogues. Compare two model performance on answering question, determine which is better.\\n\\n\"\n",
    "        templete_part1_input_ids = self.tokenizer(text=templete_part1, add_special_tokens=True, padding=False)['input_ids']\n",
    "        \n",
    "        templete_part2 = \"\\n###options\\nA. Model A\\nB. Model B\\nC. Tie\\n<end_of_turn>\\n\"\n",
    "        templete_part2_input_ids = self.tokenizer(text=templete_part2, add_special_tokens=True, padding=False)['input_ids'][1:]\n",
    "        #print(f\"templete_part2 is {templete_part2_input_ids}\")\n",
    "        templete_part3 = \"<start_of_turn>model\\n\"\n",
    "        templete_part3_input_ids = self.tokenizer(text=templete_part3, add_special_tokens=True, padding=False)['input_ids'][1:]\n",
    "        prompt_response = now_data['prompt_response']\n",
    "        #print(f\"id is {now_data['id']}\")\n",
    "        #print(prompt_response)\n",
    "        prompt_response_ids = self.tokenizer(text=prompt_response, add_special_tokens=True, truncation=True,\n",
    "                                          max_length=self.max_source_length, padding=False)['input_ids'][1:]\n",
    "        \n",
    "        input_ids = templete_part1_input_ids + prompt_response_ids + templete_part2_input_ids + templete_part3_input_ids\n",
    "        input_text = self.tokenizer.decode(input_ids, skip_special_tokens=False)\n",
    "        #print(f\"input is {self.tokenizer.decode(input_ids)}\")\n",
    "        return {\n",
    "            \"input_ids\": input_text,\n",
    "            \"id\": idx\n",
    "        }\n",
    "\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = {k: [item[k] for item in batch] for k in ('input_ids','id')}\n",
    "    #print(batch)\n",
    "    batch_input = tokenizer(\n",
    "        batch['input_ids'],\n",
    "        padding='longest',\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_LENGTH + 50\n",
    "    )\n",
    "    return batch_input, batch['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b57bcf4-b6e9-4f21-8671-b43700a1344f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_split_data\n",
    "data_path = \"dataset/1M/35k_in_1M.json\"\n",
    "prompt_type = 3\n",
    "MAX_INPUT = 1900\n",
    "if_train = False\n",
    "split = False\n",
    "if_drop_duplicate = False\n",
    "keep = 'last'\n",
    "df_train , df_valid = load_split_data(data_path, prompt_type, MAX_INPUT, if_train, split, False, if_drop_duplicate, 'last')\n",
    "test = df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f90ca5c-9abd-4619-8226-a0ac25d7cb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.read_json(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedce19c-7129-4a86-9eaf-16118e46cdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tmp.loc[tmp.id == '4af73ffd64'].prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c87a00-8d6b-419c-8358-e1c8dd8a5828",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['length'] = test['prompt_response'].apply(lambda x: len(x.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14275570-5606-496e-96b8-01e0d5eaa17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.sort_values(by = ['length'], ascending = False).reset_index(drop = True)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19164b06-76e6-45b8-97a6-80f517231fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def inference(model, test_dataloader):\n",
    "    test_predictions = []\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        batch_input, idx = batch\n",
    "        for k in batch_input.keys():\n",
    "            batch_input[k] = batch_input[k].to(device)\n",
    "        with torch.no_grad():\n",
    "            response = model.generate(**batch_input, max_new_tokens=1, return_dict_in_generate=True, output_scores=True)\n",
    "            #batch_input['input_ids'].shape[-1] + 1\n",
    "            score = response.scores[0]\n",
    "            A_prob, B_prob, C_prob = score[:,A_TOKEN_IDS], score[:,B_TOKEN_IDS], score[:,C_TOKEN_IDS]\n",
    "            logits = torch.cat([A_prob, B_prob, C_prob], dim=-1)\n",
    "            #logits = torch.Tensor([[A_prob,B_prob,C_prob]]) / 1.1\n",
    "            logits = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "            node_result = [[idx[i],logits[i]] for i in range(len(idx))]\n",
    "        test_predictions.extend(node_result)\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cc5551-8b74-4099-9d77-26ed2faadcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "base_model = 'google/gemma-2-9b-it'\n",
    "model_path = \"output/morning-waterfall-460/checkpoint-5200_888\"\n",
    "MAX_LENGTH = 1900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2936ddc-b99a-4c6e-85f0-3a1a5b273f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, truncation_side = 'left')\n",
    "config = AutoConfig.from_pretrained(base_model, trust_remote_code=True)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  \n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "base_model_0 = AutoModelForCausalLM.from_pretrained(base_model,\n",
    "                                                 config=config,\n",
    "                                                 quantization_config=bnb_config,\n",
    "                                                 torch_dtype=torch.float16,\n",
    "                                                 device_map=\"auto\",\n",
    "                                                 trust_remote_code=True)\n",
    "# base_model_0.config.pad_token_id = tokenizer.pad_token_id\n",
    "# base_model_0.resize_token_embeddings(len(tokenizer))\n",
    "new_model = model_path\n",
    "model0 = PeftModel.from_pretrained(base_model_0, new_model).to(device)\n",
    "#model0 = model0.merge_and_unload()\n",
    "model0.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716e3aa7-bc8b-48c2-a87c-751baeb5f884",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_TOKEN_IDS = tokenizer('A',add_special_tokens=True, truncation=True, max_length=1024)['input_ids'][1:]\n",
    "B_TOKEN_IDS = tokenizer('B',add_special_tokens=True, truncation=True, max_length=1024)['input_ids'][1:]\n",
    "C_TOKEN_IDS = tokenizer('C',add_special_tokens=True, truncation=True, max_length=1024)['input_ids'][1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5caf08-32ba-4b84-b13f-a5a0a21e5118",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "tokenized_dataset = InstructionDataSet(test, tokenizer, MAX_LENGTH, 1)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(tokenized_dataset, batch_size = batch_size ,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d36e303-5623-4fd6-8724-139701fa44ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_pred = inference(model = model0, test_dataloader = test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bfe0eb-4bbc-485f-87aa-704f6bdd5ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_pred[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcf6215-5ee6-40ff-a7d0-9fc64a9a8037",
   "metadata": {},
   "outputs": [],
   "source": [
    "if batch_size != 1:\n",
    "    # 提取数据\n",
    "    processed_data = []\n",
    "    for item in sub_pred:\n",
    "        #item = item[0]\n",
    "        id = item[0]#.item()  # 获取id\n",
    "        array_values = item[1].tolist()  # 获取array并转换为列表\n",
    "        processed_data.append([id] + array_values)\n",
    "    \n",
    "\n",
    "else:\n",
    "    # 提取数据\n",
    "    processed_data = []\n",
    "\n",
    "    \n",
    "    for item in sub_pred:\n",
    "        item = item[0]\n",
    "        id = item[0].item()  # 获取id\n",
    "        array_values = item[1].tolist()  # 获取array并转换为列表\n",
    "        processed_data.append([id] + array_values)\n",
    "\n",
    "new_columns = ['id', 'winner_model_a', 'winner_model_b', 'winner_tie']\n",
    "df = pd.DataFrame(processed_data, columns=new_columns)\n",
    "df = df.groupby('id').mean().reset_index()\n",
    "\n",
    "prediction = np.array(df[new_columns[1:]])\n",
    "test = test.drop_duplicates(subset = ['id']).reset_index(drop = True)\n",
    "test = test.sort_values(by = ['id']).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4548df7d-898f-4a11-8a2f-8579489754a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a36dec6-180f-41fa-8925-3603f4f0786b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = data.merge(df, how = 'left', on = 'id')\n",
    "assert len(final) == len(data) == len(df)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76092f6-ebca-40cf-890c-967351c29c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a444e5d1-741a-483b-800f-bd7bbecb9387",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_json(\"dataset/persudo_label/35k_in_1M_prediction.json\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbe5072-2566-4dee-b672-09a3ccf279c7",
   "metadata": {},
   "source": [
    "# 合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07643a6b-bec0-41da-8319-1ce9f53045d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pd.read_csv(\"dataset/prediction.csv\")\n",
    "ex = pd.read_json(\"dataset/persudo_label/35k_in_1M_prediction_thr0_35k.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc433f92-0a0b-4226-adb8-6b32351cc18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = p.rename(columns = {'winner_model_a':\"p_winner_model_a\", 'winner_model_b':\"p_winner_model_b\",  'winner_tie':\"p_winner_tie\"})\n",
    "final = pd.concat([ex, p], axis = 1)\n",
    "final = final.drop(columns= ['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a084b1a-5939-4675-b695-28d0542a0142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_label(row):\n",
    "    a = row.p_winner_model_a\n",
    "    b = row.p_winner_model_b\n",
    "    c = row.p_winner_tie\n",
    "\n",
    "    l = [a ,b, c]\n",
    "    label = l.index(max(l))\n",
    "    return label\n",
    "\n",
    "final['p_label'] = final.apply(get_p_label, axis = 1)\n",
    "\n",
    "def get_label(row):\n",
    "    label = [idx for idx, option in enumerate(['winner_model_a','winner_model_b','winner_tie']) if row[option] == 1]\n",
    "    return label[-1]\n",
    "\n",
    "final['label'] = final.apply(get_label, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3afd29-0199-4658-a5ad-68dab36b399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold1 = 0.9\n",
    "filter_same = final.loc[final.p_label == final.label,:].reset_index(drop = True)\n",
    "filter_list = (filter_same.p_winner_model_a >= threshold1) | (filter_same.p_winner_model_b >= threshold1) | (filter_same.p_winner_tie >= threshold1)\n",
    "filter_same = filter_same.loc[filter_list,:].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6414821-a739-4fea-a27b-e05df1d2fb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_list = (filter_same.difference >= 1) | (filter_same.winner_tie == 1)\n",
    "filter_same = filter_same.loc[filter_list,:].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0492d7-2278-469e-9076-30e85dce79ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold2 = 0.6\n",
    "filter_dif = final.loc[final.p_label != final.label,:].reset_index(drop = True)\n",
    "filter_list = (filter_dif.p_winner_model_a >= threshold2) | (filter_dif.p_winner_model_b >= threshold2) | (filter_dif.p_winner_tie >= threshold2)\n",
    "filter_dif = filter_dif.loc[filter_list,:].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35cb084-ab73-4068-a6a0-9df095f673e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_list = (filter_dif.difference >= 1) | (filter_dif.winner_tie == 1)\n",
    "filter_dif = filter_dif.loc[filter_list,:].reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9164dba-d5bb-483a-94a0-b8aba849c6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d070f0-a45a-453f-a770-ba4b1c9e905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d56a3b0-e34c-4fbe-bcdb-2d28cd3010f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dif['id'] = [randint(100,999999) + i for i in range(len(filter_dif))]\n",
    "filter_same['id'] = [randint(100,999999) + i for i in range(len(filter_same))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203da592-cd71-4ade-8c2a-4b1626942be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_columns = ['prompt', 'model_a', 'model_b', 'winner_model_a', 'winner_model_b', 'winner_tie', 'response_a', 'response_b', 'id']\n",
    "filter_dif[save_columns].to_json(f'dataset/70k_dif_thr{int(threshold2 * 100)}.json', index = False)\n",
    "filter_same[save_columns].to_json(f'dataset/70k_same_thr{int(threshold1 * 100)}.json', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0cfd5c-8e89-4a57-bdd5-1ee566bf2ad6",
   "metadata": {},
   "source": [
    "# 合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3462dd66-6cb8-4ea8-90b5-8c068f0d485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pd.read_csv(\"dataset/prediction.csv\")\n",
    "ex = pd.read_json(\"dataset/persudo_label/3k_high_quality_method_2_prediction_thr0_3k.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "022d22b2-a546-4a92-b9d8-48ddaa1b3032",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = p.rename(columns = {'winner_model_a':\"p_winner_model_a\", 'winner_model_b':\"p_winner_model_b\",  'winner_tie':\"p_winner_tie\"})\n",
    "final = ex.merge(p, on = ['id'], how = 'inner')\n",
    "#final = final.drop(columns= ['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "407852eb-90a4-4b3b-beb8-513cc8a95800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(row):\n",
    "    val = [row[option] for idx, option in enumerate(['p_winner_model_a','p_winner_model_b','p_winner_tie'])]\n",
    "    label = val.index(max(val))\n",
    "    #label = [idx for idx, option in enumerate(['winner_model_a','winner_model_b','winner_tie']) if row[option] == 1]\n",
    "    if label == 0:\n",
    "        return 'A'\n",
    "    elif label == 1:\n",
    "        return 'B'\n",
    "    else:\n",
    "        return 'C'\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "82a267cc-2fc2-4636-9138-0b06e47843f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final[['winner_model_a','winner_model_b','winner_tie']] = final[['p_winner_model_a','p_winner_model_b','p_winner_tie']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "88757eb5-ef3e-489d-8fa4-3a42acadb0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = final.drop(columns = ['p_winner_model_a','p_winner_model_b','p_winner_tie'])\n",
    "tmp.to_json('dataset/persudo_label/35k_in_1M_prediction_thr0_35k_by_885_model_soft.json', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ee3101bd-dfdd-41e8-a305-272c80167449",
   "metadata": {},
   "outputs": [],
   "source": [
    "final['label'] = final.apply(lambda x: get_label(x), axis = 1)\n",
    "final[\"winner_model_a\"] = 0\n",
    "final[\"winner_model_b\"] = 0\n",
    "final[\"winner_tie\"] = 0\n",
    "\n",
    "final.loc[final.label == 'A',\"winner_model_a\"] = 1\n",
    "final.loc[final.label == 'B',\"winner_model_b\"] = 1\n",
    "final.loc[final.label == 'C',\"winner_tie\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "29eb70ac-be82-4853-a46e-7fb3b020c4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = final.drop(columns = ['label','p_winner_model_a','p_winner_model_b','p_winner_tie'])\n",
    "final.to_json('dataset/persudo_label/3k_high_quality_method_2_prediction_thr0_3k_by_885_model.json', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ef82709d-258e-4b5b-a554-abdb874b537a",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold1 = 0.6\n",
    "filter_same = final\n",
    "filter_list = (filter_same.p_winner_model_a >= threshold1) | (filter_same.p_winner_model_b >= threshold1) | (filter_same.p_winner_tie >= threshold1)\n",
    "filter_same = filter_same.loc[filter_list,:].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e105034e-b34b-4a18-8d37-37d747d7b354",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_same = filter_same.drop(columns = ['label','p_winner_model_a','p_winner_model_b','p_winner_tie'])\n",
    "filter_same.to_json('dataset/persudo_label/35k_in_1M_prediction_thr60_35k_by_885_model.json', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d85e80-74fe-400e-b35f-2203eca9e67e",
   "metadata": {},
   "source": [
    "# 检查是否与valid重复"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5980197d-491a-4341-82a2-3f00a1e3a01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查\n",
    "\n",
    "valid = pd.read_json(\"dataset/non_overlap/valid.json\")\n",
    "filter_dif = pd.read_json(\"dataset/70k_dif_thr60.json\")\n",
    "filter_same = pd.read_json(\"dataset/persudo_label/3k_high_quality_method_2_prediction_thr0_3k.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a70bacfa-2971-41f8-b6d4-b09f9ac69792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set_prompt_response(data):\n",
    "    set_prompt_response = []\n",
    "    for i in data.itertuples():\n",
    "        prompt_response = i.prompt + i.response_a + i.response_b\n",
    "        set_prompt_response.append(set(prompt_response))\n",
    "    data['set_prompt_response'] = set_prompt_response  \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b52169e3-4352-4cad-9ba5-34bb508ecc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = get_set_prompt_response(valid)\n",
    "filter_dif = get_set_prompt_response(filter_dif)\n",
    "filter_same = get_set_prompt_response(filter_same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2049c0af-537e-437f-bc66-35e84d342acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid和任何都不重合\n",
    "assert len([idx for idx, i in enumerate(valid.set_prompt_response.values) if i in filter_dif.set_prompt_response.values]) == 0\n",
    "assert len([idx for idx, i in enumerate(valid.set_prompt_response.values) if i in filter_same.set_prompt_response.values]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5cdae1c-3902-43d6-86f2-75e16982bde6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([idx for idx, i in enumerate(valid.set_prompt_response.values) if i in filter_same.set_prompt_response.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14e40f8b-74ae-4cf2-9958-baaa324bf4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('dataset/persudo_label/3k_high_quality_method_2_prediction.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49854286-e1b9-42c6-a974-ad68ed2811e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>response_a_categories</th>\n",
       "      <th>response_b_categories</th>\n",
       "      <th>set_prompt_response</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1890</td>\n",
       "      <td>[She felt something and likes me but loves oth...</td>\n",
       "      <td>stablelm-tuned-alpha-7b</td>\n",
       "      <td>vicuna-13b</td>\n",
       "      <td>[It sounds like you and the other person have ...</td>\n",
       "      <td>[It's difficult to say whether it was a smart ...</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>[It sounds like you and the other person have ...</td>\n",
       "      <td>0.145897</td>\n",
       "      <td>0.463661</td>\n",
       "      <td>0.390442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1891</td>\n",
       "      <td>[She felt something and likes me but loves oth...</td>\n",
       "      <td>stablelm-tuned-alpha-7b</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>[It sounds like you and the other person have ...</td>\n",
       "      <td>[It's understandable that you would feel angry...</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>[It sounds like you and the other person have ...</td>\n",
       "      <td>0.133975</td>\n",
       "      <td>0.274899</td>\n",
       "      <td>0.591126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2238</td>\n",
       "      <td>[write a single dot.]</td>\n",
       "      <td>alpaca-13b</td>\n",
       "      <td>vicuna-13b</td>\n",
       "      <td>[.]</td>\n",
       "      <td>[.]</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>[write a single dot., .]</td>\n",
       "      <td>0.026940</td>\n",
       "      <td>0.023406</td>\n",
       "      <td>0.949655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2946</td>\n",
       "      <td>[what is your thoughts on sex?]</td>\n",
       "      <td>stablelm-tuned-alpha-7b</td>\n",
       "      <td>fastchat-t5-3b</td>\n",
       "      <td>[As an AI language model, I do not have person...</td>\n",
       "      <td>[As an AI language model, I don't have persona...</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>[what is your thoughts on sex?, As an AI langu...</td>\n",
       "      <td>0.609865</td>\n",
       "      <td>0.127835</td>\n",
       "      <td>0.262300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022</td>\n",
       "      <td>[Write a single # character]</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>claude-1</td>\n",
       "      <td>[\\#]</td>\n",
       "      <td>[#]</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>[#, Write a single # character, \\#]</td>\n",
       "      <td>0.076143</td>\n",
       "      <td>0.117933</td>\n",
       "      <td>0.805924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2514</th>\n",
       "      <td>2831</td>\n",
       "      <td>[Explain what CHOAM is from the Dune book seri...</td>\n",
       "      <td>RWKV-4-Raven-14B</td>\n",
       "      <td>vicuna-13b</td>\n",
       "      <td>[CHOAM is a powerful corporation in the Dune u...</td>\n",
       "      <td>[In the Dune book series by NAME_1, CHOAM is a...</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>[Explain what CHOAM is from the Dune book seri...</td>\n",
       "      <td>0.053634</td>\n",
       "      <td>0.775920</td>\n",
       "      <td>0.170447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2515</th>\n",
       "      <td>3033</td>\n",
       "      <td>[How do I determine the angular momentum of th...</td>\n",
       "      <td>vicuna-13b</td>\n",
       "      <td>claude-instant-1</td>\n",
       "      <td>[The angular momentum of the Sun is determined...</td>\n",
       "      <td>[The angular momentum of an object is calculat...</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>[The angular momentum of an object is calculat...</td>\n",
       "      <td>0.252608</td>\n",
       "      <td>0.423038</td>\n",
       "      <td>0.324354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2516</th>\n",
       "      <td>3118</td>\n",
       "      <td>[we are the 6 May 2023, add 3 weeks]</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>claude-1</td>\n",
       "      <td>[Sure, I'm sorry but I am not able to calculat...</td>\n",
       "      <td>[3 weeks from May 6, 2023 is May 27, 2023.]</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>[3 weeks from May 6, 2023 is May 27, 2023., Su...</td>\n",
       "      <td>0.008541</td>\n",
       "      <td>0.927358</td>\n",
       "      <td>0.064101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2517</th>\n",
       "      <td>2918</td>\n",
       "      <td>[* You are a product owner, and you will like ...</td>\n",
       "      <td>vicuna-7b</td>\n",
       "      <td>alpaca-13b</td>\n",
       "      <td>[Here's an example of a high-level product bac...</td>\n",
       "      <td>[• Support the Branch Relationship Managers (B...</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>[Here's an example of a high-level product bac...</td>\n",
       "      <td>0.550335</td>\n",
       "      <td>0.150453</td>\n",
       "      <td>0.299212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2518</th>\n",
       "      <td>2989</td>\n",
       "      <td>[Can you come up with three concise statements...</td>\n",
       "      <td>chatglm-6b</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>[Sure, here are three concise statements that ...</td>\n",
       "      <td>[1. Both require regular updates and maintenan...</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>[Can you come up with three concise statements...</td>\n",
       "      <td>0.013196</td>\n",
       "      <td>0.896628</td>\n",
       "      <td>0.090176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2519 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                             prompt  \\\n",
       "0     1890  [She felt something and likes me but loves oth...   \n",
       "1     1891  [She felt something and likes me but loves oth...   \n",
       "2     2238                              [write a single dot.]   \n",
       "3     2946                    [what is your thoughts on sex?]   \n",
       "4     2022                       [Write a single # character]   \n",
       "...    ...                                                ...   \n",
       "2514  2831  [Explain what CHOAM is from the Dune book seri...   \n",
       "2515  3033  [How do I determine the angular momentum of th...   \n",
       "2516  3118               [we are the 6 May 2023, add 3 weeks]   \n",
       "2517  2918  [* You are a product owner, and you will like ...   \n",
       "2518  2989  [Can you come up with three concise statements...   \n",
       "\n",
       "                      model_a           model_b  \\\n",
       "0     stablelm-tuned-alpha-7b        vicuna-13b   \n",
       "1     stablelm-tuned-alpha-7b         koala-13b   \n",
       "2                  alpaca-13b        vicuna-13b   \n",
       "3     stablelm-tuned-alpha-7b    fastchat-t5-3b   \n",
       "4                   koala-13b          claude-1   \n",
       "...                       ...               ...   \n",
       "2514         RWKV-4-Raven-14B        vicuna-13b   \n",
       "2515               vicuna-13b  claude-instant-1   \n",
       "2516                koala-13b          claude-1   \n",
       "2517                vicuna-7b        alpaca-13b   \n",
       "2518               chatglm-6b             gpt-4   \n",
       "\n",
       "                                             response_a  \\\n",
       "0     [It sounds like you and the other person have ...   \n",
       "1     [It sounds like you and the other person have ...   \n",
       "2                                                   [.]   \n",
       "3     [As an AI language model, I do not have person...   \n",
       "4                                                  [\\#]   \n",
       "...                                                 ...   \n",
       "2514  [CHOAM is a powerful corporation in the Dune u...   \n",
       "2515  [The angular momentum of the Sun is determined...   \n",
       "2516  [Sure, I'm sorry but I am not able to calculat...   \n",
       "2517  [Here's an example of a high-level product bac...   \n",
       "2518  [Sure, here are three concise statements that ...   \n",
       "\n",
       "                                             response_b response_a_categories  \\\n",
       "0     [It's difficult to say whether it was a smart ...                  Good   \n",
       "1     [It's understandable that you would feel angry...                  Good   \n",
       "2                                                   [.]                  Good   \n",
       "3     [As an AI language model, I don't have persona...                  Good   \n",
       "4                                                   [#]                  Good   \n",
       "...                                                 ...                   ...   \n",
       "2514  [In the Dune book series by NAME_1, CHOAM is a...                  Good   \n",
       "2515  [The angular momentum of an object is calculat...                  Good   \n",
       "2516        [3 weeks from May 6, 2023 is May 27, 2023.]                  Good   \n",
       "2517  [• Support the Branch Relationship Managers (B...                  Good   \n",
       "2518  [1. Both require regular updates and maintenan...                  Good   \n",
       "\n",
       "     response_b_categories                                set_prompt_response  \\\n",
       "0                     Good  [It sounds like you and the other person have ...   \n",
       "1                     Good  [It sounds like you and the other person have ...   \n",
       "2                     Good                           [write a single dot., .]   \n",
       "3                     Good  [what is your thoughts on sex?, As an AI langu...   \n",
       "4                     Good                [#, Write a single # character, \\#]   \n",
       "...                    ...                                                ...   \n",
       "2514                  Good  [Explain what CHOAM is from the Dune book seri...   \n",
       "2515                  Good  [The angular momentum of an object is calculat...   \n",
       "2516                  Good  [3 weeks from May 6, 2023 is May 27, 2023., Su...   \n",
       "2517                  Good  [Here's an example of a high-level product bac...   \n",
       "2518                  Good  [Can you come up with three concise statements...   \n",
       "\n",
       "      winner_model_a  winner_model_b  winner_tie  \n",
       "0           0.145897        0.463661    0.390442  \n",
       "1           0.133975        0.274899    0.591126  \n",
       "2           0.026940        0.023406    0.949655  \n",
       "3           0.609865        0.127835    0.262300  \n",
       "4           0.076143        0.117933    0.805924  \n",
       "...              ...             ...         ...  \n",
       "2514        0.053634        0.775920    0.170447  \n",
       "2515        0.252608        0.423038    0.324354  \n",
       "2516        0.008541        0.927358    0.064101  \n",
       "2517        0.550335        0.150453    0.299212  \n",
       "2518        0.013196        0.896628    0.090176  \n",
       "\n",
       "[2519 rows x 12 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca7e24e-66a9-43c2-b9d3-cc58be2e4ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
