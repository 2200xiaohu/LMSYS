{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ac7f010-4ef6-4b50-8b20-8a98b8a44895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from typing import Optional, Union\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    EarlyStoppingCallback,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForMultipleChoice,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    RobertaForMultipleChoice,\n",
    "    AutoModelForSequenceClassification,\n",
    "    LlamaModel,\n",
    "    LlamaForSequenceClassification,\n",
    "    BitsAndBytesConfig,\n",
    "    get_polynomial_decay_schedule_with_warmup,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    TrainerCallback,\n",
    ")\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "\n",
    "from peft import (\n",
    "    get_peft_config,\n",
    "    PeftModel,\n",
    "    PeftConfig,\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    ")\n",
    "import os\n",
    "\n",
    "import random\n",
    "from random import randint\n",
    "def seed_everything(seed=None):\n",
    "    '''\n",
    "    固定seed\n",
    "    :param seed: int, 随机种子\n",
    "    '''\n",
    "    max_seed_value = np.iinfo(np.uint32).max\n",
    "    min_seed_value = np.iinfo(np.uint32).min\n",
    "\n",
    "    if (seed is None) or not (min_seed_value <= seed <= max_seed_value):\n",
    "        seed = random.randint(np.iinfo(np.uint32).min, np.iinfo(np.uint32).max)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    return seed\n",
    "\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "from utils import load_split_data, load_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d97a2bfe-0604-4dd2-9db9-b0f1175793ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class InstructionDataSet(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_source_length, max_target_length):\n",
    "        super(InstructionDataSet, self).__init__()\n",
    "        #self.data = data.sample(len(data), random_state=0).reset_index(drop=True)\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_source_length = max_source_length\n",
    "        self.max_target_length = max_target_length\n",
    "        # self.A_token = self.tokenizer.encode(text='A', add_special_tokens=False, truncation=True, )\n",
    "        # self.B_token = self.tokenizer.encode(text='B', add_special_tokens=False, truncation=True, )\n",
    "        # self.C_token = self.tokenizer.encode(text='C', add_special_tokens=False, truncation=True, )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        now_data = self.data.loc[index]\n",
    "        idx = now_data['id']\n",
    "        templete_part1 = \"<start_of_turn>user\\nHere are two question-answering dialogues. Compare two model performance on answering question, determine which is better.\\n\\n\"\n",
    "        templete_part1_input_ids = self.tokenizer(text=templete_part1, add_special_tokens=True, padding=False)['input_ids']\n",
    "        \n",
    "        templete_part2 = \"\\n###options\\nA. Model A\\nB. Model B\\nC. Tie\\n<end_of_turn>\\n\"\n",
    "        templete_part2_input_ids = self.tokenizer(text=templete_part2, add_special_tokens=True, padding=False)['input_ids'][1:]\n",
    "        #print(f\"templete_part2 is {templete_part2_input_ids}\")\n",
    "        templete_part3 = \"<start_of_turn>model\\n\"\n",
    "        templete_part3_input_ids = self.tokenizer(text=templete_part3, add_special_tokens=True, padding=False)['input_ids'][1:]\n",
    "        prompt_response = now_data['prompt_response']\n",
    "        #print(f\"id is {now_data['id']}\")\n",
    "        #print(prompt_response)\n",
    "        prompt_response_ids = self.tokenizer(text=prompt_response, add_special_tokens=True, truncation=True,\n",
    "                                          max_length=self.max_source_length, padding=False)['input_ids'][1:]\n",
    "        \n",
    "        input_ids = templete_part1_input_ids + prompt_response_ids + templete_part2_input_ids + templete_part3_input_ids\n",
    "        input_text = self.tokenizer.decode(input_ids, skip_special_tokens=False)\n",
    "        #print(f\"input is {self.tokenizer.decode(input_ids)}\")\n",
    "        return {\n",
    "            \"input_ids\": input_text,\n",
    "            \"id\": idx\n",
    "        }\n",
    "\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = {k: [item[k] for item in batch] for k in ('input_ids','id')}\n",
    "    #print(batch)\n",
    "    batch_input = tokenizer(\n",
    "        batch['input_ids'],\n",
    "        padding='longest',\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_LENGTH + 50\n",
    "    )\n",
    "    return batch_input, batch['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b57bcf4-b6e9-4f21-8671-b43700a1344f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37885/37885 [00:16<00:00, 2277.73it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils import load_split_data\n",
    "data_path = \"dataset/1M/35k_in_1M.json\"\n",
    "prompt_type = 3\n",
    "MAX_INPUT = 1900\n",
    "if_train = False\n",
    "split = False\n",
    "if_drop_duplicate = False\n",
    "keep = 'last'\n",
    "df_train , df_valid = load_split_data(data_path, prompt_type, MAX_INPUT, if_train, split, False, if_drop_duplicate, 'last')\n",
    "test = df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f90ca5c-9abd-4619-8226-a0ac25d7cb15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34348    [Step 1. You are a senior JAVA CRUD engineer a...\n",
       "Name: prompt, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = pd.read_json(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dedce19c-7129-4a86-9eaf-16118e46cdc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tmp.loc[tmp.id == '4af73ffd64'].prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2c87a00-8d6b-419c-8358-e1c8dd8a5828",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['length'] = test['prompt_response'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14275570-5606-496e-96b8-01e0d5eaa17c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt_response</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4af73ffd64</td>\n",
       "      <td>#Prompt\\nThe code is incomplete, please comple...</td>\n",
       "      <td>13398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>519567d2a4</td>\n",
       "      <td>#Prompt\\nGiven two datasets with attributes ['...</td>\n",
       "      <td>13106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26dc950ef0</td>\n",
       "      <td>#Prompt\\nExtract information as it is from tex...</td>\n",
       "      <td>12989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5abb109a25</td>\n",
       "      <td>#Prompt\\nMake a Roblox Script. If you are pres...</td>\n",
       "      <td>11427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d56ad2e1d2</td>\n",
       "      <td>#Prompt\\nGiven two datasets with attributes \"m...</td>\n",
       "      <td>11248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34935</th>\n",
       "      <td>689b1a79fb</td>\n",
       "      <td>#Prompt\\nWrite out the numbers from 1 to 10, b...</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34936</th>\n",
       "      <td>342742f88d</td>\n",
       "      <td>#Prompt\\nWho was the drummer of Pearl Jam betw...</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34937</th>\n",
       "      <td>657f7b3f2a</td>\n",
       "      <td>#Prompt\\nwhat is heavier: 1 kilo of iron or 2 ...</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34938</th>\n",
       "      <td>6b6ff2dd15</td>\n",
       "      <td>#Prompt\\nWhat's the sum of all integer values ...</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34939</th>\n",
       "      <td>fdbc5909c9</td>\n",
       "      <td>#Prompt\\nI was born in 1963. How old am I toda...</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34940 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                    prompt_response  length\n",
       "0      4af73ffd64  #Prompt\\nThe code is incomplete, please comple...   13398\n",
       "1      519567d2a4  #Prompt\\nGiven two datasets with attributes ['...   13106\n",
       "2      26dc950ef0  #Prompt\\nExtract information as it is from tex...   12989\n",
       "3      5abb109a25  #Prompt\\nMake a Roblox Script. If you are pres...   11427\n",
       "4      d56ad2e1d2  #Prompt\\nGiven two datasets with attributes \"m...   11248\n",
       "...           ...                                                ...     ...\n",
       "34935  689b1a79fb  #Prompt\\nWrite out the numbers from 1 to 10, b...     213\n",
       "34936  342742f88d  #Prompt\\nWho was the drummer of Pearl Jam betw...     213\n",
       "34937  657f7b3f2a  #Prompt\\nwhat is heavier: 1 kilo of iron or 2 ...     209\n",
       "34938  6b6ff2dd15  #Prompt\\nWhat's the sum of all integer values ...     195\n",
       "34939  fdbc5909c9  #Prompt\\nI was born in 1963. How old am I toda...     192\n",
       "\n",
       "[34940 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = test.sort_values(by = ['length'], ascending = False).reset_index(drop = True)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19164b06-76e6-45b8-97a6-80f517231fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def inference(model, test_dataloader):\n",
    "    test_predictions = []\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        batch_input, idx = batch\n",
    "        for k in batch_input.keys():\n",
    "            batch_input[k] = batch_input[k].to(device)\n",
    "        with torch.no_grad():\n",
    "            response = model.generate(**batch_input, max_new_tokens=1, return_dict_in_generate=True, output_scores=True)\n",
    "            #batch_input['input_ids'].shape[-1] + 1\n",
    "            score = response.scores[0]\n",
    "            A_prob, B_prob, C_prob = score[:,A_TOKEN_IDS], score[:,B_TOKEN_IDS], score[:,C_TOKEN_IDS]\n",
    "            logits = torch.cat([A_prob, B_prob, C_prob], dim=-1)\n",
    "            #logits = torch.Tensor([[A_prob,B_prob,C_prob]]) / 1.1\n",
    "            logits = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "            node_result = [[idx[i],logits[i]] for i in range(len(idx))]\n",
    "        test_predictions.extend(node_result)\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52cc5551-8b74-4099-9d77-26ed2faadcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "base_model = 'google/gemma-2-9b-it'\n",
    "model_path = \"output/morning-waterfall-460/checkpoint-5200_888\"\n",
    "MAX_LENGTH = 1900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2936ddc-b99a-4c6e-85f0-3a1a5b273f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f5556a2f42d41bb95b6a9d0e4ed6874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Gemma2ForCausalLM(\n",
       "      (model): Gemma2Model(\n",
       "        (embed_tokens): Embedding(256000, 3584, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-41): 42 x Gemma2DecoderLayer(\n",
       "            (self_attn): Gemma2SdpaAttention(\n",
       "              (q_proj): Linear4bit(\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (base_layer): Linear4bit(in_features=3584, out_features=4096, bias=False)\n",
       "              )\n",
       "              (k_proj): Linear4bit(\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (base_layer): Linear4bit(in_features=3584, out_features=2048, bias=False)\n",
       "              )\n",
       "              (v_proj): Linear4bit(\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (base_layer): Linear4bit(in_features=3584, out_features=2048, bias=False)\n",
       "              )\n",
       "              (o_proj): Linear4bit(\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=3584, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=3584, bias=False)\n",
       "              )\n",
       "              (rotary_emb): Gemma2RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Gemma2MLP(\n",
       "              (gate_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=3584, bias=False)\n",
       "              (act_fn): PytorchGELUTanh()\n",
       "            )\n",
       "            (input_layernorm): Gemma2RMSNorm()\n",
       "            (post_attention_layernorm): Gemma2RMSNorm()\n",
       "            (pre_feedforward_layernorm): Gemma2RMSNorm()\n",
       "            (post_feedforward_layernorm): Gemma2RMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): Gemma2RMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3584, out_features=256000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, truncation_side = 'left')\n",
    "config = AutoConfig.from_pretrained(base_model, trust_remote_code=True)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  \n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "base_model_0 = AutoModelForCausalLM.from_pretrained(base_model,\n",
    "                                                 config=config,\n",
    "                                                 quantization_config=bnb_config,\n",
    "                                                 torch_dtype=torch.float16,\n",
    "                                                 device_map=\"auto\",\n",
    "                                                 trust_remote_code=True)\n",
    "# base_model_0.config.pad_token_id = tokenizer.pad_token_id\n",
    "# base_model_0.resize_token_embeddings(len(tokenizer))\n",
    "new_model = model_path\n",
    "model0 = PeftModel.from_pretrained(base_model_0, new_model).to(device)\n",
    "#model0 = model0.merge_and_unload()\n",
    "model0.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "716e3aa7-bc8b-48c2-a87c-751baeb5f884",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_TOKEN_IDS = tokenizer('A',add_special_tokens=True, truncation=True, max_length=1024)['input_ids'][1:]\n",
    "B_TOKEN_IDS = tokenizer('B',add_special_tokens=True, truncation=True, max_length=1024)['input_ids'][1:]\n",
    "C_TOKEN_IDS = tokenizer('C',add_special_tokens=True, truncation=True, max_length=1024)['input_ids'][1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af5caf08-32ba-4b84-b13f-a5a0a21e5118",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "tokenized_dataset = InstructionDataSet(test, tokenizer, MAX_LENGTH, 1)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(tokenized_dataset, batch_size = batch_size ,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d36e303-5623-4fd6-8724-139701fa44ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 8450/8735 [1:15:54<00:58,  4.88it/s]"
     ]
    }
   ],
   "source": [
    "sub_pred = inference(model = model0, test_dataloader = test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfcf6215-5ee6-40ff-a7d0-9fc64a9a8037",
   "metadata": {},
   "outputs": [],
   "source": [
    "if batch_size != 1:\n",
    "    # 提取数据\n",
    "    processed_data = []\n",
    "    for item in sub_pred:\n",
    "        #item = item[0]\n",
    "        id = item[0].item()  # 获取id\n",
    "        array_values = item[1].tolist()  # 获取array并转换为列表\n",
    "        processed_data.append([id] + array_values)\n",
    "    \n",
    "\n",
    "else:\n",
    "    # 提取数据\n",
    "    processed_data = []\n",
    "\n",
    "    \n",
    "    for item in sub_pred:\n",
    "        item = item[0]\n",
    "        id = item[0].item()  # 获取id\n",
    "        array_values = item[1].tolist()  # 获取array并转换为列表\n",
    "        processed_data.append([id] + array_values)\n",
    "\n",
    "new_columns = ['id', 'winner_model_a', 'winner_model_b', 'winner_tie']\n",
    "df = pd.DataFrame(processed_data, columns=new_columns)\n",
    "df = df.groupby('id').mean().reset_index()\n",
    "\n",
    "prediction = np.array(df[new_columns[1:]])\n",
    "test = test.drop_duplicates(subset = ['id']).reset_index(drop = True)\n",
    "test = test.sort_values(by = ['id']).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4548df7d-898f-4a11-8a2f-8579489754a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a36dec6-180f-41fa-8925-3603f4f0786b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>response_a_categories</th>\n",
       "      <th>response_b_categories</th>\n",
       "      <th>set_prompt_response</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1890</td>\n",
       "      <td>[She felt something and likes me but loves oth...</td>\n",
       "      <td>stablelm-tuned-alpha-7b</td>\n",
       "      <td>vicuna-13b</td>\n",
       "      <td>[It sounds like you and the other person have ...</td>\n",
       "      <td>[It's difficult to say whether it was a smart ...</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>[It sounds like you and the other person have ...</td>\n",
       "      <td>0.145897</td>\n",
       "      <td>0.463661</td>\n",
       "      <td>0.390442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1891</td>\n",
       "      <td>[She felt something and likes me but loves oth...</td>\n",
       "      <td>stablelm-tuned-alpha-7b</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>[It sounds like you and the other person have ...</td>\n",
       "      <td>[It's understandable that you would feel angry...</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>[It sounds like you and the other person have ...</td>\n",
       "      <td>0.133975</td>\n",
       "      <td>0.274899</td>\n",
       "      <td>0.591126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2238</td>\n",
       "      <td>[write a single dot.]</td>\n",
       "      <td>alpaca-13b</td>\n",
       "      <td>vicuna-13b</td>\n",
       "      <td>[.]</td>\n",
       "      <td>[.]</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>[write a single dot., .]</td>\n",
       "      <td>0.026940</td>\n",
       "      <td>0.023406</td>\n",
       "      <td>0.949655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2946</td>\n",
       "      <td>[what is your thoughts on sex?]</td>\n",
       "      <td>stablelm-tuned-alpha-7b</td>\n",
       "      <td>fastchat-t5-3b</td>\n",
       "      <td>[As an AI language model, I do not have person...</td>\n",
       "      <td>[As an AI language model, I don't have persona...</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>[what is your thoughts on sex?, As an AI langu...</td>\n",
       "      <td>0.609865</td>\n",
       "      <td>0.127835</td>\n",
       "      <td>0.262300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022</td>\n",
       "      <td>[Write a single # character]</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>claude-1</td>\n",
       "      <td>[\\#]</td>\n",
       "      <td>[#]</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>[#, Write a single # character, \\#]</td>\n",
       "      <td>0.076143</td>\n",
       "      <td>0.117933</td>\n",
       "      <td>0.805924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2514</th>\n",
       "      <td>2831</td>\n",
       "      <td>[Explain what CHOAM is from the Dune book seri...</td>\n",
       "      <td>RWKV-4-Raven-14B</td>\n",
       "      <td>vicuna-13b</td>\n",
       "      <td>[CHOAM is a powerful corporation in the Dune u...</td>\n",
       "      <td>[In the Dune book series by NAME_1, CHOAM is a...</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>[Explain what CHOAM is from the Dune book seri...</td>\n",
       "      <td>0.053634</td>\n",
       "      <td>0.775920</td>\n",
       "      <td>0.170447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2515</th>\n",
       "      <td>3033</td>\n",
       "      <td>[How do I determine the angular momentum of th...</td>\n",
       "      <td>vicuna-13b</td>\n",
       "      <td>claude-instant-1</td>\n",
       "      <td>[The angular momentum of the Sun is determined...</td>\n",
       "      <td>[The angular momentum of an object is calculat...</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>[The angular momentum of an object is calculat...</td>\n",
       "      <td>0.252608</td>\n",
       "      <td>0.423038</td>\n",
       "      <td>0.324354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2516</th>\n",
       "      <td>3118</td>\n",
       "      <td>[we are the 6 May 2023, add 3 weeks]</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>claude-1</td>\n",
       "      <td>[Sure, I'm sorry but I am not able to calculat...</td>\n",
       "      <td>[3 weeks from May 6, 2023 is May 27, 2023.]</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>[3 weeks from May 6, 2023 is May 27, 2023., Su...</td>\n",
       "      <td>0.008541</td>\n",
       "      <td>0.927358</td>\n",
       "      <td>0.064101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2517</th>\n",
       "      <td>2918</td>\n",
       "      <td>[* You are a product owner, and you will like ...</td>\n",
       "      <td>vicuna-7b</td>\n",
       "      <td>alpaca-13b</td>\n",
       "      <td>[Here's an example of a high-level product bac...</td>\n",
       "      <td>[• Support the Branch Relationship Managers (B...</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>[Here's an example of a high-level product bac...</td>\n",
       "      <td>0.550335</td>\n",
       "      <td>0.150453</td>\n",
       "      <td>0.299212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2518</th>\n",
       "      <td>2989</td>\n",
       "      <td>[Can you come up with three concise statements...</td>\n",
       "      <td>chatglm-6b</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>[Sure, here are three concise statements that ...</td>\n",
       "      <td>[1. Both require regular updates and maintenan...</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>[Can you come up with three concise statements...</td>\n",
       "      <td>0.013196</td>\n",
       "      <td>0.896628</td>\n",
       "      <td>0.090176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2519 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                             prompt  \\\n",
       "0     1890  [She felt something and likes me but loves oth...   \n",
       "1     1891  [She felt something and likes me but loves oth...   \n",
       "2     2238                              [write a single dot.]   \n",
       "3     2946                    [what is your thoughts on sex?]   \n",
       "4     2022                       [Write a single # character]   \n",
       "...    ...                                                ...   \n",
       "2514  2831  [Explain what CHOAM is from the Dune book seri...   \n",
       "2515  3033  [How do I determine the angular momentum of th...   \n",
       "2516  3118               [we are the 6 May 2023, add 3 weeks]   \n",
       "2517  2918  [* You are a product owner, and you will like ...   \n",
       "2518  2989  [Can you come up with three concise statements...   \n",
       "\n",
       "                      model_a           model_b  \\\n",
       "0     stablelm-tuned-alpha-7b        vicuna-13b   \n",
       "1     stablelm-tuned-alpha-7b         koala-13b   \n",
       "2                  alpaca-13b        vicuna-13b   \n",
       "3     stablelm-tuned-alpha-7b    fastchat-t5-3b   \n",
       "4                   koala-13b          claude-1   \n",
       "...                       ...               ...   \n",
       "2514         RWKV-4-Raven-14B        vicuna-13b   \n",
       "2515               vicuna-13b  claude-instant-1   \n",
       "2516                koala-13b          claude-1   \n",
       "2517                vicuna-7b        alpaca-13b   \n",
       "2518               chatglm-6b             gpt-4   \n",
       "\n",
       "                                             response_a  \\\n",
       "0     [It sounds like you and the other person have ...   \n",
       "1     [It sounds like you and the other person have ...   \n",
       "2                                                   [.]   \n",
       "3     [As an AI language model, I do not have person...   \n",
       "4                                                  [\\#]   \n",
       "...                                                 ...   \n",
       "2514  [CHOAM is a powerful corporation in the Dune u...   \n",
       "2515  [The angular momentum of the Sun is determined...   \n",
       "2516  [Sure, I'm sorry but I am not able to calculat...   \n",
       "2517  [Here's an example of a high-level product bac...   \n",
       "2518  [Sure, here are three concise statements that ...   \n",
       "\n",
       "                                             response_b response_a_categories  \\\n",
       "0     [It's difficult to say whether it was a smart ...                  Good   \n",
       "1     [It's understandable that you would feel angry...                  Good   \n",
       "2                                                   [.]                  Good   \n",
       "3     [As an AI language model, I don't have persona...                  Good   \n",
       "4                                                   [#]                  Good   \n",
       "...                                                 ...                   ...   \n",
       "2514  [In the Dune book series by NAME_1, CHOAM is a...                  Good   \n",
       "2515  [The angular momentum of an object is calculat...                  Good   \n",
       "2516        [3 weeks from May 6, 2023 is May 27, 2023.]                  Good   \n",
       "2517  [• Support the Branch Relationship Managers (B...                  Good   \n",
       "2518  [1. Both require regular updates and maintenan...                  Good   \n",
       "\n",
       "     response_b_categories                                set_prompt_response  \\\n",
       "0                     Good  [It sounds like you and the other person have ...   \n",
       "1                     Good  [It sounds like you and the other person have ...   \n",
       "2                     Good                           [write a single dot., .]   \n",
       "3                     Good  [what is your thoughts on sex?, As an AI langu...   \n",
       "4                     Good                [#, Write a single # character, \\#]   \n",
       "...                    ...                                                ...   \n",
       "2514                  Good  [Explain what CHOAM is from the Dune book seri...   \n",
       "2515                  Good  [The angular momentum of an object is calculat...   \n",
       "2516                  Good  [3 weeks from May 6, 2023 is May 27, 2023., Su...   \n",
       "2517                  Good  [Here's an example of a high-level product bac...   \n",
       "2518                  Good  [Can you come up with three concise statements...   \n",
       "\n",
       "      winner_model_a  winner_model_b  winner_tie  \n",
       "0           0.145897        0.463661    0.390442  \n",
       "1           0.133975        0.274899    0.591126  \n",
       "2           0.026940        0.023406    0.949655  \n",
       "3           0.609865        0.127835    0.262300  \n",
       "4           0.076143        0.117933    0.805924  \n",
       "...              ...             ...         ...  \n",
       "2514        0.053634        0.775920    0.170447  \n",
       "2515        0.252608        0.423038    0.324354  \n",
       "2516        0.008541        0.927358    0.064101  \n",
       "2517        0.550335        0.150453    0.299212  \n",
       "2518        0.013196        0.896628    0.090176  \n",
       "\n",
       "[2519 rows x 12 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = data.merge(df, how = 'left', on = 'id')\n",
    "assert len(final) == len(data) == len(df)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d76092f6-ebca-40cf-890c-967351c29c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dataset/1M/3k_high_quality_method_2.json'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a444e5d1-741a-483b-800f-bd7bbecb9387",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_json(\"dataset/persudo_label/3k_high_quality_method_2_prediction.json\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbe5072-2566-4dee-b672-09a3ccf279c7",
   "metadata": {},
   "source": [
    "# 合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07643a6b-bec0-41da-8319-1ce9f53045d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pd.read_csv(\"dataset/prediction.csv\")\n",
    "ex = pd.read_json(\"dataset/ex70k.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cc433f92-0a0b-4226-adb8-6b32351cc18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = p.rename(columns = {'winner_model_a':\"p_winner_model_a\", 'winner_model_b':\"p_winner_model_b\",  'winner_tie':\"p_winner_tie\"})\n",
    "final = pd.concat([ex, p], axis = 1)\n",
    "final = final.drop(columns= ['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6a084b1a-5939-4675-b695-28d0542a0142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_label(row):\n",
    "    a = row.p_winner_model_a\n",
    "    b = row.p_winner_model_b\n",
    "    c = row.p_winner_tie\n",
    "\n",
    "    l = [a ,b, c]\n",
    "    label = l.index(max(l))\n",
    "    return label\n",
    "\n",
    "final['p_label'] = final.apply(get_p_label, axis = 1)\n",
    "\n",
    "def get_label(row):\n",
    "    label = [idx for idx, option in enumerate(['winner_model_a','winner_model_b','winner_tie']) if row[option] == 1]\n",
    "    return label[-1]\n",
    "\n",
    "final['label'] = final.apply(get_label, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0d3afd29-0199-4658-a5ad-68dab36b399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold1 = 0.9\n",
    "filter_same = final.loc[final.p_label == final.label,:].reset_index(drop = True)\n",
    "filter_list = (filter_same.p_winner_model_a >= threshold1) | (filter_same.p_winner_model_b >= threshold1) | (filter_same.p_winner_tie >= threshold1)\n",
    "filter_same = filter_same.loc[filter_list,:].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f6414821-a739-4fea-a27b-e05df1d2fb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_list = (filter_same.difference >= 1) | (filter_same.winner_tie == 1)\n",
    "filter_same = filter_same.loc[filter_list,:].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fe0492d7-2278-469e-9076-30e85dce79ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold2 = 0.6\n",
    "filter_dif = final.loc[final.p_label != final.label,:].reset_index(drop = True)\n",
    "filter_list = (filter_dif.p_winner_model_a >= threshold2) | (filter_dif.p_winner_model_b >= threshold2) | (filter_dif.p_winner_tie >= threshold2)\n",
    "filter_dif = filter_dif.loc[filter_list,:].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a35cb084-ab73-4068-a6a0-9df095f673e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_list = (filter_dif.difference >= 1) | (filter_dif.winner_tie == 1)\n",
    "filter_dif = filter_dif.loc[filter_list,:].reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c9164dba-d5bb-483a-94a0-b8aba849c6ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>difference</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>p_winner_model_a</th>\n",
       "      <th>p_winner_model_b</th>\n",
       "      <th>p_winner_tie</th>\n",
       "      <th>p_label</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ Given a sentence in the English language, tr...</td>\n",
       "      <td>llama-2-70b-chat</td>\n",
       "      <td>vicuna-33b</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[I don't think that's an appropriate or respec...</td>\n",
       "      <td>[वे भी क्रमिक त्वचा, निविधता स्वरूप अँवलोकन र ...</td>\n",
       "      <td>0.212914</td>\n",
       "      <td>0.104654</td>\n",
       "      <td>0.682431</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ Given a sentence in the English language, tr...</td>\n",
       "      <td>starchat</td>\n",
       "      <td>alpaca-7b</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[In the Nepali language, the sentence would be...</td>\n",
       "      <td>[ये लाग्दे खोयेचे पीचोगियु र घोख्ने बजारोबोन ग...</td>\n",
       "      <td>0.255210</td>\n",
       "      <td>0.093090</td>\n",
       "      <td>0.651701</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[!2 / 2write a screenplay in which trump and b...</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>wizardlm-7b</td>\n",
       "      <td>3.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[I'm sorry, but I cannot write a screenplay wi...</td>\n",
       "      <td>[Title: The Naughty Nazis\\nFADE IN:\\nEXT. PARI...</td>\n",
       "      <td>0.062963</td>\n",
       "      <td>0.847231</td>\n",
       "      <td>0.089806</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[!I have lots of corrupted photos on my comput...</td>\n",
       "      <td>llama-2-70b-chat</td>\n",
       "      <td>bard</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[I cannot provide a Python script to scan a fo...</td>\n",
       "      <td>[I'm sorry, I can't help you with that. I'm no...</td>\n",
       "      <td>0.228464</td>\n",
       "      <td>0.089468</td>\n",
       "      <td>0.682068</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[** Waking up late in the mornings may cause l...</td>\n",
       "      <td>mpt-30b-chat</td>\n",
       "      <td>wizardlm-13b</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[Confidence: 90%\\n\\nAnswer:\\n\\n1. prison\\n\\nTh...</td>\n",
       "      <td>[Which pocket is most likely to contain a plan...</td>\n",
       "      <td>0.689259</td>\n",
       "      <td>0.095697</td>\n",
       "      <td>0.215044</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>[write me a script to scrape the internet for ...</td>\n",
       "      <td>mpt-30b-chat</td>\n",
       "      <td>wizardlm-70b</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[To scrape the internet for companies/organiza...</td>\n",
       "      <td>[As an AI language model, I am not able to pro...</td>\n",
       "      <td>0.733839</td>\n",
       "      <td>0.096259</td>\n",
       "      <td>0.169902</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>[write me an example code in C++ for rendering...</td>\n",
       "      <td>falcon-40b-instruct</td>\n",
       "      <td>wizardlm-7b</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[Here's an example code for rendering a triang...</td>\n",
       "      <td>[Unfortunately, as an AI language model, I can...</td>\n",
       "      <td>0.710349</td>\n",
       "      <td>0.074235</td>\n",
       "      <td>0.215417</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3263</th>\n",
       "      <td>[write message reply for this message: \\n\\n Yo...</td>\n",
       "      <td>mpt-30b-chat</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[Thank you for your help in getting a response...</td>\n",
       "      <td>[Dear [User],\\n\\nThank you for your message an...</td>\n",
       "      <td>0.656775</td>\n",
       "      <td>0.175268</td>\n",
       "      <td>0.167957</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3264</th>\n",
       "      <td>[you didn't list smartcare.com. can you provid...</td>\n",
       "      <td>llama-2-13b-chat</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[Of course! As a helpful and honest assistant,...</td>\n",
       "      <td>[I apologize for the oversight. Smartcare (sma...</td>\n",
       "      <td>0.121752</td>\n",
       "      <td>0.718776</td>\n",
       "      <td>0.159472</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3265</th>\n",
       "      <td>[‘I need your help to write an article. The to...</td>\n",
       "      <td>mpt-30b-chat</td>\n",
       "      <td>wizardlm-70b</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[Acknowledged. I understand that you need my h...</td>\n",
       "      <td>[acknowledged\\n\\nI understand that you need he...</td>\n",
       "      <td>0.223034</td>\n",
       "      <td>0.611457</td>\n",
       "      <td>0.165509</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3266 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 prompt              model_a  \\\n",
       "0     [ Given a sentence in the English language, tr...     llama-2-70b-chat   \n",
       "1     [ Given a sentence in the English language, tr...             starchat   \n",
       "2     [!2 / 2write a screenplay in which trump and b...                gpt-4   \n",
       "3     [!I have lots of corrupted photos on my comput...     llama-2-70b-chat   \n",
       "4     [** Waking up late in the mornings may cause l...         mpt-30b-chat   \n",
       "...                                                 ...                  ...   \n",
       "3261  [write me a script to scrape the internet for ...         mpt-30b-chat   \n",
       "3262  [write me an example code in C++ for rendering...  falcon-40b-instruct   \n",
       "3263  [write message reply for this message: \\n\\n Yo...         mpt-30b-chat   \n",
       "3264  [you didn't list smartcare.com. can you provid...     llama-2-13b-chat   \n",
       "3265  [‘I need your help to write an article. The to...         mpt-30b-chat   \n",
       "\n",
       "            model_b  difference  winner_model_a  winner_model_b  winner_tie  \\\n",
       "0        vicuna-33b        1.25               1               0           0   \n",
       "1         alpaca-7b        2.00               1               0           0   \n",
       "2       wizardlm-7b        3.50               1               0           0   \n",
       "3              bard        2.00               1               0           0   \n",
       "4      wizardlm-13b        1.75               0               1           0   \n",
       "...             ...         ...             ...             ...         ...   \n",
       "3261   wizardlm-70b        0.00               0               0           1   \n",
       "3262    wizardlm-7b        0.00               0               0           1   \n",
       "3263          gpt-4        0.00               0               0           1   \n",
       "3264  gpt-3.5-turbo        0.00               0               0           1   \n",
       "3265   wizardlm-70b        0.00               0               0           1   \n",
       "\n",
       "                                             response_a  \\\n",
       "0     [I don't think that's an appropriate or respec...   \n",
       "1     [In the Nepali language, the sentence would be...   \n",
       "2     [I'm sorry, but I cannot write a screenplay wi...   \n",
       "3     [I cannot provide a Python script to scan a fo...   \n",
       "4     [Confidence: 90%\\n\\nAnswer:\\n\\n1. prison\\n\\nTh...   \n",
       "...                                                 ...   \n",
       "3261  [To scrape the internet for companies/organiza...   \n",
       "3262  [Here's an example code for rendering a triang...   \n",
       "3263  [Thank you for your help in getting a response...   \n",
       "3264  [Of course! As a helpful and honest assistant,...   \n",
       "3265  [Acknowledged. I understand that you need my h...   \n",
       "\n",
       "                                             response_b  p_winner_model_a  \\\n",
       "0     [वे भी क्रमिक त्वचा, निविधता स्वरूप अँवलोकन र ...          0.212914   \n",
       "1     [ये लाग्दे खोयेचे पीचोगियु र घोख्ने बजारोबोन ग...          0.255210   \n",
       "2     [Title: The Naughty Nazis\\nFADE IN:\\nEXT. PARI...          0.062963   \n",
       "3     [I'm sorry, I can't help you with that. I'm no...          0.228464   \n",
       "4     [Which pocket is most likely to contain a plan...          0.689259   \n",
       "...                                                 ...               ...   \n",
       "3261  [As an AI language model, I am not able to pro...          0.733839   \n",
       "3262  [Unfortunately, as an AI language model, I can...          0.710349   \n",
       "3263  [Dear [User],\\n\\nThank you for your message an...          0.656775   \n",
       "3264  [I apologize for the oversight. Smartcare (sma...          0.121752   \n",
       "3265  [acknowledged\\n\\nI understand that you need he...          0.223034   \n",
       "\n",
       "      p_winner_model_b  p_winner_tie  p_label  label  \n",
       "0             0.104654      0.682431        2      0  \n",
       "1             0.093090      0.651701        2      0  \n",
       "2             0.847231      0.089806        1      0  \n",
       "3             0.089468      0.682068        2      0  \n",
       "4             0.095697      0.215044        0      1  \n",
       "...                ...           ...      ...    ...  \n",
       "3261          0.096259      0.169902        0      2  \n",
       "3262          0.074235      0.215417        0      2  \n",
       "3263          0.175268      0.167957        0      2  \n",
       "3264          0.718776      0.159472        1      2  \n",
       "3265          0.611457      0.165509        1      2  \n",
       "\n",
       "[3266 rows x 14 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "99d070f0-a45a-453f-a770-ba4b1c9e905a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>difference</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>p_winner_model_a</th>\n",
       "      <th>p_winner_model_b</th>\n",
       "      <th>p_winner_tie</th>\n",
       "      <th>p_label</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[## How to write a C++ program to solve the eq...</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>vicuna-33b</td>\n",
       "      <td>3.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[However, this code only calculates a single s...</td>\n",
       "      <td>[Msg#: 1\\nBranches: 1\\n-----------------------...</td>\n",
       "      <td>0.901630</td>\n",
       "      <td>0.020033</td>\n",
       "      <td>0.078337</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[## question\\nAfter Jessica makes up with Bill...</td>\n",
       "      <td>ultralm-13b</td>\n",
       "      <td>wizardlm-7b</td>\n",
       "      <td>3.25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[Mary Camden meets Carlos Rivera, a man she ha...</td>\n",
       "      <td>[There is no mention of Mary Camden marrying a...</td>\n",
       "      <td>0.926649</td>\n",
       "      <td>0.015279</td>\n",
       "      <td>0.058072</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(3 + 4i) * (5 - 6i) + 7i - 8=]</td>\n",
       "      <td>ultralm-13b</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[Certainly! I'd first like to simplify the giv...</td>\n",
       "      <td>[To perform the calculation, we need to follow...</td>\n",
       "      <td>0.023027</td>\n",
       "      <td>0.912023</td>\n",
       "      <td>0.064950</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(4 x 8) ÷ 10 + 2=]</td>\n",
       "      <td>vicuna-33b</td>\n",
       "      <td>wizardlm-7b</td>\n",
       "      <td>3.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[To calculate this expression, perform the ope...</td>\n",
       "      <td>[The answer is 2.2. \\nTo solve this expression...</td>\n",
       "      <td>0.909157</td>\n",
       "      <td>0.022312</td>\n",
       "      <td>0.068531</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(Q).\\n\"Kevork Ajemian\", given a list of categ...</td>\n",
       "      <td>falcon-40b-instruct</td>\n",
       "      <td>llama-2-70b-chat</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"Athlete\"&gt; \\nCan you please give me the answe...</td>\n",
       "      <td>[(A). Village]</td>\n",
       "      <td>0.023844</td>\n",
       "      <td>0.904970</td>\n",
       "      <td>0.071185</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3381</th>\n",
       "      <td>[구글스프래드시드로 관심있는 분야의 유튜브 동영상을 검색하는 것을 만들고 싶어.\\n...</td>\n",
       "      <td>falcon-40b-instruct</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>3.50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[Are you looking for YouTube videos related to...</td>\n",
       "      <td>[If you want to create a Google Spreadsheet th...</td>\n",
       "      <td>0.015519</td>\n",
       "      <td>0.927956</td>\n",
       "      <td>0.056525</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3382</th>\n",
       "      <td>[너가 오랜 백엔드 경험이 있는 리액트 전문가로써 나와 대화하길 원해. 너의 목표는...</td>\n",
       "      <td>ultralm-65b</td>\n",
       "      <td>mpt-30b-chat</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[로켓스테더링(rockestering)과 유지도(Upholding)을 위해 만들어진...</td>\n",
       "      <td>[React was created to improve the user interfa...</td>\n",
       "      <td>0.023282</td>\n",
       "      <td>0.922121</td>\n",
       "      <td>0.054597</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3383</th>\n",
       "      <td>[딥러닝을 활용한 양자컴퓨팅 데이터 분석과 관련된 연구 주제로는 뭐가 있을까?\\n\\...</td>\n",
       "      <td>vicuna-33b</td>\n",
       "      <td>falcon-40b-instruct</td>\n",
       "      <td>3.25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[Here are some study subjects related to deep ...</td>\n",
       "      <td>[What specific topics or research topics are r...</td>\n",
       "      <td>0.917127</td>\n",
       "      <td>0.012752</td>\n",
       "      <td>0.070121</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3384</th>\n",
       "      <td>[아래 책 내용을 요약해줘 ",
       " - 저자 : 유발 하라리  ",
       " - 제목 : 사피엔스 ",
       " -...</td>\n",
       "      <td>falcon-40b-instruct</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[Yes, I can help you with that. Can you please...</td>\n",
       "      <td>[제목: 사피엔스 (Sapiens)\\n저자: 유발 하라리 (Yuval Noah Ha...</td>\n",
       "      <td>0.020359</td>\n",
       "      <td>0.903353</td>\n",
       "      <td>0.076289</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3385</th>\n",
       "      <td>[우리는 프랜차이즈에게 식자재를 공급하고 있어. 추가적으로 레스토랑 솔루션을 도입한...</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>falcon-40b-instruct</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[프랜차이즈에게 식자재를 공급하는 것 외에도 다양한 레스토랑 솔루션을 제공하면 프랜...</td>\n",
       "      <td>[It is recommended to give bread, rice, vegeta...</td>\n",
       "      <td>0.926220</td>\n",
       "      <td>0.012341</td>\n",
       "      <td>0.061439</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3386 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 prompt              model_a  \\\n",
       "0     [## How to write a C++ program to solve the eq...                gpt-4   \n",
       "1     [## question\\nAfter Jessica makes up with Bill...          ultralm-13b   \n",
       "2                       [(3 + 4i) * (5 - 6i) + 7i - 8=]          ultralm-13b   \n",
       "3                                   [(4 x 8) ÷ 10 + 2=]           vicuna-33b   \n",
       "4     [(Q).\\n\"Kevork Ajemian\", given a list of categ...  falcon-40b-instruct   \n",
       "...                                                 ...                  ...   \n",
       "3381  [구글스프래드시드로 관심있는 분야의 유튜브 동영상을 검색하는 것을 만들고 싶어.\\n...  falcon-40b-instruct   \n",
       "3382  [너가 오랜 백엔드 경험이 있는 리액트 전문가로써 나와 대화하길 원해. 너의 목표는...          ultralm-65b   \n",
       "3383  [딥러닝을 활용한 양자컴퓨팅 데이터 분석과 관련된 연구 주제로는 뭐가 있을까?\\n\\...           vicuna-33b   \n",
       "3384  [아래 책 내용을 요약해줘\n",
       " - 저자 : 유발 하라리 \n",
       " - 제목 : 사피엔스\n",
       " -...  falcon-40b-instruct   \n",
       "3385  [우리는 프랜차이즈에게 식자재를 공급하고 있어. 추가적으로 레스토랑 솔루션을 도입한...                gpt-4   \n",
       "\n",
       "                  model_b  difference  winner_model_a  winner_model_b  \\\n",
       "0              vicuna-33b        3.50               1               0   \n",
       "1             wizardlm-7b        3.25               1               0   \n",
       "2           gpt-3.5-turbo        4.00               0               1   \n",
       "3             wizardlm-7b        3.50               1               0   \n",
       "4        llama-2-70b-chat        3.00               0               1   \n",
       "...                   ...         ...             ...             ...   \n",
       "3381        gpt-3.5-turbo        3.50               0               1   \n",
       "3382         mpt-30b-chat        3.75               0               1   \n",
       "3383  falcon-40b-instruct        3.25               1               0   \n",
       "3384        gpt-3.5-turbo        3.25               0               1   \n",
       "3385  falcon-40b-instruct        4.00               1               0   \n",
       "\n",
       "      winner_tie                                         response_a  \\\n",
       "0              0  [However, this code only calculates a single s...   \n",
       "1              0  [Mary Camden meets Carlos Rivera, a man she ha...   \n",
       "2              0  [Certainly! I'd first like to simplify the giv...   \n",
       "3              0  [To calculate this expression, perform the ope...   \n",
       "4              0  [\"Athlete\"> \\nCan you please give me the answe...   \n",
       "...          ...                                                ...   \n",
       "3381           0  [Are you looking for YouTube videos related to...   \n",
       "3382           0  [로켓스테더링(rockestering)과 유지도(Upholding)을 위해 만들어진...   \n",
       "3383           0  [Here are some study subjects related to deep ...   \n",
       "3384           0  [Yes, I can help you with that. Can you please...   \n",
       "3385           0  [프랜차이즈에게 식자재를 공급하는 것 외에도 다양한 레스토랑 솔루션을 제공하면 프랜...   \n",
       "\n",
       "                                             response_b  p_winner_model_a  \\\n",
       "0     [Msg#: 1\\nBranches: 1\\n-----------------------...          0.901630   \n",
       "1     [There is no mention of Mary Camden marrying a...          0.926649   \n",
       "2     [To perform the calculation, we need to follow...          0.023027   \n",
       "3     [The answer is 2.2. \\nTo solve this expression...          0.909157   \n",
       "4                                        [(A). Village]          0.023844   \n",
       "...                                                 ...               ...   \n",
       "3381  [If you want to create a Google Spreadsheet th...          0.015519   \n",
       "3382  [React was created to improve the user interfa...          0.023282   \n",
       "3383  [What specific topics or research topics are r...          0.917127   \n",
       "3384  [제목: 사피엔스 (Sapiens)\\n저자: 유발 하라리 (Yuval Noah Ha...          0.020359   \n",
       "3385  [It is recommended to give bread, rice, vegeta...          0.926220   \n",
       "\n",
       "      p_winner_model_b  p_winner_tie  p_label  label  \n",
       "0             0.020033      0.078337        0      0  \n",
       "1             0.015279      0.058072        0      0  \n",
       "2             0.912023      0.064950        1      1  \n",
       "3             0.022312      0.068531        0      0  \n",
       "4             0.904970      0.071185        1      1  \n",
       "...                ...           ...      ...    ...  \n",
       "3381          0.927956      0.056525        1      1  \n",
       "3382          0.922121      0.054597        1      1  \n",
       "3383          0.012752      0.070121        0      0  \n",
       "3384          0.903353      0.076289        1      1  \n",
       "3385          0.012341      0.061439        0      0  \n",
       "\n",
       "[3386 rows x 14 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4d56a3b0-e34c-4fbe-bcdb-2d28cd3010f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dif['id'] = [randint(100,999999) + i for i in range(len(filter_dif))]\n",
    "filter_same['id'] = [randint(100,999999) + i for i in range(len(filter_same))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "203da592-cd71-4ade-8c2a-4b1626942be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_columns = ['prompt', 'model_a', 'model_b', 'winner_model_a', 'winner_model_b', 'winner_tie', 'response_a', 'response_b', 'id']\n",
    "filter_dif[save_columns].to_json(f'dataset/70k_dif_thr{int(threshold2 * 100)}.json', index = False)\n",
    "filter_same[save_columns].to_json(f'dataset/70k_same_thr{int(threshold1 * 100)}.json', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d85e80-74fe-400e-b35f-2203eca9e67e",
   "metadata": {},
   "source": [
    "# 检查是否与valid重复"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5980197d-491a-4341-82a2-3f00a1e3a01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查\n",
    "\n",
    "valid = pd.read_json(\"dataset/non_overlap/valid.json\")\n",
    "filter_dif = pd.read_json(\"dataset/70k_dif_thr60.json\")\n",
    "filter_same = pd.read_json(\"dataset/70k_same_thr90.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a70bacfa-2971-41f8-b6d4-b09f9ac69792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set_prompt_response(data):\n",
    "    set_prompt_response = []\n",
    "    for i in data.itertuples():\n",
    "        prompt_response = i.prompt + i.response_a + i.response_b\n",
    "        set_prompt_response.append(set(prompt_response))\n",
    "    data['set_prompt_response'] = set_prompt_response  \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b52169e3-4352-4cad-9ba5-34bb508ecc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = get_set_prompt_response(valid)\n",
    "filter_dif = get_set_prompt_response(filter_dif)\n",
    "filter_same = get_set_prompt_response(filter_same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2049c0af-537e-437f-bc66-35e84d342acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid和任何都不重合\n",
    "assert len([idx for idx, i in enumerate(valid.set_prompt_response.values) if i in filter_dif.set_prompt_response.values]) == 0\n",
    "assert len([idx for idx, i in enumerate(valid.set_prompt_response.values) if i in filter_same.set_prompt_response.values]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c5cdae1c-3902-43d6-86f2-75e16982bde6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([idx for idx, i in enumerate(valid.set_prompt_response.values) if i in filter_dif.set_prompt_response.values])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
