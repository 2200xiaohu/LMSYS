{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac7f010-4ef6-4b50-8b20-8a98b8a44895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from typing import Optional, Union\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    EarlyStoppingCallback,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForMultipleChoice,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    RobertaForMultipleChoice,\n",
    "    AutoModelForSequenceClassification,\n",
    "    LlamaModel,\n",
    "    LlamaForSequenceClassification,\n",
    "    BitsAndBytesConfig,\n",
    "    get_polynomial_decay_schedule_with_warmup,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    TrainerCallback,\n",
    ")\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "\n",
    "from peft import (\n",
    "    get_peft_config,\n",
    "    PeftModel,\n",
    "    PeftConfig,\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    ")\n",
    "import os\n",
    "\n",
    "import random\n",
    "from random import randint\n",
    "def seed_everything(seed=None):\n",
    "    '''\n",
    "    固定seed\n",
    "    :param seed: int, 随机种子\n",
    "    '''\n",
    "    max_seed_value = np.iinfo(np.uint32).max\n",
    "    min_seed_value = np.iinfo(np.uint32).min\n",
    "\n",
    "    if (seed is None) or not (min_seed_value <= seed <= max_seed_value):\n",
    "        seed = random.randint(np.iinfo(np.uint32).min, np.iinfo(np.uint32).max)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    return seed\n",
    "\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "from utils import load_split_data, load_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97a2bfe-0604-4dd2-9db9-b0f1175793ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class InstructionDataSet(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_source_length, max_target_length):\n",
    "        super(InstructionDataSet, self).__init__()\n",
    "        #self.data = data.sample(len(data), random_state=0).reset_index(drop=True)\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_source_length = max_source_length\n",
    "        self.max_target_length = max_target_length\n",
    "        # self.A_token = self.tokenizer.encode(text='A', add_special_tokens=False, truncation=True, )\n",
    "        # self.B_token = self.tokenizer.encode(text='B', add_special_tokens=False, truncation=True, )\n",
    "        # self.C_token = self.tokenizer.encode(text='C', add_special_tokens=False, truncation=True, )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        now_data = self.data.loc[index]\n",
    "        idx = now_data['id']\n",
    "        templete_part1 = \"<start_of_turn>user\\nHere are two question-answering dialogues. Compare two model performance on answering question, determine which is better.\\n\\n\"\n",
    "        templete_part1_input_ids = self.tokenizer(text=templete_part1, add_special_tokens=True, padding=False)['input_ids']\n",
    "        \n",
    "        templete_part2 = \"\\n###options\\nA. Model A\\nB. Model B\\nC. Tie\\n<end_of_turn>\\n\"\n",
    "        templete_part2_input_ids = self.tokenizer(text=templete_part2, add_special_tokens=True, padding=False)['input_ids'][1:]\n",
    "        #print(f\"templete_part2 is {templete_part2_input_ids}\")\n",
    "        templete_part3 = \"<start_of_turn>model\\n\"\n",
    "        templete_part3_input_ids = self.tokenizer(text=templete_part3, add_special_tokens=True, padding=False)['input_ids'][1:]\n",
    "        prompt_response = now_data['prompt_response']\n",
    "        #print(f\"id is {now_data['id']}\")\n",
    "        #print(prompt_response)\n",
    "        prompt_response_ids = self.tokenizer(text=prompt_response, add_special_tokens=True, truncation=True,\n",
    "                                          max_length=self.max_source_length, padding=False)['input_ids'][1:]\n",
    "        \n",
    "        input_ids = templete_part1_input_ids + prompt_response_ids + templete_part2_input_ids + templete_part3_input_ids\n",
    "        input_text = self.tokenizer.decode(input_ids, skip_special_tokens=False)\n",
    "        #print(f\"input is {self.tokenizer.decode(input_ids)}\")\n",
    "        return {\n",
    "            \"input_ids\": input_text,\n",
    "            \"id\": idx\n",
    "        }\n",
    "\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = {k: [item[k] for item in batch] for k in ('input_ids','id')}\n",
    "    #print(batch)\n",
    "    batch_input = tokenizer(\n",
    "        batch['input_ids'],\n",
    "        padding='longest',\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_LENGTH + 50\n",
    "    )\n",
    "    return batch_input, batch['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b57bcf4-b6e9-4f21-8671-b43700a1344f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_split_data\n",
    "data_path = \"dataset/1M/35k_in_1M.json\"\n",
    "prompt_type = 3\n",
    "MAX_INPUT = 1900\n",
    "if_train = False\n",
    "split = False\n",
    "if_drop_duplicate = False\n",
    "keep = 'last'\n",
    "df_train , df_valid = load_split_data(data_path, prompt_type, MAX_INPUT, if_train, split, False, if_drop_duplicate, 'last')\n",
    "test = df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f90ca5c-9abd-4619-8226-a0ac25d7cb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.read_json(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedce19c-7129-4a86-9eaf-16118e46cdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tmp.loc[tmp.id == '4af73ffd64'].prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c87a00-8d6b-419c-8358-e1c8dd8a5828",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['length'] = test['prompt_response'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14275570-5606-496e-96b8-01e0d5eaa17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.sort_values(by = ['length'], ascending = False).reset_index(drop = True)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19164b06-76e6-45b8-97a6-80f517231fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def inference(model, test_dataloader):\n",
    "    test_predictions = []\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        batch_input, idx = batch\n",
    "        for k in batch_input.keys():\n",
    "            batch_input[k] = batch_input[k].to(device)\n",
    "        with torch.no_grad():\n",
    "            response = model.generate(**batch_input, max_new_tokens=1, return_dict_in_generate=True, output_scores=True)\n",
    "            #batch_input['input_ids'].shape[-1] + 1\n",
    "            score = response.scores[0]\n",
    "            A_prob, B_prob, C_prob = score[:,A_TOKEN_IDS], score[:,B_TOKEN_IDS], score[:,C_TOKEN_IDS]\n",
    "            logits = torch.cat([A_prob, B_prob, C_prob], dim=-1)\n",
    "            #logits = torch.Tensor([[A_prob,B_prob,C_prob]]) / 1.1\n",
    "            logits = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "            node_result = [[idx[i],logits[i]] for i in range(len(idx))]\n",
    "        test_predictions.extend(node_result)\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cc5551-8b74-4099-9d77-26ed2faadcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "base_model = 'google/gemma-2-9b-it'\n",
    "model_path = \"output/morning-waterfall-460/checkpoint-5200_888\"\n",
    "MAX_LENGTH = 1900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2936ddc-b99a-4c6e-85f0-3a1a5b273f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, truncation_side = 'left')\n",
    "config = AutoConfig.from_pretrained(base_model, trust_remote_code=True)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  \n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "base_model_0 = AutoModelForCausalLM.from_pretrained(base_model,\n",
    "                                                 config=config,\n",
    "                                                 quantization_config=bnb_config,\n",
    "                                                 torch_dtype=torch.float16,\n",
    "                                                 device_map=\"auto\",\n",
    "                                                 trust_remote_code=True)\n",
    "# base_model_0.config.pad_token_id = tokenizer.pad_token_id\n",
    "# base_model_0.resize_token_embeddings(len(tokenizer))\n",
    "new_model = model_path\n",
    "model0 = PeftModel.from_pretrained(base_model_0, new_model).to(device)\n",
    "#model0 = model0.merge_and_unload()\n",
    "model0.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716e3aa7-bc8b-48c2-a87c-751baeb5f884",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_TOKEN_IDS = tokenizer('A',add_special_tokens=True, truncation=True, max_length=1024)['input_ids'][1:]\n",
    "B_TOKEN_IDS = tokenizer('B',add_special_tokens=True, truncation=True, max_length=1024)['input_ids'][1:]\n",
    "C_TOKEN_IDS = tokenizer('C',add_special_tokens=True, truncation=True, max_length=1024)['input_ids'][1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5caf08-32ba-4b84-b13f-a5a0a21e5118",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "tokenized_dataset = InstructionDataSet(test, tokenizer, MAX_LENGTH, 1)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(tokenized_dataset, batch_size = batch_size ,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d36e303-5623-4fd6-8724-139701fa44ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_pred = inference(model = model0, test_dataloader = test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bfe0eb-4bbc-485f-87aa-704f6bdd5ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_pred[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcf6215-5ee6-40ff-a7d0-9fc64a9a8037",
   "metadata": {},
   "outputs": [],
   "source": [
    "if batch_size != 1:\n",
    "    # 提取数据\n",
    "    processed_data = []\n",
    "    for item in sub_pred:\n",
    "        #item = item[0]\n",
    "        id = item[0]#.item()  # 获取id\n",
    "        array_values = item[1].tolist()  # 获取array并转换为列表\n",
    "        processed_data.append([id] + array_values)\n",
    "    \n",
    "\n",
    "else:\n",
    "    # 提取数据\n",
    "    processed_data = []\n",
    "\n",
    "    \n",
    "    for item in sub_pred:\n",
    "        item = item[0]\n",
    "        id = item[0].item()  # 获取id\n",
    "        array_values = item[1].tolist()  # 获取array并转换为列表\n",
    "        processed_data.append([id] + array_values)\n",
    "\n",
    "new_columns = ['id', 'winner_model_a', 'winner_model_b', 'winner_tie']\n",
    "df = pd.DataFrame(processed_data, columns=new_columns)\n",
    "df = df.groupby('id').mean().reset_index()\n",
    "\n",
    "prediction = np.array(df[new_columns[1:]])\n",
    "test = test.drop_duplicates(subset = ['id']).reset_index(drop = True)\n",
    "test = test.sort_values(by = ['id']).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4548df7d-898f-4a11-8a2f-8579489754a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a36dec6-180f-41fa-8925-3603f4f0786b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = data.merge(df, how = 'left', on = 'id')\n",
    "assert len(final) == len(data) == len(df)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76092f6-ebca-40cf-890c-967351c29c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a444e5d1-741a-483b-800f-bd7bbecb9387",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_json(\"dataset/persudo_label/35k_in_1M_prediction.json\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbe5072-2566-4dee-b672-09a3ccf279c7",
   "metadata": {},
   "source": [
    "# 合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07643a6b-bec0-41da-8319-1ce9f53045d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pd.read_csv(\"dataset/prediction.csv\")\n",
    "ex = pd.read_json(\"dataset/ex70k.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc433f92-0a0b-4226-adb8-6b32351cc18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = p.rename(columns = {'winner_model_a':\"p_winner_model_a\", 'winner_model_b':\"p_winner_model_b\",  'winner_tie':\"p_winner_tie\"})\n",
    "final = pd.concat([ex, p], axis = 1)\n",
    "final = final.drop(columns= ['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a084b1a-5939-4675-b695-28d0542a0142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_label(row):\n",
    "    a = row.p_winner_model_a\n",
    "    b = row.p_winner_model_b\n",
    "    c = row.p_winner_tie\n",
    "\n",
    "    l = [a ,b, c]\n",
    "    label = l.index(max(l))\n",
    "    return label\n",
    "\n",
    "final['p_label'] = final.apply(get_p_label, axis = 1)\n",
    "\n",
    "def get_label(row):\n",
    "    label = [idx for idx, option in enumerate(['winner_model_a','winner_model_b','winner_tie']) if row[option] == 1]\n",
    "    return label[-1]\n",
    "\n",
    "final['label'] = final.apply(get_label, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3afd29-0199-4658-a5ad-68dab36b399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold1 = 0.9\n",
    "filter_same = final.loc[final.p_label == final.label,:].reset_index(drop = True)\n",
    "filter_list = (filter_same.p_winner_model_a >= threshold1) | (filter_same.p_winner_model_b >= threshold1) | (filter_same.p_winner_tie >= threshold1)\n",
    "filter_same = filter_same.loc[filter_list,:].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6414821-a739-4fea-a27b-e05df1d2fb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_list = (filter_same.difference >= 1) | (filter_same.winner_tie == 1)\n",
    "filter_same = filter_same.loc[filter_list,:].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0492d7-2278-469e-9076-30e85dce79ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold2 = 0.6\n",
    "filter_dif = final.loc[final.p_label != final.label,:].reset_index(drop = True)\n",
    "filter_list = (filter_dif.p_winner_model_a >= threshold2) | (filter_dif.p_winner_model_b >= threshold2) | (filter_dif.p_winner_tie >= threshold2)\n",
    "filter_dif = filter_dif.loc[filter_list,:].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35cb084-ab73-4068-a6a0-9df095f673e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_list = (filter_dif.difference >= 1) | (filter_dif.winner_tie == 1)\n",
    "filter_dif = filter_dif.loc[filter_list,:].reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9164dba-d5bb-483a-94a0-b8aba849c6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d070f0-a45a-453f-a770-ba4b1c9e905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d56a3b0-e34c-4fbe-bcdb-2d28cd3010f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dif['id'] = [randint(100,999999) + i for i in range(len(filter_dif))]\n",
    "filter_same['id'] = [randint(100,999999) + i for i in range(len(filter_same))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203da592-cd71-4ade-8c2a-4b1626942be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_columns = ['prompt', 'model_a', 'model_b', 'winner_model_a', 'winner_model_b', 'winner_tie', 'response_a', 'response_b', 'id']\n",
    "filter_dif[save_columns].to_json(f'dataset/70k_dif_thr{int(threshold2 * 100)}.json', index = False)\n",
    "filter_same[save_columns].to_json(f'dataset/70k_same_thr{int(threshold1 * 100)}.json', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d85e80-74fe-400e-b35f-2203eca9e67e",
   "metadata": {},
   "source": [
    "# 检查是否与valid重复"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5980197d-491a-4341-82a2-3f00a1e3a01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查\n",
    "\n",
    "valid = pd.read_json(\"dataset/non_overlap/valid.json\")\n",
    "filter_dif = pd.read_json(\"dataset/70k_dif_thr60.json\")\n",
    "filter_same = pd.read_json(\"dataset/70k_same_thr90.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70bacfa-2971-41f8-b6d4-b09f9ac69792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set_prompt_response(data):\n",
    "    set_prompt_response = []\n",
    "    for i in data.itertuples():\n",
    "        prompt_response = i.prompt + i.response_a + i.response_b\n",
    "        set_prompt_response.append(set(prompt_response))\n",
    "    data['set_prompt_response'] = set_prompt_response  \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52169e3-4352-4cad-9ba5-34bb508ecc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = get_set_prompt_response(valid)\n",
    "filter_dif = get_set_prompt_response(filter_dif)\n",
    "filter_same = get_set_prompt_response(filter_same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2049c0af-537e-437f-bc66-35e84d342acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid和任何都不重合\n",
    "assert len([idx for idx, i in enumerate(valid.set_prompt_response.values) if i in filter_dif.set_prompt_response.values]) == 0\n",
    "assert len([idx for idx, i in enumerate(valid.set_prompt_response.values) if i in filter_same.set_prompt_response.values]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cdae1c-3902-43d6-86f2-75e16982bde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([idx for idx, i in enumerate(valid.set_prompt_response.values) if i in filter_dif.set_prompt_response.values])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
