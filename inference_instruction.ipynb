{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06762a62-6035-4580-a70a-8578f5a58fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from typing import Optional, Union\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    EarlyStoppingCallback,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForMultipleChoice,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    RobertaForMultipleChoice,\n",
    "    AutoModelForSequenceClassification,\n",
    "    LlamaModel,\n",
    "    LlamaForSequenceClassification,\n",
    "    BitsAndBytesConfig,\n",
    "    get_polynomial_decay_schedule_with_warmup,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    TrainerCallback,\n",
    ")\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "\n",
    "from peft import (\n",
    "    get_peft_config,\n",
    "    PeftModel,\n",
    "    PeftConfig,\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    ")\n",
    "import os\n",
    "\n",
    "import random\n",
    "def seed_everything(seed=None):\n",
    "    '''\n",
    "    固定seed\n",
    "    :param seed: int, 随机种子\n",
    "    '''\n",
    "    max_seed_value = np.iinfo(np.uint32).max\n",
    "    min_seed_value = np.iinfo(np.uint32).min\n",
    "\n",
    "    if (seed is None) or not (min_seed_value <= seed <= max_seed_value):\n",
    "        seed = random.randint(np.iinfo(np.uint32).min, np.iinfo(np.uint32).max)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    return seed\n",
    "\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a017584f-6664-4302-83e4-30932a04da33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class InstructionDataSet(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_source_length, max_target_length):\n",
    "        super(InstructionDataSet, self).__init__()\n",
    "        #self.data = data.sample(len(data), random_state=0).reset_index(drop=True)\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_source_length = max_source_length\n",
    "        self.max_target_length = max_target_length\n",
    "        # self.A_token = self.tokenizer.encode(text='A', add_special_tokens=False, truncation=True, )\n",
    "        # self.B_token = self.tokenizer.encode(text='B', add_special_tokens=False, truncation=True, )\n",
    "        # self.C_token = self.tokenizer.encode(text='C', add_special_tokens=False, truncation=True, )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        now_data = self.data.loc[index]\n",
    "        idx = now_data['id']\n",
    "        r_a = now_data['instruction_a']\n",
    "        r_b = now_data['instruction_b']\n",
    "        \n",
    "        templete_part1 = \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nHere are two question-answering dialogues. Compare two model performance on answering question, determine which is better.\\n\\n\"\n",
    "        templete_part1_input_ids = self.tokenizer(text=templete_part1, add_special_tokens=True, padding=False)['input_ids']\n",
    "        \n",
    "        model_a_input_ids = self.tokenizer(text=r_a, add_special_tokens=True, truncation=True,\n",
    "                                          max_length=self.max_source_length // 2, padding=False)['input_ids']\n",
    "        model_b_input_ids = self.tokenizer(text=r_b, add_special_tokens=True, truncation=True,\n",
    "                                          max_length=self.max_source_length // 2, padding=False)['input_ids']\n",
    "        templete_part2 = \"###options\\nA. Model A\\nB. Model B\\nC. Tie\\n<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\"\n",
    "        templete_part2_input_ids = self.tokenizer(text=templete_part2, add_special_tokens=True, padding=False)['input_ids']\n",
    "        \n",
    "        input_ids = templete_part1_input_ids + model_a_input_ids + model_b_input_ids + templete_part2_input_ids\n",
    "        input_text = self.tokenizer.decode(input_ids, skip_special_tokens=False)\n",
    "        print(f\"input :\\n {input_text}\")\n",
    "        return {\n",
    "            \"input_ids\": input_text,\n",
    "            \"id\": idx\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bff1ac-64e9-4bcc-8537-c25a29776cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class InstructionDataSet(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_source_length, max_target_length):\n",
    "        super(InstructionDataSet, self).__init__()\n",
    "        #self.data = data.sample(len(data), random_state=0).reset_index(drop=True)\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_source_length = max_source_length\n",
    "        self.max_target_length = max_target_length\n",
    "        # self.A_token = self.tokenizer.encode(text='A', add_special_tokens=False, truncation=True, )\n",
    "        # self.B_token = self.tokenizer.encode(text='B', add_special_tokens=False, truncation=True, )\n",
    "        # self.C_token = self.tokenizer.encode(text='C', add_special_tokens=False, truncation=True, )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        now_data = self.data.loc[index]\n",
    "        idx = now_data['id']\n",
    "        r_a = now_data['instruction_a']\n",
    "        r_b = now_data['instruction_b']\n",
    "        \n",
    "        templete_part1 = \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nHere are two question-answering dialogues. Compare two model performance on answering question, determine which is better.\\n\\n\"\n",
    "        templete_part1_input_ids = self.tokenizer(text=templete_part1, add_special_tokens=True, padding=False)['input_ids']\n",
    "        \n",
    "        prompt_response = now_data['prompt_response']\n",
    "        prompt_response_ids = self.tokenizer(text=prompt_response, add_special_tokens=True, truncation=True,\n",
    "                                          max_length=self.max_source_length, padding=False)['input_ids']\n",
    "        \n",
    "        templete_part2 = \"###options\\nA. Model A\\nB. Model B\\nC. Tie\\n<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\"\n",
    "        templete_part2_input_ids = self.tokenizer(text=templete_part2, add_special_tokens=True, padding=False)['input_ids']\n",
    "        \n",
    "        input_ids = templete_part1_input_ids + prompt_response_ids + templete_part2_input_ids\n",
    "        input_text = self.tokenizer.decode(input_ids, skip_special_tokens=False)\n",
    "        print(f\"input :\\n {input_text}\")\n",
    "        return {\n",
    "            \"input_ids\": input_text,\n",
    "            \"id\": idx\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506790b9-c1ff-44f8-84a2-d2a186593f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "# @dataclass\n",
    "# class DataCollatorForInstruction:\n",
    "#     tokenizer: PreTrainedTokenizerBase\n",
    "#     model: Optional[Any] = None\n",
    "#     padding: Union[bool, str, PaddingStrategy] = True\n",
    "#     max_length: Optional[int] = None\n",
    "#     pad_to_multiple_of: Optional[int] = None\n",
    "#     label_pad_token_id: int = -100\n",
    "#     return_tensors: str = \"pt\"\n",
    "\n",
    "#     def __call__(self, features, return_tensors=None):\n",
    "#         if return_tensors is None:\n",
    "#             return_tensors = self.return_tensors\n",
    "#         labels = [feature[\"labels\"] for feature in features] if \"labels\" in features[0].keys() else None\n",
    "#         print(f\"features is {features}\")\n",
    "#         # We have to pad the labels before calling `tokenizer.pad` as this method won't pad them and needs them of the\n",
    "#         # same length to return tensors.\n",
    "\n",
    "# #         features = self.tokenizer(\n",
    "# #             features,\n",
    "# #             padding='longest',\n",
    "# #             max_length=MAX_LENGTH,\n",
    "# #             pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "# #             return_tensors=return_tensors,\n",
    "# #             truncation=True\n",
    "# #         )\n",
    "\n",
    "#         # prepare decoder_input_ids\n",
    "#         if (\n",
    "#                 labels is not None\n",
    "#                 and self.model is not None\n",
    "#                 and hasattr(self.model, \"prepare_decoder_input_ids_from_labels\")\n",
    "#         ):\n",
    "#             decoder_input_ids = self.model.prepare_decoder_input_ids_from_labels(labels=features[\"labels\"])\n",
    "#             features[\"decoder_input_ids\"] = decoder_input_ids\n",
    "#         # breakpoint() # [(len(features[i]['input_ids']),len(features[i]['labels'])) for i in range(4)]\n",
    "#         return features\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = {k: [item[k] for item in batch] for k in ('input_ids','id')}\n",
    "    batch_input = tokenizer(\n",
    "        batch['input_ids'],\n",
    "        padding='longest',\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_LENGTH + 50\n",
    "    )\n",
    "    return batch_input, batch['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0208959-dfd7-4218-b958-a367f9521742",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('dataset/random_all_in_one_valid.csv')#.sample(5).reset_index(drop = True)\n",
    "#test = test.groupby('id').head(1).reset_index(drop = True)\n",
    "#test = test.loc[:500,:].reset_index(drop=True)\n",
    "#sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a1d9e5-14ed-4ba4-b701-dc878f211a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def inference(model, test_dataloader):\n",
    "    test_predictions = []\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        batch_input, idx = batch\n",
    "        for k in batch_input.keys():\n",
    "            batch_input[k] = batch_input[k].to(device)\n",
    "        with torch.no_grad():\n",
    "            #batch_input, idx = batch['input_ids'], batch['id']\n",
    "            #batch_input = {\"input_ids\": batch_input}\n",
    "            #logits = outputs.logits.cpu().detach().numpy()\n",
    "            response = model.generate(**batch_input, max_new_tokens=1, return_dict_in_generate=True, output_scores=True)\n",
    "            #batch_input['input_ids'].shape[-1] + 1\n",
    "            #print(f\"score {response.scores}\")\n",
    "            #print(f\"score is {response.scores[0]}\")\n",
    "            #print(f\"response is {response}\")\n",
    "            #redict = np.exp(logits) / np.sum(np.exp(logits), axis=-1, keepdims=True)\n",
    "            score = response.scores[0]\n",
    "            A_prob, B_prob, C_prob = score[:,A_TOKEN_IDS], score[:,B_TOKEN_IDS], score[:,C_TOKEN_IDS]\n",
    "            logits = torch.Tensor([[A_prob,B_prob,C_prob]])\n",
    "            logits = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "            print(f\"logits is {logits}\")\n",
    "            node_result = [[idx[i],logits[i]] for i in range(batch_size)]\n",
    "\n",
    "        test_predictions.append(node_result)\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5600585-ab23-4b1c-a723-7c6ab546e9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cc96ee-74c1-4f9a-a50d-9a3598330116",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = 'meta-llama/llama-3-transformers-8b-hf-v1'\n",
    "model_path = \"output/feasible-terrain-134/checkpoint-6800\"\n",
    "MAX_LENGTH = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57af0d60-712e-4efa-b0cd-3731f3198ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "config = AutoConfig.from_pretrained(base_model, trust_remote_code=True)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  \n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "base_model_0 = AutoModelForCausalLM.from_pretrained(base_model,\n",
    "                                                 config=config,\n",
    "                                                 quantization_config=bnb_config,\n",
    "                                                 torch_dtype=torch.float16,\n",
    "                                                 device_map=\"auto\",\n",
    "                                                 trust_remote_code=True)\n",
    "base_model_0.config.pad_token_id = tokenizer.pad_token_id\n",
    "base_model_0.resize_token_embeddings(len(tokenizer))\n",
    "new_model = model_path\n",
    "model0 = PeftModel.from_pretrained(base_model_0, new_model).to(device)\n",
    "#model0 = model0.merge_and_unload()\n",
    "model0.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5a3f5e-455d-4318-ad64-02280a99aa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_TOKEN_IDS = tokenizer('A',add_special_tokens=True, truncation=True, max_length=1024)['input_ids']\n",
    "B_TOKEN_IDS = tokenizer('B',add_special_tokens=True, truncation=True, max_length=1024)['input_ids']\n",
    "C_TOKEN_IDS = tokenizer('C',add_special_tokens=True, truncation=True, max_length=1024)['input_ids']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ba1af6-413e-4fdb-9357-9d7094743e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_TOKEN_IDS,B_TOKEN_IDS,C_TOKEN_IDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6316fe95-7d22-4033-a697-1852f00750fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenized_dataset = InstructionDataSet(test, tokenizer, MAX_LENGTH, 1)\n",
    "\n",
    "token_ids = tokenized_dataset[0]['input_ids']\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "\n",
    "for token, token_id in zip(tokens, token_ids):\n",
    "    print(f\"Token: {token}, ID: {token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6cf97a-1ad2-4cdb-92c1-17690144d40d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "tokenized_dataset = InstructionDataSet(test, tokenizer, MAX_LENGTH, 1)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(tokenized_dataset, batch_size = batch_size ,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e194ff9-be6d-4472-a28b-b30b91a334a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sub_pred = inference(model = model0, test_dataloader = test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87381f51-1930-4d19-8c61-c0196e9196a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取数据\n",
    "processed_data = []\n",
    "for item in sub_pred:\n",
    "    item = item[0]\n",
    "    id = item[0].item()  # 获取id\n",
    "    array_values = item[1].tolist()  # 获取array并转换为列表\n",
    "    processed_data.append([id] + array_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad7e3bf-a0b1-4249-ba3c-68b6e632dc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = ['id', 'winner_model_a', 'winner_model_b', 'winner_tie']\n",
    "df = pd.DataFrame(processed_data, columns=new_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b08e7ca-463a-4554-b57d-0b0385d21238",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby('id').mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef037aac-5011-48eb-ad82-e1f35d2ee2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09478421-4446-4043-bf81-e7d2567e798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "str2num = {'A':0, \"B\":1, \"C\":2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393efd7c-10ff-4af1-9a9d-7fdb561faced",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['label_number'] = test.label.map(str2num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf1c332-1191-46e5-8169-c1e1a1e4829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daa8e70-9a67-4a7f-89a1-81dac569fe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.array(df[new_columns[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02d8404-ca77-4f95-8880-5220225f23a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.drop_duplicates(subset = ['id']).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec914b91-dce9-4fa8-91c2-b850ace35f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(test.label_number, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18da3c9-f88c-462e-8f63-443825225783",
   "metadata": {},
   "outputs": [],
   "source": [
    "3.176938522630114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fc61cc-7b0c-417f-b743-ccae5dbd8d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['label'][:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accb6200-653d-417e-9d16-5de503023354",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
