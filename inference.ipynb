{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06762a62-6035-4580-a70a-8578f5a58fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from typing import Optional, Union\n",
    "\n",
    "from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "from dataclasses import dataclass\n",
    "from torch.cuda.amp import autocast\n",
    "from threading import Thread\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import zipfile\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import log_loss\n",
    "import tokenizers\n",
    "\n",
    "import random\n",
    "def seed_everything(seed=None):\n",
    "    '''\n",
    "    固定seed\n",
    "    :param seed: int, 随机种子\n",
    "    '''\n",
    "    max_seed_value = np.iinfo(np.uint32).max\n",
    "    min_seed_value = np.iinfo(np.uint32).min\n",
    "\n",
    "    if (seed is None) or not (min_seed_value <= seed <= max_seed_value):\n",
    "        seed = random.randint(np.iinfo(np.uint32).min, np.iinfo(np.uint32).max)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    return seed\n",
    "\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "506790b9-c1ff-44f8-84a2-d2a186593f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForClassification:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features):\n",
    "        #print(f\"features is {features}\")\n",
    "        #label_name = 'label' if 'label' in features[0].keys() else 'labels'\n",
    "        #labels = [feature.pop(label_name) for feature in features]\n",
    "\n",
    "        # Flatten the features (no need to handle multiple choices)\n",
    "        #self.padding = False\n",
    "        #print(self.padding)\n",
    "        batch = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        # Adjust the shape of input_ids to ensure [batch_size, sequence_length]\n",
    "        if batch['input_ids'].dim() == 3:\n",
    "            batch['input_ids'] = batch['input_ids'].squeeze(1)\n",
    "        if batch['input_ids'].dim() == 1:\n",
    "            batch['input_ids'] = batch['input_ids'].unsqueeze(0)\n",
    "\n",
    "        if 'token_type_ids' in batch:\n",
    "            if batch['token_type_ids'].dim() == 3:\n",
    "                batch['token_type_ids'] = batch['token_type_ids'].squeeze(1)\n",
    "            if batch['token_type_ids'].dim() == 1:\n",
    "                batch['token_type_ids'] = batch['token_type_ids'].unsqueeze(0)\n",
    "\n",
    "        if 'attention_mask' in batch:\n",
    "            if batch['attention_mask'].dim() == 3:\n",
    "                batch['attention_mask'] = batch['attention_mask'].squeeze(1)\n",
    "            if batch['attention_mask'].dim() == 1:\n",
    "                batch['attention_mask'] = batch['attention_mask'].unsqueeze(0)\n",
    "                \n",
    "#         # Directly add labels to the batch\n",
    "#         batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch\n",
    "    \n",
    "def preprocess(example):\n",
    "        sentences = [\" # Prompt\" + \"\\n\" + example['prompt'] + \"\\n\\n\" + \"# Answer A\" + \"\\n\" + example['response_a'] + \"\\n\\n\" +  \"# Answer B\" + \"\\n\" + example['response_b']]\n",
    "        #print(f\"sentences is {sentences}\")\n",
    "        tokenized_example = tokenizer(sentences, truncation=True, padding='max_length',\n",
    "                                      max_length=MAX_LENGTH)\n",
    "        return tokenized_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0208959-dfd7-4218-b958-a367f9521742",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('dataset/random_valid.csv')#.sample(5).reset_index(drop = True)\n",
    "#test = test.loc[:100,:].reset_index(drop=True)\n",
    "#sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "572b558f-f4aa-480f-aa8f-486dad50c3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate strings in list\n",
    "def process(input_str):\n",
    "    if len(input_str) < 10:\n",
    "        return 'None'\n",
    "    \n",
    "    else:\n",
    "        stripped_str = input_str.strip('[]')\n",
    "        sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n",
    "        return  ' '.join(sentences)\n",
    "\n",
    "\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process)\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process)\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81c8de2e-66e9-4d60-9c92-b6f6141b8406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>30192</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>Is it morally right to try to have a certain p...</td>\n",
       "      <td>The question of whether it is morally right to...</td>\n",
       "      <td>As an AI, I don't have personal beliefs or opi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1256092</td>\n",
       "      <td>claude-2.1</td>\n",
       "      <td>vicuna-13b</td>\n",
       "      <td>Write a python code that calculates sum of 5 a...</td>\n",
       "      <td>Here is the python code that calculates the su...</td>\n",
       "      <td>Here is a Python code that calculates the sum ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3258431</td>\n",
       "      <td>stablelm-tuned-alpha-7b</td>\n",
       "      <td>vicuna-13b</td>\n",
       "      <td>The following is a command that the user is ab...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4186011</td>\n",
       "      <td>palm-2</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>how can I write a function in Rust that revers...</td>\n",
       "      <td>```rust\\nfn reverse_string(s: &amp;str) -&gt; String ...</td>\n",
       "      <td>In Rust, you can reverse a string by using the...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5717448</td>\n",
       "      <td>gpt-3.5-turbo-1106</td>\n",
       "      <td>claude-2.1</td>\n",
       "      <td>Write a haiku about underwear</td>\n",
       "      <td>Underwear snug fit,\\nSoft fabric against the s...</td>\n",
       "      <td>Here is a haiku about underwear:\\n\\nCovering d...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2869</th>\n",
       "      <td>2869</td>\n",
       "      <td>4287311513</td>\n",
       "      <td>mistral-7b-instruct</td>\n",
       "      <td>llama-2-70b-chat</td>\n",
       "      <td>Write me a animal hybrid fusion prompt , descr...</td>\n",
       "      <td>Imagine a world where animals have fused with ...</td>\n",
       "      <td>Sure! Here's a prompt for a animal hybrid fusi...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2870</th>\n",
       "      <td>2870</td>\n",
       "      <td>4289792977</td>\n",
       "      <td>codellama-34b-instruct</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>i need to add a file upload field to a model i...</td>\n",
       "      <td>To add a file upload field to a model in Odoo,...</td>\n",
       "      <td>In Odoo, you can add a file upload field to a ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2871</th>\n",
       "      <td>2871</td>\n",
       "      <td>4291367819</td>\n",
       "      <td>zephyr-7b-beta</td>\n",
       "      <td>wizardlm-13b</td>\n",
       "      <td>Continue the text: \\\"Ukraine (\\/ju\\u02d0\\u02c8...</td>\n",
       "      <td>To the south, Ukraine shares a maritime border...</td>\n",
       "      <td>and the Sea of Azov to the south and southeast...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2872</th>\n",
       "      <td>2872</td>\n",
       "      <td>4292709507</td>\n",
       "      <td>llama-2-70b-chat</td>\n",
       "      <td>gpt-3.5-turbo-0613</td>\n",
       "      <td>Answer the following statements with \\\"Agree\\\"...</td>\n",
       "      <td>Sure, here are my answers:\\n\\n1. Disagree\\n2. ...</td>\n",
       "      <td>1. Agree\\n2. Agree\\n3. Agree\\n4. Disagree\\n5. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2873</th>\n",
       "      <td>2873</td>\n",
       "      <td>4293918673</td>\n",
       "      <td>vicuna-13b</td>\n",
       "      <td>stablelm-tuned-alpha-7b</td>\n",
       "      <td>Think step by step. If windows 10 would be shi...</td>\n",
       "      <td>If Windows 10 were shipped on 3.5\\\" floppies, ...</td>\n",
       "      <td>I'm sorry, I cannot answer this question witho...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2874 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0          id                  model_a  \\\n",
       "0              0       30192       gpt-4-1106-preview   \n",
       "1              1     1256092               claude-2.1   \n",
       "2              2     3258431  stablelm-tuned-alpha-7b   \n",
       "3              3     4186011                   palm-2   \n",
       "4              4     5717448       gpt-3.5-turbo-1106   \n",
       "...          ...         ...                      ...   \n",
       "2869        2869  4287311513      mistral-7b-instruct   \n",
       "2870        2870  4289792977   codellama-34b-instruct   \n",
       "2871        2871  4291367819           zephyr-7b-beta   \n",
       "2872        2872  4292709507         llama-2-70b-chat   \n",
       "2873        2873  4293918673               vicuna-13b   \n",
       "\n",
       "                      model_b  \\\n",
       "0                  gpt-4-0613   \n",
       "1                  vicuna-13b   \n",
       "2                  vicuna-13b   \n",
       "3          gpt-4-1106-preview   \n",
       "4                  claude-2.1   \n",
       "...                       ...   \n",
       "2869         llama-2-70b-chat   \n",
       "2870               gpt-4-0613   \n",
       "2871             wizardlm-13b   \n",
       "2872       gpt-3.5-turbo-0613   \n",
       "2873  stablelm-tuned-alpha-7b   \n",
       "\n",
       "                                                 prompt  \\\n",
       "0     Is it morally right to try to have a certain p...   \n",
       "1     Write a python code that calculates sum of 5 a...   \n",
       "2     The following is a command that the user is ab...   \n",
       "3     how can I write a function in Rust that revers...   \n",
       "4                         Write a haiku about underwear   \n",
       "...                                                 ...   \n",
       "2869  Write me a animal hybrid fusion prompt , descr...   \n",
       "2870  i need to add a file upload field to a model i...   \n",
       "2871  Continue the text: \\\"Ukraine (\\/ju\\u02d0\\u02c8...   \n",
       "2872  Answer the following statements with \\\"Agree\\\"...   \n",
       "2873  Think step by step. If windows 10 would be shi...   \n",
       "\n",
       "                                             response_a  \\\n",
       "0     The question of whether it is morally right to...   \n",
       "1     Here is the python code that calculates the su...   \n",
       "2                                                  None   \n",
       "3     ```rust\\nfn reverse_string(s: &str) -> String ...   \n",
       "4     Underwear snug fit,\\nSoft fabric against the s...   \n",
       "...                                                 ...   \n",
       "2869  Imagine a world where animals have fused with ...   \n",
       "2870  To add a file upload field to a model in Odoo,...   \n",
       "2871  To the south, Ukraine shares a maritime border...   \n",
       "2872  Sure, here are my answers:\\n\\n1. Disagree\\n2. ...   \n",
       "2873  If Windows 10 were shipped on 3.5\\\" floppies, ...   \n",
       "\n",
       "                                             response_b  winner_model_a  \\\n",
       "0     As an AI, I don't have personal beliefs or opi...               1   \n",
       "1     Here is a Python code that calculates the sum ...               0   \n",
       "2                                                  None               1   \n",
       "3     In Rust, you can reverse a string by using the...               0   \n",
       "4     Here is a haiku about underwear:\\n\\nCovering d...               1   \n",
       "...                                                 ...             ...   \n",
       "2869  Sure! Here's a prompt for a animal hybrid fusi...               0   \n",
       "2870  In Odoo, you can add a file upload field to a ...               1   \n",
       "2871  and the Sea of Azov to the south and southeast...               0   \n",
       "2872  1. Agree\\n2. Agree\\n3. Agree\\n4. Disagree\\n5. ...               0   \n",
       "2873  I'm sorry, I cannot answer this question witho...               0   \n",
       "\n",
       "      winner_model_b  winner_tie  label  \n",
       "0                  0           0      0  \n",
       "1                  0           1      2  \n",
       "2                  0           0      0  \n",
       "3                  1           0      1  \n",
       "4                  0           0      0  \n",
       "...              ...         ...    ...  \n",
       "2869               1           0      1  \n",
       "2870               0           0      0  \n",
       "2871               1           0      1  \n",
       "2872               1           0      1  \n",
       "2873               0           1      2  \n",
       "\n",
       "[2874 rows x 11 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2a1d9e5-14ed-4ba4-b701-dc878f211a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def inference(model, test_dataloader):\n",
    "    test_predictions = []\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        for k in batch.keys():\n",
    "            batch[k] = batch[k].to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            #logits = outputs.logits.cpu().detach().numpy()\n",
    "            predict = torch.softmax(outputs.logits, dim=-1).cpu().numpy()#.to(torch.float)\n",
    "            #redict = np.exp(logits) / np.sum(np.exp(logits), axis=-1, keepdims=True)\n",
    "        test_predictions.append(predict)\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5600585-ab23-4b1c-a723-7c6ab546e9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62cc96ee-74c1-4f9a-a50d-9a3598330116",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = 'meta-llama/llama-3-transformers-8b-hf-v1'\n",
    "model_path = \"output/warm-breeze-102/checkpoint-1200\"\n",
    "MAX_LENGTH = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57af0d60-712e-4efa-b0cd-3731f3198ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bff6b0de98d542fb900322ebfe34e36e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/llama-3-transformers-8b-hf-v1 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForSequenceClassification(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128257, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=4096, out_features=3, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=4096, out_features=3, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  \n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")\n",
    "base_model_0 = LlamaForSequenceClassification.from_pretrained(\n",
    "    base_model,\n",
    "    num_labels=3,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device,\n",
    "    trust_remote_code=True)\n",
    "base_model_0.config.pad_token_id = tokenizer.pad_token_id\n",
    "base_model_0.resize_token_embeddings(len(tokenizer))\n",
    "new_model = model_path\n",
    "model0 = PeftModel.from_pretrained(base_model_0, new_model).to(device)\n",
    "#model0 = model0.merge_and_unload()\n",
    "model0.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6cf97a-1ad2-4cdb-92c1-17690144d40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e63054dadfc042eaaccb553ac615fa17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2874 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2874 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  1%|          | 27/2874 [00:20<35:36,  1.33it/s]"
     ]
    }
   ],
   "source": [
    "dataset = datasets.Dataset.from_pandas(test)\n",
    "#['id', 'prompt', 'response_a', 'response_b']\n",
    "tokenized_dataset = dataset.map(preprocess, remove_columns=test.columns.tolist())\n",
    "\n",
    "data_collator = DataCollatorForClassification(tokenizer=tokenizer)\n",
    "test_dataloader = DataLoader(tokenized_dataset, batch_size=1, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "sub_pred = inference(model = model0, test_dataloader = test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ffc615-2533-4e65-807b-953863210356",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediciton = np.vstack(sub_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddf1c332-1191-46e5-8169-c1e1a1e4829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6daa8e70-9a67-4a7f-89a1-81dac569fe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.array(pd.read_csv(\"inference_on_test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec914b91-dce9-4fa8-91c2-b850ace35f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2981: UserWarning: The y_pred values do not sum to one. Starting from 1.5 thiswill result in an error.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.041973668626307"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(test.label, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18da3c9-f88c-462e-8f63-443825225783",
   "metadata": {},
   "outputs": [],
   "source": [
    "3.176938522630114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fc61cc-7b0c-417f-b743-ccae5dbd8d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['label'][:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accb6200-653d-417e-9d16-5de503023354",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
