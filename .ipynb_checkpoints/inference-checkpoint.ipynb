{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06762a62-6035-4580-a70a-8578f5a58fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from typing import Optional, Union\n",
    "\n",
    "from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "from dataclasses import dataclass\n",
    "from torch.cuda.amp import autocast\n",
    "from threading import Thread\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import zipfile\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import log_loss\n",
    "import tokenizers\n",
    "\n",
    "import random\n",
    "def seed_everything(seed=None):\n",
    "    '''\n",
    "    固定seed\n",
    "    :param seed: int, 随机种子\n",
    "    '''\n",
    "    max_seed_value = np.iinfo(np.uint32).max\n",
    "    min_seed_value = np.iinfo(np.uint32).min\n",
    "\n",
    "    if (seed is None) or not (min_seed_value <= seed <= max_seed_value):\n",
    "        seed = random.randint(np.iinfo(np.uint32).min, np.iinfo(np.uint32).max)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    return seed\n",
    "\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "506790b9-c1ff-44f8-84a2-d2a186593f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForClassification:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features):\n",
    "        #print(f\"features is {features}\")\n",
    "        #label_name = 'label' if 'label' in features[0].keys() else 'labels'\n",
    "        #labels = [feature.pop(label_name) for feature in features]\n",
    "\n",
    "        # Flatten the features (no need to handle multiple choices)\n",
    "        #self.padding = False\n",
    "        #print(self.padding)\n",
    "        batch = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        # Adjust the shape of input_ids to ensure [batch_size, sequence_length]\n",
    "        if batch['input_ids'].dim() == 3:\n",
    "            batch['input_ids'] = batch['input_ids'].squeeze(1)\n",
    "        if batch['input_ids'].dim() == 1:\n",
    "            batch['input_ids'] = batch['input_ids'].unsqueeze(0)\n",
    "\n",
    "        if 'token_type_ids' in batch:\n",
    "            if batch['token_type_ids'].dim() == 3:\n",
    "                batch['token_type_ids'] = batch['token_type_ids'].squeeze(1)\n",
    "            if batch['token_type_ids'].dim() == 1:\n",
    "                batch['token_type_ids'] = batch['token_type_ids'].unsqueeze(0)\n",
    "\n",
    "        if 'attention_mask' in batch:\n",
    "            if batch['attention_mask'].dim() == 3:\n",
    "                batch['attention_mask'] = batch['attention_mask'].squeeze(1)\n",
    "            if batch['attention_mask'].dim() == 1:\n",
    "                batch['attention_mask'] = batch['attention_mask'].unsqueeze(0)\n",
    "                \n",
    "#         # Directly add labels to the batch\n",
    "#         batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch\n",
    "    \n",
    "def preprocess(example):\n",
    "        sentences = [\" # Prompt\" + \"\\n\" + example['prompt'] + \"\\n\\n\" + \"# Answer A\" + \"\\n\" + example['response_a'] + \"\\n\\n\" +  \"# Answer B\" + \"\\n\" + example['response_b']]\n",
    "        #print(f\"sentences is {sentences}\")\n",
    "        tokenized_example = tokenizer(sentences, truncation=True, padding='max_length',\n",
    "                                      max_length=MAX_LENGTH)\n",
    "        return tokenized_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0208959-dfd7-4218-b958-a367f9521742",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('dataset/random_valid.csv')#.sample(5).reset_index(drop = True)\n",
    "test = test.loc[:200,:].reset_index(drop=True)\n",
    "#sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "572b558f-f4aa-480f-aa8f-486dad50c3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate strings in list\n",
    "def process(input_str):\n",
    "    if len(input_str) < 10:\n",
    "        return 'None'\n",
    "    \n",
    "    else:\n",
    "        stripped_str = input_str.strip('[]')\n",
    "        sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n",
    "        return  ' '.join(sentences)\n",
    "\n",
    "\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process)\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process)\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2a1d9e5-14ed-4ba4-b701-dc878f211a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def inference(model, test_dataloader):\n",
    "    test_predictions = []\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        for k in batch.keys():\n",
    "            batch[k] = batch[k].to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            #logits = outputs.logits.cpu().detach().numpy()\n",
    "            predict = torch.softmax(outputs.logits, dim=-1).cpu().numpy()#.to(torch.float)\n",
    "            #redict = np.exp(logits) / np.sum(np.exp(logits), axis=-1, keepdims=True)\n",
    "        test_predictions.append(predict)\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5600585-ab23-4b1c-a723-7c6ab546e9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62cc96ee-74c1-4f9a-a50d-9a3598330116",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = 'meta-llama/llama-3-transformers-8b-hf-v1'\n",
    "model_path = \"output/iconic-feather-92/checkpoint-2800\"\n",
    "MAX_LENGTH = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57af0d60-712e-4efa-b0cd-3731f3198ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f916531a0c34b65bfd9b2e750a73223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/llama-3-transformers-8b-hf-v1 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForSequenceClassification(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128257, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=4096, out_features=3, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=4096, out_features=3, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  \n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")\n",
    "base_model_0 = LlamaForSequenceClassification.from_pretrained(\n",
    "    base_model,\n",
    "    num_labels=3,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device,\n",
    "    trust_remote_code=True)\n",
    "base_model_0.config.pad_token_id = tokenizer.pad_token_id\n",
    "base_model_0.resize_token_embeddings(len(tokenizer))\n",
    "new_model = model_path\n",
    "model0 = PeftModel.from_pretrained(base_model_0, new_model).to(device)\n",
    "#model0 = model0.merge_and_unload()\n",
    "model0.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd6cf97a-1ad2-4cdb-92c1-17690144d40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6454028083134e75ac132c6b45f13cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/201 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 201/201 [01:28<00:00,  2.27it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.Dataset.from_pandas(test)\n",
    "#['id', 'prompt', 'response_a', 'response_b']\n",
    "tokenized_dataset = dataset.map(preprocess, remove_columns=test.columns.tolist())\n",
    "\n",
    "data_collator = DataCollatorForClassification(tokenizer=tokenizer)\n",
    "test_dataloader = DataLoader(tokenized_dataset, batch_size=1, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "sub_pred = inference(model = model0, test_dataloader = test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93ffc615-2533-4e65-807b-953863210356",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediciton = np.vstack(sub_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddf1c332-1191-46e5-8169-c1e1a1e4829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec914b91-dce9-4fa8-91c2-b850ace35f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2981: UserWarning: The y_pred values do not sum to one. Starting from 1.5 thiswill result in an error.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.443695534939231"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(test.label, prediciton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18da3c9-f88c-462e-8f63-443825225783",
   "metadata": {},
   "outputs": [],
   "source": [
    "3.176938522630114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20fc61cc-7b0c-417f-b743-ccae5dbd8d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     1\n",
       "2     2\n",
       "3     0\n",
       "4     0\n",
       "5     2\n",
       "6     1\n",
       "7     0\n",
       "8     0\n",
       "9     2\n",
       "10    2\n",
       "11    0\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['label'][:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accb6200-653d-417e-9d16-5de503023354",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
