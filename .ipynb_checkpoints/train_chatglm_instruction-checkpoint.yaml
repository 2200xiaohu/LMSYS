train_data: 'dataset/random_instruction_train.csv'
valid_data: 'dataset/random_instruction_valid.csv'
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
learning_rate: 1.e-4
lr_end: 1.e-6
warmup_ratio: 0.1
warmup_steps: 20
num_train_epochs: 2
gradient_accumulation_steps: 16
logging_steps: 5
eval_steps: 200
save_steps: 200
weight_decay: 1.e-5
MAX_INPUT: 1500
MODEL: 'THUDM/chatglm2-6b'
dropout_rate: 0.1
awp_lr: 0
awp_eps: 1.e-4
awp_start_epoch: 0.5
label_smoothing_factor: 0
output_dir: 'output'
use_cache: False
token_type: 'MC'
lora_r: 16
lora_alpha: 16
lora_dropout: 0.05
