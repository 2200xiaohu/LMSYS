{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06762a62-6035-4580-a70a-8578f5a58fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "from typing import Optional, Union\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    EarlyStoppingCallback,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForMultipleChoice,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    RobertaForMultipleChoice,\n",
    "    AutoModelForSequenceClassification,\n",
    "    LlamaModel,\n",
    "    LlamaForSequenceClassification,\n",
    "    BitsAndBytesConfig,\n",
    "    get_polynomial_decay_schedule_with_warmup,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    TrainerCallback,\n",
    ")\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "\n",
    "from peft import (\n",
    "    get_peft_config,\n",
    "    PeftModel,\n",
    "    PeftConfig,\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    ")\n",
    "import os\n",
    "\n",
    "import random\n",
    "def seed_everything(seed=None):\n",
    "    '''\n",
    "    固定seed\n",
    "    :param seed: int, 随机种子\n",
    "    '''\n",
    "    max_seed_value = np.iinfo(np.uint32).max\n",
    "    min_seed_value = np.iinfo(np.uint32).min\n",
    "\n",
    "    if (seed is None) or not (min_seed_value <= seed <= max_seed_value):\n",
    "        seed = random.randint(np.iinfo(np.uint32).min, np.iinfo(np.uint32).max)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    return seed\n",
    "\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0bff1ac-64e9-4bcc-8537-c25a29776cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class InstructionDataSet(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_source_length, max_target_length):\n",
    "        super(InstructionDataSet, self).__init__()\n",
    "        #self.data = data.sample(len(data), random_state=0).reset_index(drop=True)\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_source_length = max_source_length\n",
    "        self.max_target_length = max_target_length\n",
    "        # self.A_token = self.tokenizer.encode(text='A', add_special_tokens=False, truncation=True, )\n",
    "        # self.B_token = self.tokenizer.encode(text='B', add_special_tokens=False, truncation=True, )\n",
    "        # self.C_token = self.tokenizer.encode(text='C', add_special_tokens=False, truncation=True, )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        now_data = self.data.loc[index]\n",
    "        idx = now_data['id']\n",
    "        templete_part1 = \"<start_of_turn>user\\nHere are two question-answering dialogues. Compare two model performance on answering question, determine which is better.\\n\\n\"\n",
    "        templete_part1_input_ids = self.tokenizer(text=templete_part1, add_special_tokens=True, padding=False)['input_ids']\n",
    "        \n",
    "        templete_part2 = \"\\n###options\\nA. Model A\\nB. Model B\\nC. Tie\\n<end_of_turn>\\n\"\n",
    "        templete_part2_input_ids = self.tokenizer(text=templete_part2, add_special_tokens=True, padding=False)['input_ids'][1:]\n",
    "        #print(f\"templete_part2 is {templete_part2_input_ids}\")\n",
    "        templete_part3 = \"<start_of_turn>model\\n\"\n",
    "        templete_part3_input_ids = self.tokenizer(text=templete_part3, add_special_tokens=True, padding=False)['input_ids'][1:]\n",
    "        prompt_response = now_data['prompt_response']\n",
    "        #print(f\"id is {now_data['id']}\")\n",
    "        #print(prompt_response)\n",
    "        prompt_response_ids = self.tokenizer(text=prompt_response, add_special_tokens=True, truncation=True,\n",
    "                                          max_length=self.max_source_length, padding=False)['input_ids'][1:]\n",
    "        \n",
    "        input_ids = templete_part1_input_ids + prompt_response_ids + templete_part2_input_ids + templete_part3_input_ids\n",
    "        input_text = self.tokenizer.decode(input_ids, skip_special_tokens=False)\n",
    "        #print(f\"input is {self.tokenizer.decode(input_ids)}\")\n",
    "        return {\n",
    "            \"input_ids\": input_text,\n",
    "            \"id\": idx\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "506790b9-c1ff-44f8-84a2-d2a186593f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "# @dataclass\n",
    "# class DataCollatorForInstruction:\n",
    "#     tokenizer: PreTrainedTokenizerBase\n",
    "#     model: Optional[Any] = None\n",
    "#     padding: Union[bool, str, PaddingStrategy] = True\n",
    "#     max_length: Optional[int] = None\n",
    "#     pad_to_multiple_of: Optional[int] = None\n",
    "#     label_pad_token_id: int = -100\n",
    "#     return_tensors: str = \"pt\"\n",
    "\n",
    "#     def __call__(self, features, return_tensors=None):\n",
    "#         if return_tensors is None:\n",
    "#             return_tensors = self.return_tensors\n",
    "#         labels = [feature[\"labels\"] for feature in features] if \"labels\" in features[0].keys() else None\n",
    "#         print(f\"features is {features}\")\n",
    "#         # We have to pad the labels before calling `tokenizer.pad` as this method won't pad them and needs them of the\n",
    "#         # same length to return tensors.\n",
    "\n",
    "# #         features = self.tokenizer(\n",
    "# #             features,\n",
    "# #             padding='longest',\n",
    "# #             max_length=MAX_LENGTH,\n",
    "# #             pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "# #             return_tensors=return_tensors,\n",
    "# #             truncation=True\n",
    "# #         )\n",
    "\n",
    "#         # prepare decoder_input_ids\n",
    "#         if (\n",
    "#                 labels is not None\n",
    "#                 and self.model is not None\n",
    "#                 and hasattr(self.model, \"prepare_decoder_input_ids_from_labels\")\n",
    "#         ):\n",
    "#             decoder_input_ids = self.model.prepare_decoder_input_ids_from_labels(labels=features[\"labels\"])\n",
    "#             features[\"decoder_input_ids\"] = decoder_input_ids\n",
    "#         # breakpoint() # [(len(features[i]['input_ids']),len(features[i]['labels'])) for i in range(4)]\n",
    "#         return features\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = {k: [item[k] for item in batch] for k in ('input_ids','id')}\n",
    "    #print(batch)\n",
    "    batch_input = tokenizer(\n",
    "        batch['input_ids'],\n",
    "        padding='longest',\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_LENGTH + 50\n",
    "    )\n",
    "    return batch_input, batch['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0208959-dfd7-4218-b958-a367f9521742",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71514/71514 [00:22<00:00, 3209.00it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils import load_split_data\n",
    "data_path = \"dataset/train.csv\"\n",
    "prompt_type = 2\n",
    "MAX_INPUT = 2300\n",
    "if_train = True\n",
    "df_train , df_valid = load_split_data(data_path, prompt_type, MAX_INPUT, if_train, True)\n",
    "test = df_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f2a1d9e5-14ed-4ba4-b701-dc878f211a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def inference(model, test_dataloader):\n",
    "    test_predictions = []\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        batch_input, idx = batch\n",
    "        for k in batch_input.keys():\n",
    "            batch_input[k] = batch_input[k].to(device)\n",
    "        with torch.no_grad():\n",
    "            #batch_input, idx = batch['input_ids'], batch['id']\n",
    "            #batch_input = {\"input_ids\": batch_input}\n",
    "            #logits = outputs.logits.cpu().detach().numpy()\n",
    "            response = model.generate(**batch_input, max_new_tokens=1, return_dict_in_generate=True, output_scores=True)\n",
    "            #batch_input['input_ids'].shape[-1] + 1\n",
    "            #print(f\"score {response.scores}\")\n",
    "            #print(f\"score is {response.scores[0]}\")\n",
    "            #print(f\"response is {response}\")\n",
    "            #redict = np.exp(logits) / np.sum(np.exp(logits), axis=-1, keepdims=True)\n",
    "            score = response.scores[0]\n",
    "            A_prob, B_prob, C_prob = score[:,A_TOKEN_IDS], score[:,B_TOKEN_IDS], score[:,C_TOKEN_IDS]\n",
    "            print(f\"A_prob shape is {A_prob.shape}\")\n",
    "            logits = torch.Tensor([[A_prob,B_prob,C_prob]])\n",
    "            logits = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "            print(f\"logits is {logits.shape}\")\n",
    "            node_result = [[idx[i],logits[i]] for i in range(batch_size)]\n",
    "\n",
    "        test_predictions.append(node_result)\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5600585-ab23-4b1c-a723-7c6ab546e9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62cc96ee-74c1-4f9a-a50d-9a3598330116",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = 'google/gemma-2-9b-it'\n",
    "model_path = \"output/peachy-sun-253/checkpoint-100\"\n",
    "MAX_LENGTH = 2300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57af0d60-712e-4efa-b0cd-3731f3198ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08bbd76dd5534105b63590e6a516a78f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Gemma2ForCausalLM(\n",
       "      (model): Gemma2Model(\n",
       "        (embed_tokens): Embedding(256000, 3584, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-41): 42 x Gemma2DecoderLayer(\n",
       "            (self_attn): Gemma2SdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3584, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3584, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3584, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=4096, out_features=3584, bias=False)\n",
       "              (rotary_emb): Gemma2RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Gemma2MLP(\n",
       "              (gate_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=3584, bias=False)\n",
       "              (act_fn): PytorchGELUTanh()\n",
       "            )\n",
       "            (input_layernorm): Gemma2RMSNorm()\n",
       "            (post_attention_layernorm): Gemma2RMSNorm()\n",
       "            (pre_feedforward_layernorm): Gemma2RMSNorm()\n",
       "            (post_feedforward_layernorm): Gemma2RMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): Gemma2RMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3584, out_features=256000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, truncation_side = 'left')\n",
    "config = AutoConfig.from_pretrained(base_model, trust_remote_code=True)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  \n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "base_model_0 = AutoModelForCausalLM.from_pretrained(base_model,\n",
    "                                                 config=config,\n",
    "                                                 quantization_config=bnb_config,\n",
    "                                                 torch_dtype=torch.float16,\n",
    "                                                 device_map=\"auto\",\n",
    "                                                 trust_remote_code=True)\n",
    "# base_model_0.config.pad_token_id = tokenizer.pad_token_id\n",
    "# base_model_0.resize_token_embeddings(len(tokenizer))\n",
    "new_model = model_path\n",
    "model0 = PeftModel.from_pretrained(base_model_0, new_model).to(device)\n",
    "#model0 = model0.merge_and_unload()\n",
    "model0.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae5a3f5e-455d-4318-ad64-02280a99aa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_TOKEN_IDS = tokenizer('A',add_special_tokens=True, truncation=True, max_length=1024)['input_ids'][1:]\n",
    "B_TOKEN_IDS = tokenizer('B',add_special_tokens=True, truncation=True, max_length=1024)['input_ids'][1:]\n",
    "C_TOKEN_IDS = tokenizer('C',add_special_tokens=True, truncation=True, max_length=1024)['input_ids'][1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6ba1af6-413e-4fdb-9357-9d7094743e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([235280], [235305], [235288])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_TOKEN_IDS,B_TOKEN_IDS,C_TOKEN_IDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6316fe95-7d22-4033-a697-1852f00750fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# tokenized_dataset = InstructionDataSet(test, tokenizer, MAX_LENGTH, 1)\n",
    "\n",
    "# token_ids = tokenized_dataset[0]['input_ids']\n",
    "# tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "\n",
    "# for token, token_id in zip(tokens, token_ids):\n",
    "#     print(f\"Token: {token}, ID: {token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd6cf97a-1ad2-4cdb-92c1-17690144d40d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "tokenized_dataset = InstructionDataSet(test, tokenizer, MAX_LENGTH, 1)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(tokenized_dataset, batch_size = batch_size ,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4e194ff9-be6d-4472-a28b-b30b91a334a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/2874 [00:00<09:35,  4.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_prob shape is torch.Size([1, 1])\n",
      "A_prob shape is torch.Size([1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/2874 [00:00<08:00,  5.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_prob shape is torch.Size([1, 1])\n",
      "A_prob shape is torch.Size([1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/2874 [00:01<07:34,  6.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_prob shape is torch.Size([1, 1])\n",
      "A_prob shape is torch.Size([1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/2874 [00:01<07:19,  6.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_prob shape is torch.Size([1, 1])\n",
      "A_prob shape is torch.Size([1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/2874 [00:01<08:00,  5.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_prob shape is torch.Size([1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/2874 [00:02<09:28,  5.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_prob shape is torch.Size([1, 1])\n",
      "A_prob shape is torch.Size([1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 13/2874 [00:02<08:10,  5.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_prob shape is torch.Size([1, 1])\n",
      "A_prob shape is torch.Size([1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 15/2874 [00:02<08:20,  5.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_prob shape is torch.Size([1, 1])\n",
      "A_prob shape is torch.Size([1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 17/2874 [00:02<07:49,  6.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_prob shape is torch.Size([1, 1])\n",
      "A_prob shape is torch.Size([1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 19/2874 [00:03<07:45,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_prob shape is torch.Size([1, 1])\n",
      "A_prob shape is torch.Size([1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 21/2874 [00:03<07:20,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_prob shape is torch.Size([1, 1])\n",
      "A_prob shape is torch.Size([1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 23/2874 [00:04<09:31,  4.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_prob shape is torch.Size([1, 1])\n",
      "A_prob shape is torch.Size([1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 24/2874 [00:04<08:46,  5.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_prob shape is torch.Size([1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sub_pred \u001b[38;5;241m=\u001b[39m \u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[34], line 12\u001b[0m, in \u001b[0;36minference\u001b[0;34m(model, test_dataloader)\u001b[0m\n\u001b[1;32m      7\u001b[0m     batch_input[k] \u001b[38;5;241m=\u001b[39m batch_input[k]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m#batch_input, idx = batch['input_ids'], batch['id']\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m#batch_input = {\"input_ids\": batch_input}\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m#logits = outputs.logits.cpu().detach().numpy()\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m#batch_input['input_ids'].shape[-1] + 1\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m#print(f\"score {response.scores}\")\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m#print(f\"score is {response.scores[0]}\")\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m#print(f\"response is {response}\")\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m#redict = np.exp(logits) / np.sum(np.exp(logits), axis=-1, keepdims=True)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     score \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mscores[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/peft/peft_model.py:1491\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1489\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1490\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1491\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1493\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:1639\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1637\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_model_class()\n\u001b[1;32m   1638\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# Pull this out first, we only use it for stopping criteria\u001b[39;00m\n\u001b[0;32m-> 1639\u001b[0m generation_config, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_generation_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1640\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_model_kwargs(model_kwargs\u001b[38;5;241m.\u001b[39mcopy())\n\u001b[1;32m   1641\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_assistant(assistant_model)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:1359\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_generation_config\u001b[0;34m(self, generation_config, **kwargs)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;66;03m# TODO joao: when we can detect `fullgraph=True` in `torch.compile` (https://github.com/pytorch/pytorch/pull/120400)\u001b[39;00m\n\u001b[1;32m   1345\u001b[0m \u001b[38;5;66;03m# replace `is_torchdynamo_compiling` by the corresponding check. As it is, we are being too restrictive with\u001b[39;00m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;66;03m# the parameterization in `fullgraph=False` so as to enable `fullgraph=True`.\u001b[39;00m\n\u001b[1;32m   1347\u001b[0m \n\u001b[1;32m   1348\u001b[0m \u001b[38;5;66;03m# priority: `generation_config` argument > `model.generation_config` (the default generation config)\u001b[39;00m\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;66;03m# legacy: users may modify the model configuration to control generation. To trigger this legacy behavior,\u001b[39;00m\n\u001b[1;32m   1351\u001b[0m     \u001b[38;5;66;03m# three conditions must be met\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1354\u001b[0m     \u001b[38;5;66;03m# 3) the user must have set generation parameters in the model config.\u001b[39;00m\n\u001b[1;32m   1355\u001b[0m     \u001b[38;5;66;03m# NOTE: `torch.compile` can't compile `hash`, this legacy support is disabled with compilation.\u001b[39;00m\n\u001b[1;32m   1356\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1357\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling()\n\u001b[1;32m   1358\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39m_from_model_config\n\u001b[0;32m-> 1359\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39m_original_object_hash \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1360\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_has_non_default_generation_parameters()\n\u001b[1;32m   1361\u001b[0m     ):\n\u001b[1;32m   1362\u001b[0m         new_generation_config \u001b[38;5;241m=\u001b[39m GenerationConfig\u001b[38;5;241m.\u001b[39mfrom_model_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[1;32m   1363\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_generation_config \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:439\u001b[0m, in \u001b[0;36mGenerationConfig.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mhash\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_json_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:1101\u001b[0m, in \u001b[0;36mGenerationConfig.to_json_string\u001b[0;34m(self, use_diff, ignore_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1099\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m-> 1101\u001b[0m config_dict \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_keys_to_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1102\u001b[0m config_dict \u001b[38;5;241m=\u001b[39m convert_dataclass_to_dict(config_dict)\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mdumps(config_dict, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, sort_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:1085\u001b[0m, in \u001b[0;36mGenerationConfig.to_json_string.<locals>.convert_keys_to_string\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m metadata_field \u001b[38;5;129;01min\u001b[39;00m METADATA_FIELDS:\n\u001b[1;32m   1083\u001b[0m         config_dict\u001b[38;5;241m.\u001b[39mpop(metadata_field, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1085\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_keys_to_string\u001b[39m(obj):\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m   1087\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;28mstr\u001b[39m(key): convert_keys_to_string(value) \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sub_pred = inference(model = model0, test_dataloader = test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "87381f51-1930-4d19-8c61-c0196e9196a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取数据\n",
    "processed_data = []\n",
    "for item in sub_pred:\n",
    "    item = item[0]\n",
    "    id = item[0].item()  # 获取id\n",
    "    array_values = item[1].tolist()  # 获取array并转换为列表\n",
    "    processed_data.append([id] + array_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ad7e3bf-a0b1-4249-ba3c-68b6e632dc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = ['id', 'winner_model_a', 'winner_model_b', 'winner_tie']\n",
    "df = pd.DataFrame(processed_data, columns=new_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7b08e7ca-463a-4554-b57d-0b0385d21238",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby('id').mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef037aac-5011-48eb-ad82-e1f35d2ee2b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30192</td>\n",
       "      <td>0.964621</td>\n",
       "      <td>0.011230</td>\n",
       "      <td>0.024149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1256092</td>\n",
       "      <td>0.961766</td>\n",
       "      <td>0.011373</td>\n",
       "      <td>0.026860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3258431</td>\n",
       "      <td>0.972880</td>\n",
       "      <td>0.010640</td>\n",
       "      <td>0.016480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4186011</td>\n",
       "      <td>0.043130</td>\n",
       "      <td>0.839631</td>\n",
       "      <td>0.117239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5717448</td>\n",
       "      <td>0.863039</td>\n",
       "      <td>0.061549</td>\n",
       "      <td>0.075412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2869</th>\n",
       "      <td>4287311513</td>\n",
       "      <td>0.257485</td>\n",
       "      <td>0.545095</td>\n",
       "      <td>0.197420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2870</th>\n",
       "      <td>4289792977</td>\n",
       "      <td>0.260140</td>\n",
       "      <td>0.222510</td>\n",
       "      <td>0.517350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2871</th>\n",
       "      <td>4291367819</td>\n",
       "      <td>0.079588</td>\n",
       "      <td>0.767002</td>\n",
       "      <td>0.153410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2872</th>\n",
       "      <td>4292709507</td>\n",
       "      <td>0.028645</td>\n",
       "      <td>0.255313</td>\n",
       "      <td>0.716042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2873</th>\n",
       "      <td>4293918673</td>\n",
       "      <td>0.981845</td>\n",
       "      <td>0.001523</td>\n",
       "      <td>0.016632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2874 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  winner_model_a  winner_model_b  winner_tie\n",
       "0          30192        0.964621        0.011230    0.024149\n",
       "1        1256092        0.961766        0.011373    0.026860\n",
       "2        3258431        0.972880        0.010640    0.016480\n",
       "3        4186011        0.043130        0.839631    0.117239\n",
       "4        5717448        0.863039        0.061549    0.075412\n",
       "...          ...             ...             ...         ...\n",
       "2869  4287311513        0.257485        0.545095    0.197420\n",
       "2870  4289792977        0.260140        0.222510    0.517350\n",
       "2871  4291367819        0.079588        0.767002    0.153410\n",
       "2872  4292709507        0.028645        0.255313    0.716042\n",
       "2873  4293918673        0.981845        0.001523    0.016632\n",
       "\n",
       "[2874 rows x 4 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "35ba02d2-3deb-46af-90ff-4b95e46fbcd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2981: UserWarning: The y_pred values do not sum to one. Starting from 1.5 thiswill result in an error.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0588871269259368"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str2num = {'A':0, \"B\":1, \"C\":2}\n",
    "test['label_number'] = test.label.map(str2num)\n",
    "from sklearn.metrics import log_loss\n",
    "prediction = np.array(df[new_columns[1:]])\n",
    "test = test.drop_duplicates(subset = ['id']).reset_index(drop = True)\n",
    "log_loss(test.label_number, prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
