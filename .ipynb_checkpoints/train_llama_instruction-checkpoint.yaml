data_path: ['dataset/non_overlap/train_33k_switch_10k.json']
train_data: 'dataset/non_overlap/train_33k.json'
valid_data: 'dataset/non_overlap/valid.json'
resume_from_checkpoint: ''
all_in_one: True
prompt_type: 2
if_train: True
split: False
split_by_prompt: True
extranal_data: True
if_concat : True
if_drop_duplicate: True
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
learning_rate: 6.e-5
lr_end: 1.e-6
warmup_ratio: 0.1
warmup_steps: 30
num_train_epochs: 2
gradient_accumulation_steps: 16
logging_steps: 10
eval_steps: 600
save_steps: 200
weight_decay: 1.e-5
MAX_INPUT: 2000
MODEL: 'meta-llama/llama-3-transformers-8b-chat-hf-v1'
dropout_rate: 0.1
awp_lr: 0
awp_eps: 1.e-4
awp_start_epoch: 0.5
label_smoothing_factor: 0
output_dir: 'output'
use_cache: False
token_type: 'MC'
lora_r: 64
lora_alpha: 128
lora_dropout: 0.05
test_mode: False
